---
title: "Blog: Running Kubeflow in an Enterprise - enmeshed in the service mesh"
description: "Installing Kubeflow 1.3 in existing k8s cluster, Istio service mesh and Argo"
layout: post
toc: true
comments: true
image: images/logo.png
hide: false
categories: [kubeflow, kubeflow 1.3, install kubeflow, kubeflow pipelines, intuit, istio, service mesh, argo]
permalink: /kubeflow/
author: "<a href='https://www.linkedin.com/in/deepk2u/'>Deepak Kumar</a>"
---

Deploying Kubeflow 1.3 in an enterprise with existing k8s infrastructure, a Service Mesh (Istio) and Argo has its own host of challenges.
This blog will address how those challenges were overcome while retaining the best practices that both, the organization and Kubeflow prescribe.


## Lay of the land at Intuit

Intuit has invested heavily in building out a robust Kubernetes infrastructure that powers all of Intuit's products - TurboTax, QuickBooks and Mint. There are 1000s of services that run on 100+ K8s clusters. Managing these clusters is the Intuit Kubernetes Service (IKS) control plane. The IKS control plane provides services such as namespace management, role management and isolation etc. Connecting the services is an advanced Istio based service mesh, which complements Intuit's API Gateway and in combination, they provide robust authentication, authorization, rate limiting and other routing capabilities.

The Intuit ML Platform is built on this ecosystem, and provides model training, inference and feature management capabilities - leveraging the best of Intuit's K8s infrastructure and AWS SageMaker. This is the backdrop against which we started exploring Kubeflow to provide advanced orchestration, experimentation and other services.

## Kubeflow and Istio

Our first challenge with running Kubeflow was the compatibility of Kubeflow's Istio with Intuit's existing Service Mesh built on top of Istio. Two key problems emerge: version compatibility, and operational maintenance.

Kubeflow v1.3 defaults to Istio (v1.9), and luckily it is compatible with the older versions of Istio (v1.6), which is what Intuit runs on. Running two Istio versions is impractical, as that would defeat the benefit of a large interconnected existing service mesh. Hence, we wanted for Kubeflow to work seamlessly with Intuit's service mesh, running the older version of Istio (v1.6)

If you are new to Istio, you might want a primer on these key [Traffic Management Components](https://istio.io/latest/docs/reference/config/networking/) and [Security Cpmponents](https://istio.io/latest/docs/reference/config/security/):

1. VirtualService
2. DestinationRule
3. Gateway
4. EnvoyFilter
5. AuthorizationPolicy

### Step 1: Remove default Istio configurations and Argo from Kubeflow

The first step to running Kubeflow was to remove the Istio and Argo bundled with Kubeflow, so that it can be integrated with the Intuit service mesh.

**To Remove Kubeflow default Istio:**

We have used Kustomize to build the manifest we need for our Kubeflow installation and we are using [ArgoCD](https://argoproj.github.io/argo-cd/) to build the deploy the Kubeflow k8s manifests. 

```
.
├── base                        # Base folder for the kubeflow out of the box manifests
│   ├── kustomization.yaml      
│   ├── pipelines               # Folder for Kubeflow Pipelines module
│   │   ├── kustomization.yaml
│   ├── other modules           # Similar to Pipeliens module you can bring other modules as well
│       ├── kustomization.yaml
├── envs                        # Folder for all the Kubeflow environments
│   ├── prod               
│   │   ├── kustomization.yaml
│   ├── dev               
│       ├── kustomization.yaml
```

`base -> kustomization.yaml`
```yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
- github.com/kubeflow/manifests/common/kubeflow-roles/base?ref=v1.3.0
- github.com/kubeflow/manifests/common/kubeflow-namespace/base?ref=v1.3.0
- github.com/kubeflow/manifests/common/oidc-authservice/base?ref=v1.3.0
- github.com/kubeflow/manifests/apps/admission-webhook/upstream/overlays/cert-manager?ref=v1.3.0
- github.com/kubeflow/manifests/apps/profiles/upstream/overlays/kubeflow?ref=v1.3.0
- github.com/kubeflow/manifests/apps/centraldashboard/upstream/overlays/istio?ref=v1.3.0
- pipelines
```

`base -> pipelines -> kustomization.yaml`
```yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
bases:
  - github.com/kubeflow/pipelines/manifests/kustomize/base/installs/multi-user?ref=1.5.0
  - github.com/kubeflow/pipelines/manifests/kustomize/base/metadata/base?ref=1.5.0
  - github.com/kubeflow/pipelines/manifests/kustomize/base/metadata/options/istio?ref=1.5.0
  # To remove the default Argo from Pipelines module
  # - github.com/kubeflow/pipelines/manifests/kustomize/third-party/argo/installs/cluster?ref=1.5.0
  - github.com/kubeflow/pipelines/manifests/kustomize/third-party/mysql/base?ref=1.5.0
  - github.com/kubeflow/pipelines/manifests/kustomize/third-party/mysql/options/istio?ref=1.5.0
  - github.com/kubeflow/pipelines/manifests/kustomize/third-party/minio/base?ref=1.5.0
  - github.com/kubeflow/pipelines/manifests/kustomize/third-party/minio/options/istio?ref=1.5.0
  - github.com/kubeflow/pipelines/manifests/kustomize/third-party/metacontroller/base?ref=1.5.0

# Identifier for application manager to apply ownerReference.
# The ownerReference ensures the resources get garbage collected
# when application is deleted.
commonLabels:
  application-crd-id: kubeflow-pipelines

# !!! If you want to customize the namespace,
# please also update base/cache-deployer/cluster-scoped/cache-deployer-clusterrolebinding.yaml
namespace: kubeflow
```

Note: the reason why we had create a separate folder for piplines is, because we don't want to use Argo which comes with piplines module. If you can use default Argo, then you can simply use `https://github.com/kubeflow/manifests/apps/pipeline/upstream/env/platform-agnostic-multi-user-pns?ref=v1.3.0` instead of pipelines folder.

If you don't want to use ArgoCD, you can build the manifest using `kustomize build` command, which is essentially what ArgoCD also does. The above configuration is tested for Kustomize 3.8.x and 4.0.x, and it works with both.

### Step 2: Kustomize the Kubeflow manifests

Given the managed K8s ecosystem at Intuit, protocols for service to service communication and namespace isolation is opinionated, and we had to make following changes:

1. Enable Kubeflow namespace for [Istio injection](https://istio.io/latest/docs/setup/additional-setup/sidecar-injection/#automatic-sidecar-injection) by adding the label `istio-injection: enabled` in the namespace specification. This label is then used by Istio to add the sidecar into the namespace
2. Enable sidecar injection to all the deployments and statefulsets in kubeflow by adding annotation `sidecar.istio.io/inject: "true"` along with some Intuit specific custom labels and annotations to the Deployments and StatefulSets
3. Intuit's security policies forbid the direct use of external container registries. Intuit's internal container registry runs regular vulnerability scans and certifies docker images for use in various environments. The internal container registry also has an allow list that enables external registries to be proxied and held to the same high security standards. This was enabled for all Kubeflow containers.
4. Changes in VirtualService to route all the traffic from one central gateway instead of using Kubeflow gateway

We have used [Kustomize](https://kustomize.io/) to modify the Kubeflow application manifest.

1. For adding labels, we have used LabelTransformer
    ```yaml
        apiVersion: builtin
        kind: LabelTransformer
        metadata:
          name: deployment-labels
        labels:
          <Intuit custom labels>
          istio-injected: "true"
        fieldSpecs:
        - path: spec/template/metadata/labels
          kind: Deployment
          create: true
        - path: spec/template/metadata/labels
          kind: StatefulSet
          create: true
    ```
2. For adding annotations, we have used AnnotationsTransformer

    ```yaml
    apiVersion: builtin
    kind: AnnotationsTransformer
    metadata:
      name: deployment-annotations
    annotations:
      <Intuit custom annotations>
      sidecar.istio.io/inject: "true"
    fieldSpecs:
    - path: spec/template/metadata/annotations
      kind: Deployment
      create: true
    - path: spec/template/metadata/annotations
      kind: StatefulSet
      create: true
    ```

3. For replacing docker image urls, we used ImageTagTransformer

    ```yaml
    apiVersion: builtin
    kind: ImageTagTransformer
    metadata:
      name: image-transformer-1
    imageTag:
      name: gcr.io/ml-pipeline/cache-deployer
      newName: docker.intuit.com/gcr-rmt/ml-pipeline/cache-deployer
    ```

4. For transforming VirtualServices

    ```yaml
    - op: remove
      path: /spec/hosts/0
    - op: replace
      path: /spec/gateways/0
      value: <custom gateway>
    - op: add
      path: /spec/hosts/0
      value: <kubflow host name>
    - op: add
      path: /spec/exportTo
      value: ["."]
    ```
5. To wire everything up, we used Kustomization manifest

    `envs -> prod/dev -> kustomization.yaml`
    ```yaml
    apiVersion: kustomize.config.k8s.io/v1beta1
    kind: Kustomization

    resources:
    - kubeflow.yaml

    transformers:
    - transformers/image-transformers.yaml
    - transformers/label-transformers.yaml
    - transformers/annotations-transformers.yaml

    patchesJson6902:
    # patch VirtualService with explicit host
    # add multiple targets like below for all the VirtualServices which you need
    - path: patches/virtual-service-hosts.yaml
      target:
        group: networking.istio.io
        version: v1alpha3
        kind: VirtualService
        name: centraldashboard
    ```
6. You might face issues with `metadata_envoy` service, in our case we were getting following error
    ```
    [debug][init] [external/envoy/source/common/init/watcher_impl.cc:27] init manager Server destroyed
    unable to bind domain socket with id=0 (see --base-id option)
    2021-01-29T23:32:26.680310Z	error	Epoch 0 exited with error: exit status 1
    ```

    After looking up, we found that, when you run this docker image with Istio Sidecar injection, this problem occurs.
    The reason is, both these containers are essentially envoyproxy containers and default base-id for both
    containers is set to 0.
    
    So to make it work, we had to change CMD in this [Dockerfile](https://github.com/kubeflow/pipelines/blob/1.4.1/third_party/metadata_envoy/Dockerfile#L27)

    ```
    CMD ["/etc/envoy.yaml", "--base-id", "1"]
    ``` 

### Step 3: Custom changes needed for SSO

There are 2 major components around authentication using SSO.

1. Authservice: It is a StatefulSet that runs the oidc-auth service. It runs in istio-system namespace and directly talks to an OIDC service for authentication
2. Authn-filter: It's an EnvoyFilter which filters the traffic to authservice and checks the Kubeflow auth header and redirects to authservice if request is not authorized, check the presence of header called `kubeflow-userid`

Note: Intuit SSO supports OIDC so we did not need to use dex for the integration, but if your org SSO does not support OIDC, then you can use dex in the middle, details can be found [here](https://www.kubeflow.org/docs/started/k8s/kfctl-istio-dex/).

For our installation, we needed the authservice to be mesh enabled and it made more sense to move authservice to `kubeflow` namespace as well, which was already enabled for istio sidecar injection.

After enabling Istio mesh on `authservice`, it requires some more changes in default manifest for it to work. The reason being, `authservice` Pod not able to communicate with Intuit SSO https url, because outbound traffic from the main container pod is intercepted by istio sidecar to enforce mtls (default behavior). So, we had to exclude the https port (443) to disable mtls. This can be done using the annotation `traffic.sidecar.istio.io/excludeOutboundPorts: "443"`


### Step 4: Setting up Ingress

We exposed the `istio-ingressgateway` service as LoadBalancer using the following mechanism.

1. Setting up public hosted zone in Route 53, add host name you would like to use, like `example.com`
2. Setup ACM certificate for the host name you want to use for the Kubeflow installation, host name can be `kubeflow.example.com`
3. Update the service manifest by adding few annotations
```yaml
# Note that the backend talks over HTTP.
service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http
# TODO: Fill in with the ARN of your certificate.
service.beta.kubernetes.io/aws-load-balancer-ssl-cert: <cert arn from step 2>
service.beta.kubernetes.io/aws-load-balancer-security-groups: <to restrict access within org>
# Only run SSL on the port named "https" below.
service.beta.kubernetes.io/aws-load-balancer-ssl-ports: "https"
external-dns.alpha.kubernetes.io/hostname: kubeflow.example.com
```

After applying the new manifest, aws will automatically add the appropriate A and TXT entries in your hosted zone (`example.com`) and kubeflow will be accessible at `kubeflow.example.com`

To secure the [Gateway with https](https://www.kubeflow.org/docs/started/k8s/kfctl-istio-dex/#secure-with-https) you can also change the gateway port and add key and certificate in the Gateway.

More about these annotations can be found at [Terminate HTTPS traffic on Amazon EKS](https://aws.amazon.com/premiumsupport/knowledge-center/terminate-https-traffic-eks-acm/) and [SSL support on AWS](https://kubernetes.io/docs/concepts/services-networking/service/#ssl-support-on-aws) blog.

### Step 5: Using external Argo installation

Kubfelow uses argo workflows internally to run the pipeline in a workflow fashion. Argo generates artifacts after the workflow steps and all we need to do is configure the artifact store if we are planning to use the external argo.

1. [Install argo workflows](https://github.com/argoproj/argo-workflows/blob/master/docs/quick-start.md#install-argo-workflows) in your cluster, it gets installed in a namespace called argo
2. Remove all the argo related manifests from Kubeflow
3. To override the artifact store, you would need to change the ConfigMap workflow-controller-configmapwhich comes with the [Kubeflow manifest](https://github.com/kubeflow/manifests/blob/306d02979124bc29e48152272ddd60a59be9306c/argo/base/config-map.yaml). It uses minio as the store but you can configure it to use s3 as well. More details can be found from the [Argo](https://github.com/argoproj/argo-workflows/blob/master/docs/workflow-controller-configmap.md)[Workflow Controller Configmap github page](https://github.com/argoproj/argo-workflows/blob/master/docs/workflow-controller-configmap.md)
4. With latest version of Argo there is option to override [artifact store for namespace](https://argoproj.github.io/argo-workflows/artifact-repository-ref/) as well

**Debugging tricks:**

1. Check if EnvoyFilter is getting applied: You should have **istioctl** cmd tool, makes the job really easier, easier to install using brew

    `istioctl proxy-config listeners \&lt;pod name\&gt; --port 15001 -o json`

    See if the envoy filter is getting listed in the output. More about istio proxy debugging can be found [here](https://istio.io/latest/docs/ops/diagnostic-tools/proxy-cmd/).

2. Checking istio-ingressgateway

    ```
    # Port forward to the first istio-ingressgateway pod
    kubectl -n istio-system port-forward $(kubectl -n istio-system get pods -listio=ingressgateway -o=jsonpath="{.items[0].metadata.name}") 15000

    # Get the http routes from the port-forwarded ingressgateway pod (requires jq)
    curl --silent http://localhost:15000/config_dump | jq '\''.configs.routes.dynamic_route_configs[].route_config.virtual_hosts[]| {name: .name, domains: .domains, route: .routes[].match.prefix}'\''

    # Get the logs of the first istio-ingressgateway pod
    # Shows what happens with incoming requests and possible errors
    kubectl -n istio-system logs $(kubectl -n istio-system get pods -listio=ingressgateway -o=jsonpath="{.items[0].metadata.name}") --tail=300

    # Get the logs of the first istio-pilot pod
    # Shows issues with configurations or connecting to the Envoy proxies
    kubectl -n istio-system logs $(kubectl -n istio-system get pods -listio=pilot -o=jsonpath="{.items[0].metadata.name}") discovery --tail=300
    ```

3. Checking the authservice connectivity: istio-ingressgatewaypod should be able to access authservice. You can check that using command:

    `kubectl -n istio-system exec $(kubectl -n istio-system get pods -listio=pilot -o=jsonpath="{.items[0].metadata.name}") -- curl -v http://authservice.istio-system.svc.cluster.local:8080`

    Also make sure authservice is able to reach dex:

    In our case, authservice is in the kubeflow namespace so we made changes accordingly using the command below:

    `kubectl -n kubeflow exec authservice-0 -- wget -q -S -O '-' <oidc auth url>/.well-known/openid-configuration`

    It should show something like below:
    ```javascript
    {
      "issuer": "http://dex.kubeflow.svc.cluster.local:5556/dex",
      "authorization_endpoint": "http://dex.kubeflow.svc.cluster.local:5556/dex/auth",
      "token_endpoint": "http://dex.kubeflow.svc.cluster.local:5556/dex/token",
      "jwks_uri": "http://dex.kubeflow.svc.cluster.local:5556/dex/keys",
      "userinfo_endpoint": "http://dex.kubeflow.svc.cluster.local:5556/dex/userinfo",
      "response_types_supported": [
        "code"
      ],
      "subject_types_supported": [
        "public"
      ],
      "id_token_signing_alg_values_supported": [
        "RS256"
      ],
      "scopes_supported": [
        "openid",
        "email",
        "groups",
        "profile",
        "offline_access"
      ],
      "token_endpoint_auth_methods_supported": [
        "client_secret_basic"
      ],
      "claims_supported": [
        "aud",
        "email",
        "email_verified",
        "exp",
        "iat",
        "iss",
        "locale",
        "name",
        "sub"
      ]
    }
    ```
  
4. Checking connectivity between services: try using **curl** or **wget** from one service to another, usually one or the other is always available otherwise you can always install using apt-get command. Example use case: from ml-pipeline deployment pod you can check if pipeline apis are accessible?
  
    `kubectl -n kubeflow exec $(kubectl -n kubeflow get pods -lapp=ml-pipeline-ui -o=jsonpath="{.items[0].metadata.name}")  -- wget -q -S -O '-' ml-pipeline.kubeflow.svc.cluster.local:8888/apis/v1beta1/pipelines`

## Asks for the Kubeflow Community

The challenges that we encountered at Intuit are not unique, and will be faced by any enterprise that wants to adopt Kubeflow.

It would be nice to have Kubeflow play well with the available K8s infrastructure in an enterprise, rather than mandating its own set of infrastructure. Here are some suggestions/bugs for improvement to the ecosystem, some of which Intuit will work with the community to build out:

1. We saw Kubeflow manifest repo went through [major folder restructuring](https://github.com/kubeflow/manifests/issues/1735) for v1.3 but we think there is still room for improvements.
2. Multi Cluster / Multi Region support. [#5467](https://github.com/kubeflow/kubeflow/issues/5467)
3. Upgrade seems to be an issue in general, should figure out a way to manage this better. [#5440](https://github.com/kubeflow/kubeflow/issues/5440)
4. Multi-tenancy with group support. [#4188](https://github.com/kubeflow/kubeflow/issues/4188)
5. Installing Kubeflow in any custom namespace. [#5647](https://github.com/kubeflow/kubeflow/issues/5647)
6. Existing metadata service is not performant, we did try some settings with more resources and horizontal scaling. Community is already working on [KFP v2.0](https://docs.google.com/document/d/1fHU29oScMEKPttDA1Th1ibImAKsFVVt2Ynr4ZME05i0/edit), which might address alot of concerns around metadata service.

**References**

- [Kubeflow Pipelines (KFP) v2 System Design](https://docs.google.com/document/d/1fHU29oScMEKPttDA1Th1ibImAKsFVVt2Ynr4ZME05i0/edit)
- [Traffic Management Components](https://istio.io/latest/docs/reference/config/networking/)
- [Istio 1.6 Architecture](https://istio.io/v1.6/docs/ops/deployment/architecture/)
- [Istio 1.3 Architecture](https://istio.io/v1.3/docs/concepts/what-is-istio/#architecture)
- [Terminate HTTPS traffic on Amazon EKS](https://aws.amazon.com/premiumsupport/knowledge-center/terminate-https-traffic-eks-acm/)
- [SSL support on AWS](https://kubernetes.io/docs/concepts/services-networking/service/#ssl-support-on-aws)
- [Intuit's Modern SaaS Platform](https://www.developermarch.com/developersummit/downloadPDF/Intuit%20Modern%20SaaS%20Platform%20-%20GIDS.pdf)
- [Stitching a Service Mesh Across Hundreds of Discrete Networks](https://www.youtube.com/watch?v=EWyNbBn1vns)
- [Multicluster Istio configuration and service discovery using Admiral](https://istio.io/latest/blog/2020/multi-cluster-mesh-automation/)
- [Genius of Admiral](https://medium.com/intuit-engineering/genius-of-admiral-3307e63e3ab6)