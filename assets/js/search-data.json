{
  
    
        "post0": {
            "title": "From Raw Data to Model Serving: A Blueprint for the AI/ML Lifecycle with Kubeflow",
            "content": "From Raw Data to Model Serving: A Blueprint for the AI/ML Lifecycle with Kubeflow . Are you looking for a practical, reproducible way to take a machine learning project from raw data all the way to a deployed, production-ready model? This post is your blueprint for the AI/ML lifecycle: you’ll learn how to use Kubeflow and open source tools such as Feast to build a workflow you can run on your laptop and adapt to your own projects. . We’ll walk through the entire ML lifecycle—from data preparation to live inference—leveraging the Kubeflow platform to create a cohesive, production-grade MLOps workflow. . Project Overview . The project implements a complete MLOps workflow for a fraud detection use case. Fraud detection is a critical application in financial services, where organizations need to identify potentially fraudulent transactions in real-time while minimizing false positives that could disrupt legitimate customer activity. . Our fraud detection system leverages machine learning to analyze large volumes of transaction data, learn patterns from historical behavior, and flag suspicious transactions that deviate from normal patterns. The model considers various features such as transaction amounts, location data, merchant information, and user behavior patterns to make predictions. This makes fraud detection an ideal use case for demonstrating MLOps concepts because it requires: . Real-time inference: Fraud detection decisions must be made instantly as transactions occur | Feature consistency: The same features used in training must be available during inference to ensure model accuracy | Scalability: The system must handle high transaction volumes | Continuous learning: Models need regular retraining as fraud patterns evolve | Compliance and auditability: Financial services require comprehensive model tracking and governance | . The workflow ingests raw transaction data, proceeds through data preparation and feature engineering, then model training and registration, and finally deploys the model as a production-ready inference service that can evaluate transactions in real-time. . The entire workflow is orchestrated as a Kubeflow Pipeline, which provides a powerful framework for defining, deploying, and managing complex machine learning pipelines on Kubernetes. . Here is a high-level overview of the pipeline: . . A Note on the Data . The pipeline assumes that the initial datasets (train.csv, test.csv, etc.) are already available. For readers who wish to follow along or generate their own sample data, a script is provided in the synthetic_data_generation directory. This script was used to create the initial data for this project but is not part of the automated Kubeflow pipeline itself. . Why Kubeflow? . This project demonstrates the power of using Kubeflow to abstract away the complexity of Kubernetes infrastructure, allowing AI Engineers, Data Scientists, and ML engineers to focus on what matters most: the data and model performance. . Key Benefits . Infrastructure Abstraction: Instead of manually managing Kubernetes deployments, service accounts, networking, and storage configurations, the pipeline handles all the infrastructure complexity behind the scenes. You define your ML workflow as code, and Kubeflow takes care of orchestrating the execution across your Kubernetes cluster. . Focus on AI, Not DevOps: With the infrastructure automated, you can spend your time on the activities that directly impact model performance: . Experimenting with different feature engineering approaches | Tuning hyperparameters and model architectures | Analyzing prediction results and model behavior | Iterating on data preparation and validation strategies | . Reproducible and Scalable: The pipeline ensures that every run follows the same steps with the same environment configurations, making your experiments reproducible. When you’re ready to scale up, the same pipeline can run on larger Kubernetes clusters without code changes. . Production-Ready from Day One: By using production-grade tools like KServe for model serving, Feast for feature management, and the Model Registry for governance, your development pipeline is already structured for production deployment. . Portable and Cloud-Agnostic: The entire workflow runs on standard Kubernetes, making it portable across different cloud providers or on-premises environments. What works on your laptop will work in production. . This approach shifts the cognitive load from infrastructure management to data science innovation, enabling faster experimentation and more reliable production deployments. . . Getting Started: Prerequisites and Cluster Setup . Before diving into the pipeline, you need to set up your local environment. This project is designed to run on a local Kubernetes cluster using kind. . Prerequisites . A container engine, like Podman or Docker. | Python (3.11 or newer). | uv: A fast Python package installer. | kubectl | kind | mc (MinIO Client) | . Note: This setup was tested on a VM with 12GB RAM, 8 CPUs, and 150GB of disk space. . 1. Create a Local Kubernetes Cluster . First, create a kind cluster. The following command will set up a new cluster with a specific node image compatible with the required components: . kind create cluster -n fraud-detection-e2e-demo --image kindest/node:v1.31.6 . 2. Deploy Kubeflow Pipelines . With your cluster running, the next step is to deploy Kubeflow Pipelines. For this project, the standalone installation is recommended, as it’s lighter and faster to set up than a full Kubeflow deployment. . Follow the official Kubeflow Pipelines standalone installation guide for the latest instructions. . 3. Upload the Raw Data to MinIO . MinIO is an open source, S3-compatible object storage system. In this project, MinIO is used to store raw datasets, intermediate artifacts, and model files, making them accessible to all pipeline components running in Kubernetes. . Before uploading, you need to port-forward the MinIO service so it’s accessible locally. Run the following command in a separate terminal window: . kubectl port-forward --namespace kubeflow svc/minio-service 9000:9000 . Next, generate the synthetic data and copy it to feature_engineering/feature_repo/data/input/ if you haven’t done yet. The synthetic data generation script creates the raw_transaction_datasource.csv file that serves as the primary input for the pipeline. . cd synthetic_data_generation uv sync source .venv/bin/activate python synthetic_data_generation.py cp raw_transaction_datasource.csv ../feature_engineering/feature_repo/data/input cd .. . You should see an output similar to the following. The generation may take a few minutes depending on your hardware. . Using CPython 3.11.11 Creating virtual environment at: .venv Resolved 7 packages in 14ms Installed 6 packages in 84ms + numpy==2.3.0 + pandas==2.3.0 + python-dateutil==2.9.0.post0 + pytz==2025.2 + six==1.17.0 + tzdata==2025.2 loading data... generating transaction level data... 0 of 1,000,000 (0%) complete 100,000 of 1,000,000 (10%) complete 200,000 of 1,000,000 (20%) complete 300,000 of 1,000,000 (30%) complete 400,000 of 1,000,000 (40%) complete 500,000 of 1,000,000 (50%) complete 600,000 of 1,000,000 (60%) complete 700,000 of 1,000,000 (70%) complete 800,000 of 1,000,000 (80%) complete 900,000 of 1,000,000 (90%) complete . Next, install and configure the MinIO Client (mc) if you haven’t already. Then, set up the alias and upload the datasets: . mc alias set minio-local http://localhost:9000 minio minio123 mc mb minio-local/mlpipeline mc cp -r feature_engineering/feature_repo/data/input/ minio-local/mlpipeline/artifacts/feature_repo/data/ mc cp feature_engineering/feature_repo/feature_store.yaml minio-local/mlpipeline/artifacts/feature_repo/ . This will create the required bucket and directory structure in MinIO and upload your raw datasets, making them available for the pipeline. . Once the upload is complete, you can stop the port-forward process. . 4. Install Model Registry, KServe, Spark Operator, and Set Policies . While the datasets are uploading to MinIO, you can proceed to install the remaining Kubeflow components and set up the required Kubernetes policies. The following steps summarize what’s in setup.sh: . Install Model Registry . kubectl apply -k &quot;https://github.com/kubeflow/model-registry/manifests/kustomize/overlays/db?ref=v0.2.16&quot; . Install KServe . kubectl create namespace kserve kubectl config set-context --current --namespace=kserve curl -s &quot;https://raw.githubusercontent.com/kserve/kserve/release-0.15/hack/quick_install.sh&quot; | bash kubectl config set-context --current --namespace=kubeflow . Install Kubeflow Spark Operator . helm repo add --force-update spark-operator https://kubeflow.github.io/spark-operator helm install spark-operator spark-operator/spark-operator --namespace spark-operator --create-namespace # Make sure the Spark Operator is watching all namespaces: helm upgrade spark-operator spark-operator/spark-operator --set spark.jobNamespaces={} --namespace spark-operator . Apply Service Accounts, Roles, Secrets, and Serving Runtime . The manifests/ directory contains several YAML files that set up the necessary service accounts, permissions, secrets, and runtime configuration for both KServe and Spark jobs. Here’s what each file does: . kserve-sa.yaml: Creates a service account for KServe, referencing the MinIO secret. . | kserve-minio-secret.yaml: Creates a secret with MinIO credentials and endpoint info, so KServe can access models and artifacts in MinIO. . | kserve-role.yaml: Defines a ClusterRole allowing management of KServe InferenceService resources. . | kserve-role-binding.yaml: Binds the above ClusterRole to the pipeline-runner service account in the kubeflow namespace, so pipeline steps can create/manage inference services. . | serving-runtime.yaml: Registers a custom ServingRuntime for ONNX models, specifying the container image and runtime configuration for model serving. . | spark-sa.yaml: Creates a service account for Spark jobs in the kubeflow namespace. . | spark-role.yaml: Defines a Role granting Spark jobs permissions to manage pods, configmaps, services, secrets, PVCs, and SparkApplication resources in the namespace. . | spark-role-binding.yaml: Binds the above Role to both the spark and pipeline-runner service accounts in the kubeflow namespace. . | kustomization.yaml: A Kustomize manifest that groups all the above resources for easy application. . | . Apply all of these with: . kubectl apply -k ./manifests -n kubeflow . These resources ensure that KServe and Spark jobs have the right permissions and configuration to run in your Kubeflow environment. . Building and Understanding the Pipeline Images . In Kubeflow Pipelines, each step of a pipeline runs inside a container. This containerized approach provides several key benefits: isolation between steps, reproducible environments, and the ability to use different runtime requirements for different stages of your pipeline. . While Kubeflow Pipelines provides default images for common tasks, most real-world ML projects require custom images tailored to their specific needs. Each pipeline component in this project uses a specialized container image that includes the necessary dependencies, libraries, and code to execute that particular step of the ML workflow. . This section covers how to build these custom images. For detailed information about what each image does and how the code inside each container works, refer to the individual pipeline step sections that follow. . Note: You only need to build and push these images if you want to modify the code for any of the pipeline components. If you’re using the project as-is, you can use the prebuilt images referenced in the pipeline. . The pipeline uses custom container images for the following components: . Image Locations . data_preparation/Containerfile . | feature_engineering/Containerfile . | pipeline/Containerfile . | rest_predictor/Containerfile . | train/Containerfile . | . How to Build . You can build each image using Podman or Docker. For example, to build the data preparation image: . cd data_preparation podman build -t fraud-detection-e2e-demo-data-preparation:latest . # or # docker build -t fraud-detection-e2e-demo-data-preparation:latest . . You can also refer to the build_images.sh script in the project root to see how to build all images in sequence. . Repeat this process for each component, adjusting the tag and directory as needed. . Entry points . data_preparation: python main.py | feature_engineering: python feast_feature_engineering.py | pipeline: Used for orchestrating the pipeline steps (see fraud-detection-e2e.py) | rest_predictor: python predictor.py | train: python train.py | . Pushing Images . After building, push the images to a container registry accessible by your Kubernetes cluster. Update the image references in your pipeline as needed. . The Kubeflow Pipeline . The main pipeline definition is in pipeline/fraud-detection-e2e.py. This file is the entrypoint for the Kubeflow pipeline and orchestrates all the steps described below. . With your environment and permissions set up, you’re ready to run the end-to-end pipeline. Let’s walk through each stage of the workflow and see how Kubeflow orchestrates the entire machine learning lifecycle—from data preparation to real-time inference. . 1. Data Preparation with Spark . Apache Spark is a powerful open source engine for large-scale data processing and analytics. In this project, we use Spark to efficiently process and transform raw transaction data before it enters the ML pipeline. . To run Spark jobs on Kubernetes, we use the Kubeflow Spark Operator. The Spark Operator makes it easy to submit and manage Spark applications as native Kubernetes resources, enabling scalable, distributed data processing as part of your MLOps workflow. . Container Image for Data Preparation . This pipeline step uses a custom container image built from data_preparation/Containerfile. The image includes: . PySpark and dependencies: Required libraries for distributed data processing | MinIO client libraries: For reading from and writing to object storage | Custom data processing code: The main.py script that implements the data transformation logic | . The container runs with the entry point python main.py, which orchestrates all the data preparation tasks within the Spark job. . The pipeline begins by launching a Spark job that performs several key data preparation steps, implemented in data_preparation/main.py: . Combining Datasets . The job reads the raw train.csv, test.csv, and validate.csv datasets, adds a set column to each, and combines them: . train_set = spark.read.csv(INPUT_DIR + &quot;train.csv&quot;, header=True, inferSchema=True) test_set = spark.read.csv(INPUT_DIR + &quot;test.csv&quot;, header=True, inferSchema=True) validate_set = spark.read.csv(INPUT_DIR + &quot;validate.csv&quot;, header=True, inferSchema=True) train_set = train_set.withColumn(&quot;set&quot;, lit(&quot;train&quot;)) test_set = test_set.withColumn(&quot;set&quot;, lit(&quot;test&quot;)) validate_set = validate_set.withColumn(&quot;set&quot;, lit(&quot;valid&quot;)) all_sets = train_set.unionByName(test_set).unionByName(validate_set) . Type Conversion and Feature Engineering . It converts certain columns to boolean types and generates unique IDs: . all_sets = all_sets.withColumn(&quot;fraud&quot;, col(&quot;fraud&quot;) == 1.0) all_sets = all_sets.withColumn(&quot;repeat_retailer&quot;, col(&quot;repeat_retailer&quot;) == 1.0) all_sets = all_sets.withColumn(&quot;used_chip&quot;, col(&quot;used_chip&quot;) == 1.0) all_sets = all_sets.withColumn(&quot;used_pin_number&quot;, col(&quot;used_pin_number&quot;) == 1.0) all_sets = all_sets.withColumn(&quot;online_order&quot;, col(&quot;online_order&quot;) == 1.0) w = Window.orderBy(lit(1)) all_sets = ( all_sets .withColumn(&quot;idx&quot;, row_number().over(w)) .withColumn(&quot;user_id&quot;, concat(lit(&quot;user_&quot;), col(&quot;idx&quot;) - lit(1))) .withColumn(&quot;transaction_id&quot;, concat(lit(&quot;txn_&quot;), col(&quot;idx&quot;) - lit(1))) .drop(&quot;idx&quot;) ) . Timestamping . The job adds created and updated timestamp columns: . for date_col in [&quot;created&quot;, &quot;updated&quot;]: all_sets = all_sets.withColumn(date_col, current_timestamp()) . Point-in-Time Feature Calculation . Using the raw transaction history, the Spark job calculates features such as the number of previous transactions, average/max/stddev of previous transaction amounts, and days since the last/first transaction: . def calculate_point_in_time_features(label_dataset: DataFrame, transactions_df: DataFrame) -&gt; DataFrame: # ... (see full code in data_preparation/main.py) # Aggregates and joins features for each user at each point in time . Output . The final processed data is saved as both a CSV (for entity definitions) and a Parquet file (for feature storage) in MinIO: . entity_df.write.option(&quot;header&quot;, True).mode(&quot;overwrite&quot;).csv(entity_file_name) df.write.mode(&quot;overwrite&quot;).parquet(parquet_file_name) . All of this logic is orchestrated by the prepare_data component in the pipeline, which launches the Spark job on Kubernetes. . 2. Feature Engineering with Feast . Feast is an open source feature store that lets you manage and serve features for both training and inference, ensuring consistency and reducing the risk of training/serving skew. In machine learning, a “feature” is an individual measurable property or characteristic of the data being analyzed—in our fraud detection case, features include transaction amounts, distances from previous transactions, merchant types, and user behavior patterns that help the model distinguish between legitimate and fraudulent activity. . Container Image for Feature Engineering . This pipeline step uses a custom container image built from feature_engineering/Containerfile. The image includes: . Feast feature store: The complete Feast installation for feature management | Python dependencies: Required libraries for feature processing and materialization | Feature repository definition: The repo_definition.py file that defines the feature views and entities | MinIO client libraries: For uploading the materialized features and online store to object storage | . The container runs with the entry point python feast_feature_engineering.py, which handles the Feast operations including applying feature definitions, materializing features, and uploading the results to MinIO. . After data preparation, the pipeline uses Feast to register, materialize, and store features for downstream steps. This process starts with defining the features you want to use. For example, in feature_repo/repo_definition.py, you’ll find a FeatureView that lists features like distance_from_home and ratio_to_median_purchase_price: . transactions_fv = FeatureView( name=&quot;transactions&quot;, entities=[transaction], schema=[ Field(name=&quot;user_id&quot;, dtype=feast.types.String), Field(name=&quot;distance_from_home&quot;, dtype=feast.types.Float32), Field(name=&quot;ratio_to_median_purchase_price&quot;, dtype=feast.types.Float32), # ... other features ], online=True, source=transaction_source, ) . Once the features are defined, the pipeline runs two key Feast commands. First, it applies the feature definitions to the store: . subprocess.run([&quot;feast&quot;, &quot;apply&quot;], cwd=feature_repo_path, check=True) . Then, it materializes the computed features from the Parquet file into Feast’s online store, making them available for real-time inference: . subprocess.run([&quot;feast&quot;, &quot;materialize&quot;, start_date, end_date], cwd=feature_repo_path, check=True) . Finally, the resulting feature data and the online store database are uploaded to MinIO, so they’re accessible to the rest of the pipeline: . client.fput_object(MINIO_BUCKET, object_path, local_file_path) . By using Feast in this way, you ensure that the same features are available for both model training and real-time predictions, making your ML workflow robust and reproducible. . 3. Model Training . With the features materialized in Feast, the next step is to train the fraud detection model. The pipeline’s train_model component retrieves the processed features and prepares them for training. The features used include behavioral and transaction-based signals such as distance_from_last_transaction, ratio_to_median_purchase_price, used_chip, used_pin_number, and online_order. . Container Image for Model Training . This pipeline step uses a custom container image built from train/Containerfile. The image includes: . Machine learning libraries: TensorFlow/Keras for neural network training, scikit-learn for data preprocessing | ONNX Runtime: For converting and exporting the trained model to ONNX format | PySpark: For loading and processing the feature data from Parquet files | MinIO client libraries: For downloading features and uploading the trained model artifacts | . The container runs with the entry point python train.py. . The training script loads the features, splits the data into train, validation, and test sets, and scales the input features for better model performance: . train_features = features.filter(features[&quot;set&quot;] == &quot;train&quot;) validate_features = features.filter(features[&quot;set&quot;] == &quot;valid&quot;) test_features = features.filter(features[&quot;set&quot;] == &quot;test&quot;) # ... select and scale features ... . It then builds and trains a neural network model using Keras, handling class imbalance and exporting the trained model in ONNX format for portable, high-performance inference: . model = build_model(feature_indexes) model.fit(x_train, y_train, epochs=2, validation_data=(x_val, y_val), class_weight=class_weights) save_model(x_train, model, model_path) # Exports to ONNX . By structuring the training step this way, the pipeline ensures that the model is trained on the same features that will be available at inference time, supporting a robust and reproducible MLOps workflow. . 4. Model Registration . Once the model is trained, it’s important to track, version, and manage it before deploying to production. This is where the Kubeflow Model Registry comes in. The Model Registry acts as a centralized service for managing machine learning models and their metadata, making it easier to manage deployments, rollbacks, and audits. . Container Image for Model Registration . This pipeline step uses a custom container image built from pipeline/Containerfile. The image includes: . Kubeflow Pipelines SDK: For pipeline orchestration and component definitions | Model Registry client: Python libraries for interacting with the Kubeflow Model Registry | Pipeline orchestration code: The core pipeline definition and component functions | . The container is used as the base image for the register_model component, which executes the model registration logic inline within the pipeline definition. This approach allows the registration step to run lightweight operations without requiring a separate, specialized container image. . In the pipeline, the register_model component takes the trained model artifact and registers it in the Model Registry. This process includes: . Assigning a unique name and version: The model is registered with a name (e.g., &quot;fraud-detection&quot;) and a version, which is typically tied to the pipeline run ID for traceability. | Storing metadata: Along with the model artifact, metadata such as the model format, storage location, and additional tags or descriptions can be stored for governance and reproducibility. | Making the model discoverable: Registered models can be easily found and referenced for deployment, monitoring, or rollback. | . Here’s how the registration step is implemented in the pipeline: . @dsl.component(base_image=PIPELINE_IMAGE) def register_model(model: Input[Model]) -&gt; NamedTuple(&#39;outputs&#39;, model_name=str, model_version=str): from model_registry import ModelRegistry registry = ModelRegistry( server_address=&quot;http://model-registry-service.kubeflow.svc.cluster.local&quot;, port=8080, author=&quot;fraud-detection-e2e-pipeline&quot;, user_token=&quot;non-used&quot;, is_secure=False ) model_name = &quot;fraud-detection&quot; model_version = &quot;&quot; registry.register_model( name=model_name, uri=model.uri, version=model_version, model_format_name=&quot;onnx&quot;, model_source_class=&quot;pipelinerun&quot;, model_source_group=&quot;fraud-detection&quot;, model_source_id=&quot;&quot;, model_source_kind=&quot;kfp&quot;, model_source_name=&quot;fraud-detection-e2e-pipeline&quot;, ) return (model_name, model_version) . By registering the model in this way, you ensure that every model deployed for inference is discoverable, reproducible, and governed—an essential part of any production-grade MLOps workflow. . 5. Real-Time Inference with KServe . The final stage of the pipeline is deploying the registered model as a real-time inference service using KServe. KServe is an open source model serving platform for Kubernetes that standardizes how you deploy, scale, and manage machine learning models in production. . Container Image for Real-Time Inference . This pipeline step uses a custom container image built from rest_predictor/Containerfile. The image includes: . KServe Python SDK: For building custom model serving endpoints | ONNX Runtime: For running the trained model in ONNX format | Feast feature store client: For retrieving real-time features during inference | Model Registry client: For downloading the registered model artifacts | Custom predictor code: The predictor.py script that implements the inference logic | . The container runs with the entry point python predictor.py. . The pipeline’s serve component creates a KServe InferenceService using this custom Python predictor. . This is done by creating a Kubernetes custom resource (CR) of kind InferenceService, which tells KServe how to deploy and manage the model server. The resource specifies the container image, command, arguments, and service account to use for serving the model. . Here’s how the InferenceService is defined and created in the pipeline: . inference_service = kserve.V1beta1InferenceService( api_version=kserve.constants.KSERVE_GROUP + &quot;/v1beta1&quot;, kind=&quot;InferenceService&quot;, metadata=client.V1ObjectMeta( name=model_name + &quot;-&quot; + job_id, namespace=kserve.utils.get_default_target_namespace(), labels={ &quot;modelregistry/registered-model-id&quot;: model.id, &quot;modelregistry/model-version-id&quot;: model_version.id }, ), spec=kserve.V1beta1InferenceServiceSpec( predictor=kserve.V1beta1PredictorSpec( service_account_name=&quot;kserve-sa&quot;, containers=[ V1Container( name=&quot;inference-container&quot;, image=rest_predictor_image, command=[&quot;python&quot;, &quot;predictor.py&quot;], args=[&quot;--model-name&quot;, model_name, &quot;--model-version&quot;, model_version_name] ) ] ) ), ) ks_client = kserve.KServeClient() ks_client.create(inference_service) . The custom predictor does more than just run the model: it also integrates directly with the Feast online feature store. When a prediction request arrives with a user_id, the predictor first fetches the user’s latest features from Feast and then feeds them to the ONNX model for inference. Here’s a simplified view of the predictor’s logic: . class ONNXModel(kserve.Model): def load(self): # ... download model and initialize Feast feature store ... self.feature_store = FeatureStore(repo_path=feature_repo_path) self.model = ort.InferenceSession(&quot;/app/model&quot;) self.ready = True async def predict(self, payload: Dict) -&gt; Dict: user_id = payload.get(&quot;user_id&quot;) feature_dict = self.feature_store.get_online_features( entity_rows=[{&quot;user_id&quot;: user_id}], features=features_to_request, ).to_dict() input_data = np.array([ [ feature_dict[&quot;distance_from_last_transaction&quot;][0], feature_dict[&quot;ratio_to_median_purchase_price&quot;][0], feature_dict[&quot;used_chip&quot;][0], feature_dict[&quot;used_pin_number&quot;][0], feature_dict[&quot;online_order&quot;][0], ] ], dtype=np.float32) result = self.model.run(None, {self.model.get_inputs()[0].name: input_data}) return {&quot;user_id&quot;: user_id, &quot;prediction&quot;: result[0].tolist()} . Note: By default, KServe supports several model serving runtimes, including Triton Inference Server (often used via the kserve-tritonserver runtime). However, the official Triton server does not support macOS/arm64, which is why this project uses a custom Python predictor for local development and demonstration. If you are running on a supported platform (such as x86_64 Linux), you may want to use the kserve-tritonserver runtime for production workloads, as it offers high performance and native ONNX support. If you want to use Feast for online feature retrieval at inference time, a custom Python predictor (like the one in this repo) is the most straightforward approach. If you use the standard kserve-tritonserver runtime, you would need to implement feature fetching as a Triton Python backend or as a pre-processing step outside of Triton, since Triton itself does not natively integrate with Feast. . By structuring the inference step this way, the pipeline ensures that the deployed model always uses the freshest features for each prediction, supporting robust, real-time fraud detection. . Importing and Running the Pipeline . Once your environment is set up and the data is uploaded, you’re ready to run the pipeline. . Import the Pipeline . Open the Kubeflow Pipelines UI (usually at http://localhost:8080 if you used the default port-forward). | Click Pipelines in the sidebar, then click Upload pipeline. | Upload the compiled pipeline YAML file (e.g., pipeline/fraud-detection-e2e.yaml). | Run the Pipeline . After uploading, click on your pipeline in the list. | Click Create run. | Optionally customize the run name and description (the defaults work fine), then click Start. | You can monitor the progress and view logs for each step directly in the UI. . Testing the Live Endpoint . With the inference service running, you can now interact with your deployed model in real time. Let’s see how to send prediction requests and interpret the results. . Before sending requests, port-forward the inference pod so the service is accessible locally. Run this command in a separate terminal window: . kubectl -n kubeflow get pods -l component=predictor -o jsonpath=&quot;{.items[*].metadata.name}&quot; | tr &#39; &#39; &#39; n&#39; | grep &#39;^fraud-detection&#39; | head -n1 | xargs -I {} kubectl port-forward -n kubeflow pod/{} 8081:8080 . With the port-forward active, you can now send a request to the model: . curl -X POST http://localhost:8081/v1/models/onnx-model:predict -H &quot;Content-Type: application/json&quot; -d &#39;{&quot;user_id&quot;: &quot;user_0&quot;}&#39; . The service retrieves features for user_0, runs a prediction, and returns the fraud probability. . {&quot;user_id&quot;:&quot;user_0&quot;,&quot;prediction&quot;:[[0.8173668384552002]]} . Note: The result of the prediction may vary depending on the initial raw data you uploaded. Try sending requests with a few different user_id values (e.g., &quot;user_1&quot;, &quot;user_2&quot;, etc.) to see how the predictions change. . Conclusion . This post has walked you through a complete, reproducible AI/ML workflow—from raw data to a live model serving endpoint—using Kubeflow and open source tools. Along the way, you’ve seen how to prepare data with Spark, manage features with Feast, train and register models, and deploy real-time inference services with KServe, all orchestrated in a portable pipeline you can run on your own laptop. . By following this blueprint, you can adapt and extend the process for your own machine learning projects, whether you’re working locally or scaling up to production. Kubeflow’s modular platform and ecosystem make it possible to manage the entire ML lifecycle in a consistent, automated, and open way. . Ready to try it yourself? The complete source code for this project is available on GitHub. .",
            "url": "https://blog.kubeflow.org/fraud-detection-e2e/",
            "relUrl": "/fraud-detection-e2e/",
            "date": " • Jul 15, 2025"
        }
        
    
  
    
        ,"post1": {
            "title": "Kubeflow 1.10 Release Announcement",
            "content": "Kubeflow 1.10.0 delivers essential updates that enhance the flexibility, efficiency, and scalability of machine learning workflows. The new features span across several components, improving both user experience and system performance. . Highlight features . Trainer 2.0 | New UI for Model Registry | Spark Operator as a core Kubeflow component | Kubernetes and container security (CISO compatibility) | Hyperparameter Optimization for LLMs Fine-Tuning | Loop parallelism in Pipelines | New parameter distributions for Katib | Deeper Model Registry integrations with KServe | New Python SDK, OCI storage, and model caching for KServe | New security contexts and rootless Istio-CNI integrations for Spark Operator | . Kubeflow Platform (Manifests &amp; Security) . The Kubeflow Platform Working Group focuses on simplifying Kubeflow installation, operations, and security. See details below. . Manifests: . Spark Operator 2.1.0 included in Kubeflow platform, although not installed yet by default | Documentation updates that make it easier to install, extend and upgrade Kubeflow | For more details and future plans please consult the 1.10.0 and 1.10.1/1.11.0 milestones | . Notebooks Dashboard Pipelines Katib Trainer KServe Model Registry Spark . 1.10 | 1.10 | 2.4.1 | 0.18 | 1.9 | 0.14 | 0.2.15 | 2.1.0 | . Kubernetes Kind Kustomize Cert Manager Knative Istio Dex Oauth2-proxy . 1.31-1.33 | 0.26 | 5.4.3 | 1.16.1 | 1.16 | 1.24 | 2.41 | 7.7 | . Security: . CVE reductions - regular scanning with trivy | Kubernetes and container security best practices: Rootless containers / PodSecurityStandards restricted for: Istio-CNI, Knative, Dex, Oauth2-proxy, Spark | 50 % done: KFP, Notebooks / Workspaces, Katib, Trainer, Kserve, … | Istio-CNI as default for rootless Kubeflow postponed to 1.10.1 | . | OIDC-authservice has been replaced by oauth2-proxy | Oauth2-proxy and Dex documentation for external OIDC authentication (Keycloak, and OIDC providers such as Azure, Google etc.) | . Trivy CVE scans March 25 2025: . Working Group Images Critical CVE High CVE Medium CVE Low CVE . Katib | 17 | 11 | 101 | 417 | 734 | . Pipelines | 15 | 57 | 490 | 4030 | 1922 | . Workbenches(Notebooks) | 12 | 12 | 59 | 179 | 224 | . Kserve | 16 | 21 | 305 | 6803 | 1588 | . Manifests | 14 | 8 | 4 | 94 | 52 | . Trainer | 1 | 0 | 0 | 1 | 0 | . Model Registry | 6 | 1 | 13 | 153 | 188 | . Spark | 1 | 5 | 37 | 1640 | 141 | . All Images | 81 | 115 | 1009 | 13275 | 4804 | . Pipelines . Support for Placeholders in Resource Limits . Kubeflow Pipelines 2.4.1 introduces support for placeholders in resource limits, enhancing flexibility in pipeline execution.This update allows users to define dynamic resource limits using parameterized values, enabling more adaptable and reusable pipeline definitions. . Support for Loop Parallelism . Kubeflow Pipelines 2.4.1 introduces a new Parallelism Limit for ParallelFor tasks, giving users the ability to run massively parallel inference pipelines, with more control over parallel execution in their workflows. This feature allows users to specify the maximum number of parallel iterations, preventing resource overutilization and improving system stability. When running large pipelines with GPUs, proper use of this feature could save your team thousands of dollars in compute expenses. . Implement SubDAG Output Resolution . Kubeflow 1.10 ensures that pipelines using nested DAGs work correctly and reliably when treated as components. Outputs from deeply nested DAGs will now resolve properly, avoiding broken dependencies. . Model Registry . Model Registry introduces a new user interface and enhanced model management capabilities. . Model Registry UI . The new Kubeflow Model Registry UI provides a user-friendly web interface for managing machine learning models within the Kubeflow platform. It centralizes model metadata, version tracking, and artifact management, streamlining MLOps workflows. . Key features include: . Easy model registration with custom metadata | Comprehensive model management with filtering and sorting | Archiving capabilities | Version control | Metadata editing | . . The UI interacts with the Model Registry’s REST API, making it accessible to users of all technical backgrounds and enhancing collaboration across data science, ML engineering, and MLOps teams. . To get started with the Model Registry UI, which is currently in Alpha, you can follow the instructions here. . The Kubeflow Model Registry UI Team would like to conduct user research to identify possible enhancements we can contribute in future iterations of the Kubeflow Model Registry UI. If you are interested in participating in this study, please fill out this survey. . Custom Storage Initializer . The Model Registry Custom Storage Initializer (CSI) is a custom implementation of the KServe ClusterStorageContainer. This feature allows users to utilize Model Registry metadata to download and deploy models efficiently. With the newest release of the Model Registry, it is now possible to install and use the Custom Storage Initializer (CSI). . You can find detailed installation instructions and a small example in the “Getting Started” section of the Model Registry component on the Kubeflow website. . For additional information and future developments towards better integration with KServe, you can refer to the slides here. . Training Operator (Trainer) &amp; Katib . Kubeflow 1.10 enhances the Training Operator and Katib, providing new tools and APIs for hyperparameter optimization, particularly for large language models. . Moreover, the Kubeflow Training Operator now supports JAX for distributed training, enabling users to leverage JAX’s capabilities for efficient and scalable model training. . Finally, if you want to get involved with Trainer V2, take a look at this KEP and issue. . Hyperparameter Optimization API for LLMs . Katib introduces a new high-level API for hyperparameter tuning, streamlining LLMOps workflows in Kubernetes. This API integrates Katib and the Training Operator to automate hyperparameter optimization, reducing manual effort for data scientists fine-tuning large language models. For more information, refer to the feature release blog post. . Support for Various Parameter Distributions . Katib now adds support for multiple probability distributions. Previously limited to uniform distributions, Katib now supports log-uniform, normal, and log-normal distributions, providing data scientists with greater flexibility in tuning hyperparameters. This is particularly useful for parameters like learning rates, which benefit from log-uniform sampling, or values expected to vary around a mean, suited for normal distributions. . Push-Based Metrics Collection . Katib now allows users to push metrics to Katib DB directly. The new push-based design provides administrative and performanace improvements to the existing pull based design. For further details, please refer to the Push-Based Metrics Collection blog post. . Dashboard &amp; Notebooks . Kubeflow 1.10 improves the observability and usability of Notebooks, while providing updated default images. . Prometheus Metrics for Notebooks . Both the Notebooks component and CRUD backends now feature Prometheus metrics. Notebooks expose custom metrics using the prom-client library, and CRUD backends utilize the prometheus_flask_exporter library. This ensures consistent metrics integration across all backend services. . More Descriptive Error Messages . Error messages for notebook creation failures due to resource constraints are now more descriptive. Users can quickly identify issues such as insufficient resources. . Spark Operator . The Spark Operator, now integrated as a core Kubeflow component, includes several key enhancements focusing on architecture, security, and performance: . Rebuilt with Controller Runtime (v2.0.0): Modernized core architecture using controller-runtime, aligning with Kubernetes controller patterns for improved structure, extensibility, and testability. | YuniKorn Gang Scheduling Support (v2.0.0): Enables efficient scheduling of Spark driver &amp; executor pods as a group, ideal for large-scale data pipelines with resource guarantees. | Enhanced Security Contexts &amp; SeccompProfile Support (v2.1.1): Adds support for seccompProfile: RuntimeDefault &amp; readOnlyRootFilesystem, aligning with Kubernetes Pod Security Standards and minimizing security risk. | . KServe . KServe v0.14.1 introduces several essential features that enhance its capabilities for deploying and managing machine learning models. . New Python SDK . The release includes a new Python SDK with both REST and GRPC inference clients, offering asynchronous support and the ability to handle tensor data in binary format. . OCI Storage for Models . OCI storage for models has also been promoted to a stable feature, with improvements to stability by configuring OCI models as init containers. . Model Cache Feature . Additionally, the introduction of the Model Cache feature leverages local node storage to reduce model load times, especially for large models, enhancing scalability. . Hugging Face Integration . KServe v0.14.1 further expands integration with Hugging Face, enabling direct model deployment from the Hugbing Face hub via a new hf:// URI schema. . What comes next? . If you want to take a peek into the Kubeflow 1.11 roadmap planning and contribute with your ideas, see Notebooks, Manifests &amp; Security, Pipelines, Model Registry, Katib, Training Operator. . How to get started with 1.10 . Visit the Kubeflow 1.10 release page or head over to the Getting Started and Support pages. . Join the Community . We would like to thank everyone for the contribution to Kubeflow 1.10, especially Ricardo Martinelli De Oliveira for his work as the v1.10 Release Manager, all the release team and the working group leads, who relentlessly dedicate their time to this great project. . Release team members : Ricardo Martinelli De Oliveira, Dimitris Poulopoulos, Matteo Mortari, Julius von Kohout Valentina Rodriguez Sosa, Helber Belmiro, Vraj Bhatt, Diego Lovison, Dagvanorov Lkhagvajav, Sailesh Duddupudi, Manos Vlassis, Tarek Abouzeid, Milos Grubjesic . Working Group leads : Andrey Velichkevich, Julius von Kohout, Mathew Wicks, … . Kubeflow Steering Committee : Andrey Velichkevich, Julius von Kohout, Yuan Tang, Johnu George, Francisco Javier Araceo . Participating Distributions : Charmed Kubeflow (Canonical), Nutanix, OpenShift AI (RedHat), QBO . You can find more details about Kubeflow distributions here. . Want to help? . The Kubeflow community Working Groups hold open meetings and are always looking for more volunteers and users to unlock the potential of machine learning. If you’re interested in becoming a Kubeflow contributor, please feel free to check out the resources below. We look forward to working with you! . Visit our Kubeflow website or Kubeflow GitHub Page. | Join the Kubeflow Slack channel. | Join the kubeflow-discuss mailing list. | Attend our weekly community meeting. | .",
            "url": "https://blog.kubeflow.org/kubeflow-1.10-release/",
            "relUrl": "/kubeflow-1.10-release/",
            "date": " • Mar 26, 2025"
        }
        
    
  
    
        ,"post2": {
            "title": "🚀 Announcing the Kubeflow Spark Operator Benchmarking Results",
            "content": "Kubernetes has become the go-to platform for running large-scale Apache Spark workloads. But as workloads scale, how do you ensure your Spark jobs run efficiently without hitting bottlenecks? Managing thousands of concurrent Spark jobs can introduce severe performance challenges—from CPU saturation in the Spark Operator to Kubernetes API slowdowns and job scheduling inefficiencies. . To address these challenges, we are excited to introduce the Kubeflow Spark Operator Benchmarking Results and Toolkit—a comprehensive framework to analyze performance, pinpoint bottlenecks, and optimize your Spark on Kubernetes deployments. . 🔍 What’s Included? . This benchmarking effort provides three key outcomes to help you take full control of your Spark on Kubernetes deployment: . ✅ Benchmarking Results – A detailed evaluation of performance insights and tuning recommendations for large-scale Spark workloads. 🛠 Benchmarking Test Toolkit – A fully reproducible test suite to help users evaluate their own Spark Operator performance and validate improvements. 📊 Open-Sourced Grafana Dashboard – A battle-tested visualization tool designed specifically to track large-scale Spark Operator deployments, providing real-time monitoring of job processing efficiency, API latencies, and system health. . ❌ The Challenges: Why Benchmarking Matters . Running thousands of Spark jobs on Kubernetes at scale uncovers several performance roadblocks that can cripple efficiency if left unresolved: . 🚦 Spark Operator Becomes CPU-Bound: When handling thousands of Spark jobs, the controller pod maxes out CPU resources, limiting job submission rates. | 🐢 High API Server Latency: As workloads scale, Kubernetes API responsiveness degrades—job status updates slow down, affecting observability and scheduling efficiency. | 🕒 Webhook Overhead Slows Job Starts: Using webhooks adds ~60 seconds of extra latency per job, reducing throughput in high-concurrency environments. | 💥 Namespace Overload Causes Failures: Running 6,000+ SparkApplications in a single namespace resulted in pod failures due to excessive environment variables and service object overload. | . 💡 So, how do you fix these issues and optimize your Spark Operator deployment? That’s where our benchmarking results and toolkit come in. . 🛠 Tuning Best Practices for Spark Operator . Based on our benchmarking findings, we provide clear, actionable recommendations for improving Spark Operator performance at scale. . If you’re running thousands of concurrent Spark jobs, here’s what you need to do: . Deploy Multiple Spark Operator Instances . 💡 Why? A single Spark Operator instance struggles to keep up with high job submission rates. ✅ Solution: When a single Spark Operator instance struggles with high job submission rates, leading to CPU saturation and slower job launches, deploying multiple instances can help. Distribute the workload by assigning different namespaces to each instance. For example, one instance can manage `20 namespaces while another handles a separate set of 20 namespaces. This prevents bottlenecks and ensures efficient Spark job execution. . Disable Webhooks for Faster Job Starts . 💡 Why? Webhooks introduce ~60 seconds of delay per job due to validation and mutation overhead, reducing throughput in large workloads. ✅ Solution: Instead of using webhooks for volume mounts, node selectors, or taints, define Spark Pod Templates directly within the Spark job definition—no additional files are needed. Disable webhooks by setting webhook.enable=false in the Helm chart. . Increase Controller Workers . 💡 Why? By default, the operator runs with 10 controller workers, but our benchmarks showed increasing this to 20 or 30 workers improved job throughput. ✅ Solution: Set controller.workers=20 if your Operator pod runs on a 36-core CPU or higher to enable faster parallel job execution. For larger workloads (e.g., 72+ cores), increase to 40+ workers for better parallel job execution. . Enable a Batch Scheduler (Volcano / YuniKorn) . 💡 Why? Kubernetes’ default scheduler isn’t optimized for batch workloads, leading to inefficient job placements. ✅ Solution: Enable Volcano or YuniKorn (batchScheduler.enable=true) to optimize job scheduling. These schedulers provide gang scheduling, queue management, and multi-tenant resource sharing. Benchmarks show that Apache YuniKorn schedules jobs faster than the default Kubernetes scheduler. . Optimize API Server Scaling . 💡 Why? API server latency spikes to 600ms+ under heavy load, affecting Spark job responsiveness. ✅ Solution: Scale API server replicas, allocate more CPU and memory, and optimize event handling. Ensure your Kubernetes API server and etcd auto-scale to handle bursty workloads efficiently. Monitor kube-apiserver metrics and scale etcd accordingly. If running thousands of Spark pods, consider manually increasing control plane node sizes. . Distribute Spark Jobs Across Multiple Namespaces . 💡 Why? Running too many jobs in a single namespace causes environment variable overflows, leading to pod failures. ✅ Solution: When too many pods are placed in a single namespace, operations like listing or modifying resources can generate large API server responses, increasing latency. For example, retrieving all pods may result in a substantial size in response, consuming significant server resources. Additionally, etcd, Kubernetes’ key-value store, can become a bottleneck when handling frequent updates from a high number of pods in one namespace. Heavy read and write operations can strain etcd, causing increased latencies and potential timeouts. To improve performance and stability, it is recommended to distribute workloads across multiple namespaces. . Monitor &amp; Tune Using the Open-Source Grafana Dashboard . 💡 Why? Observability is key to identifying performance bottlenecks. ✅ Solution: Use our Spark Operator Scale Test Dashboard to track job submission rates, API latencies, and CPU utilization in real time. . 📖 Learn More &amp; Get Started . The Kubeflow Spark Operator Benchmarking Results and Toolkit provide an in-depth performance playbook for running Spark at scale on Kubernetes. Whether you’re troubleshooting an existing deployment or planning for future growth, this toolkit arms you with data-driven insights and best practices for success. . 🚀 Ready to optimize your Spark workloads? Dive into the full results and toolkit below: 📖 Kubeflow Spark Operator Benchmarks .",
            "url": "https://blog.kubeflow.org/operators/benchmarking/performance/2025/03/15/kubeflow-spark-operator-benchmarks.html",
            "relUrl": "/operators/benchmarking/performance/2025/03/15/kubeflow-spark-operator-benchmarks.html",
            "date": " • Mar 15, 2025"
        }
        
    
  
    
        ,"post3": {
            "title": "Optimizing RAG Pipelines with Katib: Hyperparameter Tuning for Better Retrieval & Generation",
            "content": "Introduction . As artificial intelligence and machine learning models become more sophisticated, optimising their performance remains a critical challenge. Kubeflow provides a robust component, Katib, designed for hyperparameter optimization and neural architecture search. As a part of the Kubeflow ecosystem, Katib enables scalable, automated tuning of underlying machine learning models, reducing the manual effort required for parameter selection while improving model performance across diverse ML workflows. . With Retrieval-Augmented Generation (RAG) becoming an increasingly popular approach for improving search and retrieval quality, optimizing its parameters is essential to achieving high-quality results. RAG pipelines involve multiple hyperparameters that influence retrieval accuracy, hallucination reduction, and language generation quality. In this blog, we will explore how Katib can be leveraged to fine-tune a RAG pipeline, ensuring optimal performance by systematically adjusting key hyperparameters. . Let’s Get Started! . STEP 1: Setup . Since compute resources are scarcer than a perfectly labeled dataset :), we’ll use a lightweight Kind cluster (Kubernetes in Docker) cluster to run this example locally. Rest assured, this setup can seamlessly scale to larger clusters by increasing the dataset size and the number of hyperparameters to tune. . To get started, we’ll first install the Katib control plane in our cluster by following the steps outlined in the documentation. . STEP 2: Implementing RAG pipeline . In this implementation, we use a retriever model, which encodes queries and documents into vector representations to find the most relevant matches, to fetch relevant documents based on a query and a generator model to produce coherent text responses. . Implementation Details: . Retriever: Sentence Transformer &amp; FAISS (Facebook AI Similarity Search) Index A SentenceTransformer model (paraphrase-MiniLM-L6-v2) encodes predefined documents into vector representations. | FAISS is used to index these document embeddings and perform efficient similarity searches to retrieve the most relevant documents. | . | Generator: Pre-trained GPT-2 Model A Hugging Face GPT-2 text generation pipeline (which can be replaced with any other model) is used to generate responses based on the retrieved documents. I chose GPT-2 for this example as it is lightweight enough to run on my local machine while still generating coherent responses. | . | Query Processing &amp; Response Generation When a query is submitted, the retriever encodes it and searches the FAISS index for the top-k most similar documents. | These retrieved documents are concatenated to form the input context, which is then passed to the GPT-2 model to generate a response. | . | Evaluation: BLEU (Bilingual Evaluation Understudy) Score Calculation To assess the quality of generated responses, we use the BLEU score, a popular metric for evaluating text generation. | The evaluate function takes a query, retrieves documents, generates a response, and compares it against a ground-truth reference to compute a BLEU score with smoothing functions from the nltk library. | . | To run Katib, we will use the Katib SDK, which provides a programmatic interface for defining and running hyperparameter tuning experiments in Kubeflow. . Katib requires an objective function, which: . Defines what we want to optimize (e.g., BLEU score for text generation quality). | Executes the RAG pipeline with different hyperparameter values. | Returns an evaluation metric so Katib can compare different hyperparameter configurations. | def objective(parameters): # Import dependencies inside the function (required for Katib) import numpy as np import faiss from sentence_transformers import SentenceTransformer from transformers import pipeline from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction # Function to fetch documents (Modify as needed) def fetch_documents(): &quot;&quot;&quot;Returns a predefined list of documents or loads them from a file.&quot;&quot;&quot; return [ ... ] # OR, to load from a file: # with open(&quot;/path/to/documents.json&quot;, &quot;r&quot;) as f: # return json.load(f) # Define the RAG pipeline within the function def rag_pipeline_execute(query, top_k, temperature): &quot;&quot;&quot;Retrieves relevant documents and generates a response using GPT-2.&quot;&quot;&quot; # Initialize retriever retriever_model = SentenceTransformer(&quot;paraphrase-MiniLM-L6-v2&quot;) # Sample documents documents = fetch_documents() # Encode documents doc_embeddings = retriever_model.encode(documents) index = faiss.IndexFlatL2(doc_embeddings.shape[1]) index.add(np.array(doc_embeddings)) # Encode query and retrieve top-k documents query_embedding = retriever_model.encode([query]) distances, indices = index.search(query_embedding, top_k) retrieved_docs = [documents[i] for i in indices[0]] # Generate response using GPT-2 generator = pipeline(&quot;text-generation&quot;, model=&quot;gpt2&quot;, tokenizer=&quot;gpt2&quot;) context = &quot; &quot;.join(retrieved_docs) generated = generator(context, max_length=50, temperature=temperature, num_return_sequences=1) return generated[0][&quot;generated_text&quot;] # TODO: Provide queries and ground truth directly here or load them dynamically from a file/external volume. query = &quot;&quot; # Example: &quot;Tell me about the Eiffel Tower.&quot; ground_truth = &quot;&quot; # Example: &quot;The Eiffel Tower is a famous landmark in Paris.&quot; # Extract hyperparameters top_k = int(parameters[&quot;top_k&quot;]) temperature = float(parameters[&quot;temperature&quot;]) # Generate response response = rag_pipeline_execute(query, top_k, temperature) # Compute BLEU score reference = [ground_truth.split()] # Tokenized reference candidate = response.split() # Tokenized candidate response smoothie = SmoothingFunction().method1 bleu_score = sentence_bleu(reference, candidate, smoothing_function=smoothie) # Print BLEU score in Katib-compatible format print(f&quot;BLEU={bleu_score}&quot;) . Note: Make sure to return the result in the format of &lt;parameter&gt;=&lt;value&gt; for Katib’s metrics collector to be able to utilize it. More ways to configure the output are available in Katib Metrics Collector guide. . STEP 3: Run a Katib Experiment . Once our pipeline is encapsulated within the objective function, we can configure Katib to optimize the BLEU score by tuning the hyperparameters: . top_k: The number of documents retrieved (eg. between 10 and 20). | temperature: The randomness of text generation (eg. between 0.5 and 1.0). | Define hyperparameter search space . parameters = { &quot;top_k&quot;: katib.search.int(min=10, max=20), &quot;temperature&quot;: katib.search.double(min=0.5, max=1.0, step=0.1) } . Let’s submit the experiment! We’ll use the tune API that will run multiple trials to find the optimal top_k and temperature values for our RAG pipeline. . katib_client = katib.KatibClient(namespace=&quot;kubeflow&quot;) name = &quot;rag-tuning-experiment&quot; katib_client.tune( name=name, objective=objective, parameters=parameters, algorithm_name=&quot;grid&quot;, # Grid search for hyperparameter tuning objective_metric_name=&quot;BLEU&quot;, objective_type=&quot;maximize&quot;, objective_goal=0.8, max_trial_count=10, # Run up to 10 trials parallel_trial_count=2, # Run 2 trials in parallel resources_per_trial={&quot;cpu&quot;: &quot;1&quot;, &quot;memory&quot;: &quot;2Gi&quot;}, base_image=&quot;python:3.10-slim&quot;, packages_to_install=[ &quot;transformers==4.36.0&quot;, &quot;sentence-transformers==2.2.2&quot;, &quot;faiss-cpu==1.7.4&quot;, &quot;numpy==1.23.5&quot;, &quot;huggingface_hub==0.20.0&quot;, &quot;nltk==3.9.1&quot; ] ) . Once the experiment is submitted, we can see output indicating that Katib has started the trials: . Experiment Trials status: 0 Trials, 0 Pending Trials, 0 Running Trials, 0 Succeeded Trials, 0 Failed Trials, 0 EarlyStopped Trials, 0 MetricsUnavailable Trials Current Optimal Trial: {&#39;best_trial_name&#39;: None, &#39;observation&#39;: {&#39;metrics&#39;: None}, &#39;parameter_assignments&#39;: None} Experiment conditions: [{&#39;last_transition_time&#39;: datetime.datetime(2025, 3, 13, 19, 40, 32, tzinfo=tzutc()), &#39;last_update_time&#39;: datetime.datetime(2025, 3, 13, 19, 40, 32, tzinfo=tzutc()), &#39;message&#39;: &#39;Experiment is created&#39;, &#39;reason&#39;: &#39;ExperimentCreated&#39;, &#39;status&#39;: &#39;True&#39;, &#39;type&#39;: &#39;Created&#39;}] Waiting for Experiment: kubeflow/rag-tuning-experiment to reach Succeeded condition ..... Experiment Trials status: 9 Trials, 0 Pending Trials, 2 Running Trials, 7 Succeeded Trials, 0 Failed Trials, 0 EarlyStopped Trials, 0 MetricsUnavailable Trials Current Optimal Trial: {&#39;best_trial_name&#39;: &#39;rag-tuning-experiment-66tmh9g7&#39;, &#39;observation&#39;: {&#39;metrics&#39;: [{&#39;latest&#39;: &#39;0.047040418725887996&#39;, &#39;max&#39;: &#39;0.047040418725887996&#39;, &#39;min&#39;: &#39;0.047040418725887996&#39;, &#39;name&#39;: &#39;BLEU&#39;}]}, &#39;parameter_assignments&#39;: [{&#39;name&#39;: &#39;top_k&#39;, &#39;value&#39;: &#39;10&#39;}, {&#39;name&#39;: &#39;temperature&#39;, &#39;value&#39;: &#39;0.6&#39;}]} Experiment conditions: [{&#39;last_transition_time&#39;: datetime.datetime(2025, 3, 13, 19, 40, 32, tzinfo=tzutc()), &#39;last_update_time&#39;: datetime.datetime(2025, 3, 13, 19, 40, 32, tzinfo=tzutc()), &#39;message&#39;: &#39;Experiment is created&#39;, &#39;reason&#39;: &#39;ExperimentCreated&#39;, &#39;status&#39;: &#39;True&#39;, &#39;type&#39;: &#39;Created&#39;}, {&#39;last_transition_time&#39;: datetime.datetime(2025, 3, 13, 19, 40, 52, tzinfo=tzutc()), &#39;last_update_time&#39;: datetime.datetime(2025, 3, 13, 19, 40, 52, tzinfo=tzutc()), &#39;message&#39;: &#39;Experiment is running&#39;, &#39;reason&#39;: &#39;ExperimentRunning&#39;, &#39;status&#39;: &#39;True&#39;, &#39;type&#39;: &#39;Running&#39;}] Waiting for Experiment: kubeflow/rag-tuning-experiment to reach Succeeded condition . We can also see the experiments and trials being run to search for the optimized parameter: . kubectl get experiments.kubeflow.org -n kubeflow NAME TYPE STATUS AGE rag-tuning-experiment Running True 10m . kubectl get trials --all-namespaces NAMESPACE NAME TYPE STATUS AGE kubeflow rag-tuning-experiment-7wskq9b9 Running True 10m kubeflow rag-tuning-experiment-cll6bt4z Running True 10m kubeflow rag-tuning-experiment-hzxrzq2t Running True 10m . The list of completed trials and their results will be shown in the UI like below. Steps to access Katib UI are available in the documentation: . . Conclusion . In this experiment, we leveraged Kubeflow Katib to optimize a Retrieval-Augmented Generation (RAG) pipeline, systematically tuning key hyperparameters like top_k and temperature to enhance retrieval precision and generative response quality. . For anyone working with RAG systems or hyperparameter optimization, Katib is a powerful tool—enabling scalable, efficient, and intelligent tuning of machine learning models! We hope this tutorial helps you streamline hyperparameter tuning and unlock new efficiencies in your ML workflows! .",
            "url": "https://blog.kubeflow.org/katib/rag/",
            "relUrl": "/katib/rag/",
            "date": " • Feb 21, 2025"
        }
        
    
  
    
        ,"post4": {
            "title": "Synthetic Data Generation with Kubeflow Pipelines",
            "content": "Synthetic Data Generation - Why and How? . When creating insights, decisions, and actions from data, the best results come from real data. But accessing real data often requires lengthy security and legal processes. The data may also be incomplete, biased, or too small, and during early exploration, we may not even know if it’s worth pursuing. While real data is essential for proper evaluation, gaps or limited access frequently hinder progress until the formal process is complete. . To address these challenges, synthetic data provides an alternative. It mimics real data’s statistical properties while preserving privacy and accessibility. Synthetic data generators (synthesizers) are models trained on real data to generate new datasets that follow the same statistical distributions and relationships but do not contain real records. This allows for accelerated development, improved data availability, and enhanced privacy. . Depending on the technique used, synthetic data not only mirrors statistical base properties of real data but also preserves correlations between features. These synthesizers — such as those based on Gaussian Copulas, Generative Adversarial Networks (GANs), and Variational Autoencoders (VAEs) — enable the creation of high-fidelity synthetic datasets. See more description of these techniques below. . Key Benefits of Using Synthetic Data . While the above focuses on speed of development in general, and augmentation of data to improve performance of analytical modes, there are more motivations for creating (synthetic) data: . Enhanced Privacy and Security Mimics real datasets without containing sensitive or personally identifiable information, mitigating privacy risks and ensuring compliance with regulations like GDPR. . | Improved Data Availability Enables testing and training of models without requiring extensive real-world data collection. . | Innovation and Experimentation Allows safe experimentation with new algorithms and models without exposing sensitive data, fostering rapid prototyping in a secure environment. . | Ethical and Responsible AI Development Ensures training data is free from biases present in real-world datasets, promoting fair and unbiased AI systems. . | Accelerated Testing and Deployment Supports testing of new products, services, and systems in a controlled yet realistic setting, ensuring they are robust, scalable, and ready for real-world use. . | Cost Efficiency Reduces expenses related to data collection, storage, and compliance by eliminating the need for large-scale real-world data acquisition. . | Regulatory Compliance Simplification Helps organizations navigate complex data regulations by offering a compliant alternative to real-world datasets, easing cross-border data transfers. . | Balanced and Augmented Datasets Supplements real-world data by balancing underrepresented classes, improving model performance, and reducing biases in AI training. . | Resilience Against Data Scarcity Enables AI development in domains where real-world data is limited, expensive, or difficult to obtain—such as healthcare and cybersecurity—by generating high-quality alternative datasets. . | . To realize these benefits, we need effective tools for generating synthetic data. Different frameworks exist for this purpose, ranging from cloud-based platforms to open-source solutions. In this post, we focus on open-source synthetic data generation frameworks that provide control, flexibility, and on-premise deployment options. . Frameworks for Creating Synthetic Data . This post focuses exclusively on open source frameworks. Some data cannot be sent to the cloud, so some cloud-based synthetic data generation solutions are not always a good fit. For data already in cloud, we can use other cloud-based frameworks to generate synthetic data. . Synthesizers are motivated by multiple factors, but in this context, our focus remains on generating synthetic data for on-premise use. . So, what framework did we (initially) choose? Currently, we are using the open source version of SDV, an easy-to-use framework with a strong community and many useful features out-of-the-box (e.g. built-in evaluators, many modeling techniques). The field of synthetic data is evolving rapidly. While we do not aim to cover the latest advancements exhaustively, the use of Foundation models is certainly an area of interest. . One of the most widely used open-source libraries for synthetic data generation is Synthetic Data Vault (SDV). It provides multiple synthesizers, each tailored for different types of data and statistical properties. . The Synthetic Data Vault (SDV) . When you initialize and fit a synthesizer (like GaussianCopulaSynthesizer, CTGANSynthesizer, etc. - see below), it trains a model based on the dataset you provide. This model learns the distribution of the data, capturing the relationships and dependencies between different features in the dataset. The synthesizer doesn’t memorize individual records from the dataset. Instead, it tries to learn the underlying statistical patterns, correlations, and distributions present in the data. . Below are the (free) synthesizers provided by SDV that we evaluated on each use case. Each synthesizer does this differently: . GaussianCopulaSynthesizer: Uses statistical copula functions to model relationships between features, ensuring accurate marginal distributions. | CTGANSynthesizer: Uses Generative Adversarial Networks (GANs) to learn complex data distributions, particularly effective for categorical and mixed-type data. | TVAESynthesizer: Leverages Variational Autoencoders (VAEs) to capture latent representations, useful for continuous and structured data. | CopulaGANSynthesizer: Combines Copula-based statistical modeling with GANs to generate data with complex dependencies. | PARSynthesizer: Uses autoregressive models to generate sequential data while preserving temporal dependencies. | . There are more synthesizers, also from SDV, but not all are open source. We used the first four, when evaluating optimal synthesizer for our different use cases. . Generators - generating new data - on demand . Synthesizers are statistical and (more often) AI models trained to mimic the real data. Once developed, the resulting models are used to create as much synthetic data as you find useful for your use case. Once trained, the synthesizer uses the learned model to generate new synthetic data that follows the same statistical properties and distributions as the original dataset, without directly copying any real data points. If you need more data? Just call the generator. . Evaluation Criteria for Synthetic Data . But, how good is synthetic data, how do we evaluate it? . There are many aspects to consider when making use of synthetic data, and it is important to evaluate which synthetic data generation technique (synthesizer) is best for our specific dataset and use case. . We need to ensure a good balance between: . Usability – How useful is the synthetic data for the intended use case? | Fidelity – How well does the synthetic data preserve statistical properties of the real data? | Privacy – Does the generated data ensure an acceptable level of privacy for the given use case? | . For now, we are focusing only on usability and fidelity, using framework-provided measurements for fidelity and workflows described below to assess usability. . Comments on privacy and privacy preserving techniques . Ensuring privacy in synthetic data is a non-trivial problem, even if there are techniques to ensure levels of privacy, it remains an active area of research. . Privacy problems, in synthetic data? . While synthetic data enhances privacy by removing personally identifiable information, it is not inherently risk-free. Some key challenges include: . Overfitting and Memorization: If a synthesizer is overfitted, it may generate synthetic records that closely resemble real data, leading to privacy leakage. | Anomaly Exposure: Unique individuals or rare events in the dataset (e.g., a very wealthy individual or a rare disease) may be unintentionally replicated in synthetic data, creating a risk of re-identification. | Re-identification Attacks: Even if synthetic data is statistically different from real data, attackers may use background knowledge to infer sensitive details about individuals. | . One additional problem here is that it might be the anomalies we really are looking for. Currently we are experimenting with various differential privacy strategies, but it is still early days, and we do not focus on them in the examples below. . Our On-Premise Analytics Platform: ARCUS . ARCUS is Telia’s advanced on-premise analytics platform, designed to support a wide range of use cases. The platform provides a Kubeflow-based MLOps environment for descriptive, predictive, generative, and (ongoing) agentic AI. Fully built on open-source, ARCUS integrates a comprehensive stack of components into a unified platform - where Kubernetes is the cornerstone. . Needed environment to create synthetic data . For an efficient, automated selection of the best synthesizer, we need a number of things - from the underlying platform with GPUs and MLOps (Kubeflow). . Kubeflow pipelines | GPU capabilities (for performance and efficiency) | Development (IDE) environment (for framework building and running) | Modern data platform (MinIO, Airflow) automating the synthetic data generation datasets | . Parallelism needed . In the (Kube)flows below, we run evaluations in parallel - one for respective synthesizer, followed by a comparison of usability and fidelity scores, selecting the ‘winner’. . Note: In earlier version of Kubeflow we noticed that the parallelism wasn’t acting as expected, waiting for all threads to complete before moving to next step. We had to create a temporary workaround for this, now solved in Kubeflow Pipelines 2.3.0. . Below, we briefly describe the base flow for selecting synthesizer, followed by one use case where we use the resulting data generator for ML development in cloud. . Exploring the Creation and Usefulness of Synthetic Data . This is what we want to do: we have a use case, the supporting data, and developed ML model. . How similar is the synthetic data compared to the real data (interesting for e.g. visualization use cases)? | How well do the ML models based on synthetic data keep up with ML models based on real data? | . Validation of synthetic data techniques . Create the synthetic data and save the best synthetic data generator. In this step similarity measures are created by the out of the box SDV framework | Create the ML model (in our case classifier model) both on real data and the using the synthetic data. Compare the performance of both models against the same real data testset. | . From above, we have an example where the final synthesizer is collected and saved. This step is used in the example below, exporting the resulting synthetic data generator to cloud. . Using Synthetic Data Generators to Enable Multiple Environments without Data Transfer . Below is a usecase where we need to make use of both on-premise and cloud, without moving data to cloud. . Problem statement: . Our data cannot be moved from on-premise to cloud. | We need extra compute power, in our public cloud environment, to create an ML model for use on-premise. | The ML model is to be used on-premise, on new incoming data streams (that cannot be moved to cloud) | . Solution: . Create synthetic data for our on-premise environment use-cases, and - as a side product we save away the synthetic data generator (the pickled model used to create synthetic data). | Copy the synthetic data generator to cloud | Use the synthetic data generator in the cloud, creating synthetic data for training of an ML model | Copy the ML model on-premise, and use it for new incoming data | Evaluate: Compare the on-premise AI model with the model created in the cloud - against the same test data | . Division of work, what is done on-premise with Kubeflow, and what is done in cloud (AWS SageMaker)? . On-premise . See the above Validation of synthetic data techniques. . Develop the model on real data – for the comparison later with the cloud model. | Create synthetic generators, evaluate the generators, and export the best generator to AWS. | . Cloud . Use the imported synthetic generator (from on-premise) | Create synthetic data using the synthetic data generator | Develop the model and determine which synthetic generator is the best | Increase the amount of synthetic data, to see if the increase of synthetic data improves model performance (not for sure it will, see below comment) | Export model to on-premise | . . In some more detail below. . . On-premise . Compare real data model against synthetic data model – using real test data. | . In the current examples we see near equivalent performance of the ML models (a few percentage points lower for models created using synthetic data). We experimented with increasing the size of the synthetic dataset, with minor improvements. Augmenting the training data is expected (not tested here) to have more effects when using deep learning algorithms. . Summary . Clearly, the above workflows would be very cumbersome to build and maintain without Kubeflow. Our solution is entirely open source, Kubernetes based, and uses Kubeflow and SDV to give us the scalability, robustness, and detailed control that is required. . The area of synthetic data generation is moving fast with the overall AI field. Reports from IBM and others, of the increased usage of synthetic data for e.g. LLM training is frequent but the application areas are much greater. We also expect more capable synthesizers and, hopefully, privacy preserving techniques to keep up with the innovation in this area. Our original main motivator was speed up in innovation and experimentation, and overall - speed to market. Often a key pain for our teams. . Looking ahead, we are exploring the development of a synthesizer catalog — ideally integrated into our overall data catalog — to enable users to rapidly experiment with ideas and get started more efficiently. .",
            "url": "https://blog.kubeflow.org/kfp/2025/02/16/synthetic-data-using-kfp.html",
            "relUrl": "/kfp/2025/02/16/synthetic-data-using-kfp.html",
            "date": " • Feb 16, 2025"
        }
        
    
  
    
        ,"post5": {
            "title": "Kubeflow and Me: A Story Started with Push-based Metrics Collection",
            "content": "This summer, I gained a precious opportunity to participate in the Google Summer of Code(GSoC), in which I would contribute to Katib and fulfill a project named “Push-based Metrics Collection in Katib” within 12 weeks. Firstly, I got to know about GSoC and Kubeflow with the recommendation from the former active maintainer Ce Gao(gaocegege)’s personal blog. And I was deeply impressed by the idea of cloud native AI toolkits, I decided to dive into this area and learn some skills to enhance my career and future. In the blog, I’ll provide my personal insight into Katib, for those who are interested in cloud native, AI, and hyperparameters tuning. . Problem . The project aims to provide a Python SDK API interface for users to push metrics to Katib DB directly. . The current implementation of Metrics Collector is pull-based, raising design problems such as determining the frequency at which we scrape the metrics, performance issues like the overhead caused by too many sidecar containers, and restrictions on developing environments that must support sidecar containers and admission webhooks. And also, for data scientists, they need to pay attention to the format of metrics printed in the training scripts, which is error prone and may be hard to recognize. . Solution . We decided to implement a new API for Katib Python SDK to offer users a push-based way to store metrics directly into the Kaitb DB and resolve those issues raised by pull-based metrics collection. . In the new design, users just need to set metrics_collector_config={&quot;kind&quot;: &quot;Push&quot;} in the tune() function and call the report_metrics() API in their objective function to push metrics to Katib DB directly. There are no sidecar containers and restricted metric log formats any more. After that, Trial Controller will continuously collect metrics from Katib DB and update the status of Trial, which is the same as pull-based metrics collection. . If you are interested in it, please refer to this doc and example for more details. . . My Contributions during the GSoC . I raised numerous PRs for the Katib and Training-Operator project. Some of them are related to my GSoC project, and others may contribute to the completeness of UTs (Unit Tests), simplicity of dependency management, and the compatibility of the UI component. . For reference, the coding period can be rougly divided into 3 stages: . Convert the proposal to a KEP and discuss the architecture, API design, etc. (~4 weeks) with the mentors . | Develop a push-based metrics collection interface according to the KEP. (~8 weeks) . | Write some examples and documentation &amp; Present my work to the Kubeflow Community. . | Also, I raised some issues not only to describe the problems and bugs I met during the coding period, but also to suggest the future enhancement direction for Katib and the Training-Operator. . There is a Github Issue tracks the progress of developing push-based metrics collection for katib during the GSoC coding phase. If you are interested in my work or Katib, please can check this issue for more details. . Lessons Learned . Think Twice, Code Once: Andrey taught me that we should think of the API specification and all the related details before coding. This can significantly reduce the workload of the coding period and avoid big refactor of the project. Meanwhile, my understanding of Katib got clear gradually during the over-and-over rounds of re-think and re-design of the architecture. . | Dive into the Source Code: Engineering projects nowadays are extremely complex and need much effort to understand them. The best way to get familiar with the project is to dive into the source code and run several examples. . | Communication: Communication is the most important thing when collaborating with others. Expressing your idea precisely and making others understand you easily are significant skills not only in the open source community but also in various scenarios such as at a company and in group work. . | In the End . Special Thanks: . To my mentors @andreyvelich @johnugeorge @tenzen-y, especially to Andrey. Your great knowledge about the code base and the industry impressed me a lot. Thanks for your timely response to my PRs and for always attending the weekly meetings to solve my pending problems, from which I benefited a lot. What’s more, I can well remember that, in that night, you explained the usage of Kubeflow in the industry to me with greate patience, and encouraged me not to doubt about myself, just do it and explore more, contribute more. You ignite the flame of my desire to contribute to cloud native AI. . | To @gaocegege. You recommend me to the Kubeflow Community. Thanks for your patient answers for my endless silly questions. . | To Google. Thanks for offering such a precious opportunity for me to begin my journey in the open source world! . | I hold a firm belief that every small step counts, and everybody in the community is unique and of great significance. There is no doubt that our joint efforts will surely contribute to the flourishing of our Kubeflow Community, make it the world-best community managing AI lifecycle on Kubernetes, and attract much more attention from the industry. Then, more and more new comers will pour in and work along with us. . Again, I’ll continue to contribute to Kubeflow. . Links . For more details about Kubeflow and the upcoming GSoC’25 event, please check: . What is Kubeflow? | Kubeflow GSoC’25 Event | .",
            "url": "https://blog.kubeflow.org/gsoc-2024-project-6/",
            "relUrl": "/gsoc-2024-project-6/",
            "date": " • Sep 28, 2024"
        }
        
    
  
    
        ,"post6": {
            "title": "LLM Hyperparameter Optimization API: My Google Summer of Code Journey with Kubeflow",
            "content": "This summer, I had the opportunity to participate in the Google Summer of Code (GSoC) program, where I contributed to Kubeflow, an open-source machine learning toolkit. My project focused on developing a high-level API for optimizing hyperparameters in Large Language Models (LLMs) within Katib, Kubeflow’s automated hyperparameter tuning system. I’d like to share insights from this experience with others interested in Kubeflow, GSoC, or optimizing LLMs. . Motivation . The rapid advancements and rising popularity of LLMs, such as GPT and BERT, have created a growing demand for efficient LLMOps in Kubernetes. To address this, we have developed a train API within the Training Python SDK, simplifying the process of fine-tuning LLMs using distributed PyTorchJob workers. However, hyperparameter optimization remains a crucial yet labor-intensive task for enhancing model performance. . Goal . Hyperparameter optimization is essential but time-consuming, especially for LLMs with billions of parameters. This API simplifies the process by handling Kubernetes infrastructure, allowing data scientists to focus on model performance rather than system configuration. . With this API, users can import pretrained models and datasets from Hugging Face and Amazon S3, define parameters including the hyperparameter search space, optimization objective, and resource configuration. The API then automates the creation of Experiment, which contains multiple Trials with different hyperparameter settings using PyTorch distributed training. It then collects and analyzes the metrics from each Trial to identify the optimal hyperparameter configuration. . For detailed instruction on using the API, please refer to this guide . . My Contributions to the GSoC Project . My work on the project can be broadly divided into four stages: . Stage 1: Designing the API, drafting the project proposal, and refining it into a Kubeflow Enhancement Proposal (KEP). | Stage 2: Developing and implementing the high-level API. | Stage 3: Implementing unit tests and end-to-end tests for the API. | Stage 4: Creating documentation and presenting the work to the Kubeflow community. | . In addition, I addressed several critical bugs in previous Katib and Training Operator releases and contributed new features, such as writing end-to-end tests for the train API. . For those interested, here is a detailed summary of all the pull requests I submitted during this process. . Lessons Learned . This is my first experience contributing to an open source project, and I gained extensive technical knowledge throughout this project, including Docker, Kubernetes, and Kubeflow itself. Before developing and implementing the API, I invested significant time onboarding and familiarizing myself with Kubeflow. The official documentation and GitHub repository were invaluable resources during this process. . Beyond these technical skills, I also learned several key lessons that extend into broader personal and professional growth. . Think from the User’s Perspective . One key lesson was the importance of considering the user’s needs. Discussing API design with my mentors taught me to focus on what functionalities users need and how they prefer to use them. Listening to users’ feedback is crucial for effective product design. . Don’t Fear Bugs . I used to feel overwhelmed by bugs and unsure how to tackle them. When a bug caused a container failure during a Katib trial, my mentor guided me through the debugging process, teaching me how to systematically trace and understand the issue. The key is to approach debugging methodically and think through each step of the problem. . Communication is Important . Communication is important in collaboration, especially in open source projects. There are various ways of communicating in open-source projects, such as GitHub issues or PRs, Slack, and community meetings. And I’m grateful to my mentor for discussing my challenges during our weekly meetings and providing invaluable guidance. . Every Contribution Counts . Initially, I thought contributing to open source was complex. I learned that every contribution, no matter how small, is valuable and appreciated. For example, contributing to documentation is crucial, especially for newcomers. . In The End . I am deeply grateful to everyone who supported me throughout this project. Your suggestions, advice, and encouragement were invaluable in helping me complete my work. . I especially want to extend my heartfelt thanks to my mentor, Andrey Velichkevich. His deep knowledge of both the project and the industry, combined with his willingness to help, has been incredibly inspiring. I greatly appreciate the time and effort he dedicated to guiding me, from the high-level design of the API to the finer details like code formatting. I have learned so much from his mentorship. . Looking ahead, I am excited to continue contributing to Kubeflow. I also look forward to helping future contributors by improving documentation and sharing my experiences with newcomers in the community. . If you’re interested in open-source and want to be part of Kubeflow, GSoC 2025 applications are now open! Check out the details here—we’d love to have you join us! .",
            "url": "https://blog.kubeflow.org/gsoc-2024-project-4/",
            "relUrl": "/gsoc-2024-project-4/",
            "date": " • Sep 19, 2024"
        }
        
    
  
    
        ,"post7": {
            "title": "Kubeflow 1.9: New Tools for Model Management and Training Optimization",
            "content": "Kubeflow 1.9 significantly simplifies the development, tuning and management of secure machine learning models and LLMs. Highlights include: . Model Registry: Centralized management for ML models, versions, and artifacts. | Fine-Tune APIs for LLMs: Simplifies fine-tuning of LLMs with custom datasets. | Pipelines: Consolidation of Tekton and Argo Workflows backends for improved flexibility. | Security Enhancements: Network policies, Oauth2-proxy, and CVE scanning. | Integration Upgrades: Improved integrations with Ray, Seldon, BentoML, and KServe for LLM GPU optimizations. | Installation and Documentation: Streamlined installation, updated platform dependencies, and enhanced documentation. | . These updates aim to simplify workflows, improve integration dependencies, and provide Kubernetes-native operational efficiencies for enterprise scale, security, and isolation. . Model Registry . A model registry provides a central catalog for ML model developers to index and manage models, versions, and ML artifacts metadata. It fills a gap between model experimentation and production activities. It provides a central interface for all stakeholders in the ML lifecycle to collaborate on ML models. Model registry has been asked by the community for a long time and we are delighted to introduce it to the Kubeflow ecosystem. . This initial release includes REST APIs and a Python SDK to track model artifacts and model metadata with a standardized format that can be reused across Kubeflow components, such as to deploy Inference Servers. You can get started by following the Model Registry tutorial on the Kubeflow website, or see a short demo video of the Model Registry in action. . We are just getting started. This is an Alpha version and we look forward to feedback. The model registry working group meets biweekly: you can provide feedback by joining the meeting or directly on the repository. . Fine-Tune APIs for LLMs . In the rapidly evolving ML/AI landscape, the ability to fine-tune pre-trained models represents a significant leap towards achieving custom solutions with less effort and time. Fine-tuning with custom datasets allows practitioners to adapt large language models (LLMs) to their specific needs. . However, fine-tuning tasks often require extensive manual intervention, including the configuration of training environments and the distribution of data across nodes. The new Fine-Tune API aims to simplify this process, offering an easy-to-use Python interface that abstracts away the complexity involved in setting up and executing fine-tuning tasks on distributed systems. . By providing this API, Training Operator not only simplifies the user experience for ML practitioners but also leverages its existing infrastructure for distributed training. You can take advantage of Kubernetes’ ability to dynamically schedule GPU, thus saving on compute resources and cost. Training Operator also gives you fault tolerant guarantees, to save your training procedures from cluster node failures. . Pipelines . v1 Feature Parity . We made significant progress towards KFPv1 feature parity by adding more Kubernetes resources to the Pipelines code with the new kfp-kubernetes 1.2.0 Python package. We encourage every KFP user to test the new V2 functionality and plan your migration from V1 to V2. We still have some outstanding features that need to be ported over to V2, please help us to identify what’s missing by openining a new issue in the KFP repository. . Argo Workflows and Tekton Backends Consolidation . The Pipelines Tekton backend has been merged into the main Kubeflow Pipelines repository. You can now choose what workflow engine to use from the same Pipelines version. This proves the extensibility and flexibility of the KFP v2 architecture, which encourages other contributors to bring support for other workflow engines. . Both Argo Workflows and Tekton provide unique advantages. Argo Workflows is known for its simplicity and ease of use, making it a popular choice for many users. Tekton offers extensive customization options with its pipeline definitions and reusable components, which can be advantageous for integrating into various CI/CD systems. Depending on your specific requirements and preferences, you can leverage the strengths of either Argo Workflows or Tekton to optimize your machine learning workflows. . In this blog post, you can find more details about the benefits of running KFP with either Tekton or Argo Workflows. . Argo Workflows Upgrade . Kubeflow Pipelines’s Argo Workflows backend is upgraded to 3.4.16. This upgrade moves the supported version closer to the latest upstream version and brings lots of CVE resolutions. The previous minor version was no longer being patched by the Argo community, so lots of security issues had accumulated over time. . Katib . Kubeflow 1.9 ships with Katib 0.17, which brings official support for ARM64, getting us one step closer to full ARM64 coverage. . For Data Scientists who submit training jobs with the Python SDK, you can now set the algorithm settings and environment variables from the tune method. Previously, you had to rely directly on Kubernetes CRD submission for these. You can also take advantage of the latest features from TensorFlow 2.16 and PyTorch 2.2. The team also worked to resolve environmental conflicts that prevented the Katib Python SDK to be installed alongside the Kubeflow Python SDK. . There are tons of additional improvements and bug fixes. Check out the full changelog here. . Central Dashboard . This release bring several improvements to the Kubeflow Central Dashboard, including: . Styling improvements to the sidebar, including grouping all Kubeflow Pipelines links to reduce clutter | Significant improvements to the “manage contributors” page, including the ability to manage contributors for all profiles that you are the owner of, and see which profiles you have access to, even when you are not the owner | Allow external services to parse the current profile (namespace) by sending the namespace selector value to non-iframed applications | Significant updates to dependencies to reduce CVEs | . . Notebooks . With this release, we provide significant updates to all example notebook images including PyTorch 2.3.0, Tensorflow 2.15.1 and many other library updates. While you can continue to use the old images, we recommend updating to use the greatest and latest ML libraries. . Additionally, notebooks images now run with a non-root SecurityContext, allowing for an improved security. . Take a look at the changelog for a full list of bug fixes and improvements. . While this release was light on new Notebooks features, the Working Group is hard at work on an exciting new project: we are actively developing Notebooks V2, with contributions from various companies, in the new repository. Take a look here and join our Working Group meetings to get involved! . Kubeflow Platform (Security and Manifests) . Security . Network Policies . Network policies are enabled for the Kubeflow core services as a second layer of defense before Istio authorization policies. This gives administrators a better network overview and segmentation while also enforcing common enterprise security guidelines. You can read more about the current implementation and architecture here. . Authentication . Oauth2-proxy replaces oidc-authservice, which brings improved token-based authentication. Machine Learning engineers can now use tokens instead of insecure passwords for CI/CD automation of Kubeflow deployment and maintenance (e.g. using GitHub actions). You can read more about the current implementation and architecture here. . CVE Scanning . With this release we are introducing automated CVE scanning with Trivy on the manifests master branch. We appreciate contributions to reduce the number of CVEs, the Security Working Group needs help to build a more secure platform. You can find more details about our security scanning process and disclosure policy here. Here are is a summary from June 25th: . . You can find a detailed Security WG roadmap here. . Manifests . Installation and documentation improvements . The documentation has been improved and now contains guidelines for upgrading and extending the Kubeflow Platform for administrators. New users can now install kubeflow on their laptop in just a few minutes . Platform dependencies updates: . Component | Kubernetes | Kustomize | Istio | Dex | Cert-Manager | Knative | . KF 1.9 Version | 1.27 - 1.29 | 5.2.1+ | 1.22.1 | 2.39.1 | 1.14.5 | 1.12.4 | . NOTE Kubernetes 1.30+ is also expected to work, but was not officially tested. . NOTE Kustomize 5.2.1+ support with way less warnings. So platform engineers have a modern and supported installation tool chain. . Integration with third-party ML tools . We have updated the third-party components in /contrib to provide integration with the broader ML ecosystem. . BentoML 1.2.28 and 1.1.21 | Seldon 1.18.1 | Ray 2.23 and Kuberay 1.1.1 | . Find a detailed Manifests WG roadmap here. . KServe . We upgraded to KServe 0.13. This release includes: . Enhanced Hugging Face Runtime Support: Hugging Face models are supported out-of-the-box, implementing a KServe Hugging Face Serving Runtime. Currently supported tasks include sequence classification, token classification, fill mask, text generation, and text to text generation. | vLLM Support: Dedicated runtime support for vLLM is now included, streamlining the deployment process for LLMs. | OpenAI Schema Integration: KServe now supports endpoints for generative transformer models, following the OpenAI protocol. This enables KServe to be used directly with OpenAI’s client libraries or third-party tools like LangChain and LlamaIndex. | . Documentation . MLOps is a complex subject and users have asked for clear, up-to-date and comprehensive documentation. We are happy to announce that we started a restructuring process to better align the various components’ docs to have a similar structure. We are revamping our docs to better align with user expectations and how you expect technical docs to be organized. We will continue to improve the quality and completeness of our docs, by adding new user guides, tutorials, and reference architecture topics. . We are looking for new members who can help us craft complete and high quality documentation. Please get involved by opening and reviewing PRs in the Kubeflow website. . Honorable Mentions . Google Spark Operator migration to Kubeflow . We’re excited to announce the migration of Google’s Spark Operator to the Kubeflow Spark Operator, marking the launch of a significant addition to the Kubeflow ecosystem. The Kubeflow Spark Operator simplifies the deployment and management of Apache Spark applications on Kubernetes. This announcement isn’t just about a new piece of technology, it’s about building a stronger, open-governed, and more collaborative community around Spark on Kubernetes. . Kubeflow Spark Operator is not yet officially included in the Kubeflow release, but you can install it by following the instructions here. . Read more about Kubeflow Spark operator in the announcement blog post. . Google Summer of Code . This year, Kubeflow was excited to participate in Google Summer of Code (GSoC), attracting a wave of enthusiastic students! Over 250 students joined our Slack channel, eager to learn about contributing to the Kubeflow community and crafting impactful proposals. . We were also fortunate to have a dedicated group of 20 mentors ready to guide these talented individuals. From a pool of nearly 70 proposals, we have selected 10 proposals and were awarded with 8 outstanding students by Google. They are now actively contributing to various Kubeflow features, making a real difference in various Kubeflow components. . We’ll be following their progress and sharing their accomplishments through a series of blog posts in the future, so stay tuned! A big thank you to all the mentors and students who are making Kubeflow’s GSoC 2024 a huge success! . Thanks to all the studets: Adem Baccara, Biswajit Pattnaik, Hansini Karunarathne, Hezhi Xie, Sandipan Panda, Shao Wang, Shashank Mittal, SIVASUBRAMANIAM L. Visit this page for more details about each project and respective mentors. . What’s next . The community continues to see growth, especially with the ever growing interest from the CloudNative community in AI topics. We have recently elected the first Kubeflow Steering Committee. This is the first step towards a more mature governance structure and a democratic and open community. . If you want to take a peek into the Kubeflow 1.10 roadmap planning and contribute with your ideas, see Notebooks, Manifests &amp; Security, Pipelines, Model Registry, Katib, Training Operator. . How to get started with 1.9 . Visit the Kubeflow 1.9 release page or head over to the Getting Started section to learn more about installation, architecture and quick start examples. . Join the Community . We would like to thank everyone for their contribution to Kubeflow 1.9, especially Ricardo Martinelli De Oliveira for his work as the v1.9 Release Manager, all the release team and the working group leads, who relentlessly dedicate their time to this great project. . Release team members: Ricardo Martinelli De Oliveira, Stefano Fioravanzo, Helber Belmiro, Diego Lovison, Ajay Nagar, Mathew Wicks, Steven Irvin, Milos Grubjesic, Andrew Scribner, Julius von Kohout. . Working Group leads: Andrey Velichkevich, Ce Gao, Chaoran Yu, Chen Sun, Christian Kadner, Ilias Katsakioris, James Liu, James Wu, Johnu George, Julius von Kohout, Kimonas Sotirchos, Mathew Wicks, Matteo Mortari, Ramesh Reddy, Stefano Fioravanzo, Tommy Li, Vara Bonthu, Yannis Zarkadas, Yuan Tang, Yuki Iwai. . Kubeflow Steering Committee: Andrey Velichkevich, Johnu George, Josh Bottum, James Wu, Yuan Tang. . Participating Distributions: Charmed Kubeflow (Canonical), IBM IKS, Nutanix, OpenShift AI (RedHat), Oracle Cloud Infrastructure, DeployKF, VMWare, QBO. You can find more details about Kubeflow distributions here. . Want to help? . The Kubeflow community Working Groups hold open meetings and are always looking for more volunteers and users to unlock the potential of machine learning. If you’re interested in becoming a Kubeflow contributor, please feel free to check out the resources below. We look forward to working with you! . Visit our Kubeflow website or Kubeflow GitHub Page | Join the Kubeflow Slack channel | Join the kubeflow-discuss mailing list | Attend our weekly community meeting | .",
            "url": "https://blog.kubeflow.org/kubeflow-1.9-release/",
            "relUrl": "/kubeflow-1.9-release/",
            "date": " • Jul 22, 2024"
        }
        
    
  
    
        ,"post8": {
            "title": "Announcing the Kubeflow Spark Operator: Building a Stronger Spark on Kubernetes Community",
            "content": "We’re excited to announce the migration of Google’s Spark Operator to the Kubeflow Spark Operator, marking the launch of a significant addition to the Kubeflow ecosystem. The Kubeflow Spark Operator simplifies the deployment and management of Apache Spark applications on Kubernetes. This announcement isn’t just about a new piece of technology, it’s about building a stronger, open-governed, and more collaborative community around Spark on Kubernetes. . The Journey to Kubeflow Spark Operator . The journey of the Kubeflow Spark Operator began with Google Cloud Platform’s Spark on Kubernetes Operator (https://cloud.google.com/blog/products/data-analytics/data-analytics-meet-containers-kubernetes-operator-for-apache-spark-now-in-beta). With over 2.3k stars and 1.3k forks on GitHub, this project laid the foundation for a robust Spark on Kubernetes experience, enabling users to deploy Spark workloads seamlessly across Kubernetes clusters. . Growth and innovation require not just code but also community. Acknowledging the resource and time limitations faced by Google Cloud’s original maintainers, Kubeflow has taken up the mantle.This transition is not merely administrative but a strategic move towards fostering a vibrant, diverse, and more actively engaged community. . Why Kubeflow? . Enhanced Community Engagement: Transitioning to Kubeflow opens the door to a broader developer base, encouraging contributions and collaboration. Since Kubeflow is a CNCF incubating project this transition will help consolidate Cloud Native and Spark communities to work more closely to build robust infrastructure to run Spark applications on Kubernetes. . | Stronger Governance: Kubeflow’s governance model provides a structured environment for decision-making and project management, ensuring sustainable growth for the Spark Operator. . | Unified Ecosystem: By bringing the Spark Operator under the Kubeflow umbrella, we’re not just merging projects; we’re building a cohesive ecosystem that enhances the Spark on Kubernetes experience. . | Integration with AI/ML: Kubeflow provides several components to address many stages of the AI/ML lifecycle. The Spark distributed data processing capabilities are a natural expansion, allowing the Spark community to closely collaborate and better integrate within the end-to-end ML lifecycle. . | . What’s Next? . We are dedicated to not just maintaining but enhancing the Kubeflow Spark Operator for the long term. Here’s what you can look forward to: . Upcoming roadmap: As part of the first release, we aim to update the documentation with references to Kubeflow, address GitHub workflow issues, and update the container registry with Kubeflow, along with any other critical issues. . | Ongoing Support and Enhancements: At the time of migration to the Kubeflow repository, the repository comprised 450+ issues and 60+ pull requests. We kindly request contributors to rebase their code and update the PR with a comment indicating its continued relevance. As for open issues, they will be considered for resolution as the broader community and contributors engage in upcoming releases.The operator will continue to evolve, incorporating new features and improvements to stay at the forefront of Kubernetes deployments. . | Rich Community Resources: From detailed documentation to hands-on tutorials, we’re crafting resources to help you succeed with the Spark Operator. We are planning to host regular Spark Operator calls to discuss users issues, questions, and future roadmaps. . | Open Doors for Contributions: This is a call to arms for developers, writers, and enthusiasts! Your contributions are the lifeblood of this project, and there’s a place for everyone to make a mark. . | Kubeflow Working Group Data: To consolidate efforts around new data tools in the Kubeflow ecosystem such as Spark Operator and Model Registry the new Working Group Data will be formalized soon. Feel free to review this PR to get involved and provide your feedback on the charter. . | . Join the Movement . The Kubeflow Spark Operator is more than just software. It’s a community endeavor. Here’s how you can be a part of this journey: . Dive In: Visit our GitHub repository to start your journey with the Kubeflow Spark Operator. . | Contribute: Every code snippet, documentation update, and piece of feedback counts. Find out how you can contribute on GitHub. . | Be Part of the Community: Join the CNCF Slack Workspace and then join the conversation in the #kubeflow-spark-operator channel. Whether you’re seeking advice, sharing insights, or just listening in, your presence enriches us. Follow this guide to learn more about Kubeflow community. . | Kubeflow Spark Operator Community Call: We’re excited to announce Spark Operator Community Monthly Meetings for Open Source Contributors starting May 17th, 2024 (10-11 AM PST). These meetings, held every third Friday, are your chance to discuss project updates, share ideas, and collaborate with the community. You can find the Zoom call details and meeting notes in this Google Doc. . | . In the spirit of collaboration fostered on platforms like Slack, and with the generous support of the Google Cloud team, we’re set to sail into a promising future. The Kubeflow Spark Operator isn’t just a tool, it’s our collective step towards harnessing the true potential of Spark on Kubernetes. Together, let’s shape the future of cloud-native big data processing. . Reference Issues . Action items for adoption of Spark Kubernetes Operator in Kubeflow . | WG Data(name provisional)proposal . | Update Documentation: Redirect Helm Chart Installation Links to Kubeflow Repository . | Update Release Workflows: Change Container Registry to Kubeflow’s ghcr.io . | .",
            "url": "https://blog.kubeflow.org/operators/2024/04/15/kubeflow-spark-operator.html",
            "relUrl": "/operators/2024/04/15/kubeflow-spark-operator.html",
            "date": " • Apr 15, 2024"
        }
        
    
  
    
        ,"post9": {
            "title": "Kubeflow Project Steering Committee Announced",
            "content": "We’re thrilled to announce the results of the 2023 Kubeflow Steering Committee (KSC) election. . The Kubeflow community has shown a strong commitment to the project’s future by casting their votes for the new leadership. Please welcome Yuan (Terry) Tang (Red Hat), Andrey Velichkevich (Apple), and Johnu George (Nutanix) who will be joining Josh Bottom (Consultant) and James Wu (Google) as KSC members. The three new members will serve a two-year term ending in 2025, beginning immediately. . The election saw a turnout of 72.88%, with 43 out of 59 eligible voters participating. The three nominees were chosen from a pool of candidates, which also included Kimonas Sotirchos, Julius von Kohout, and Mathew Wicks. . This election represents a significant step forward for the Kubeflow project. We extend our deepest gratitude to the interim KSC members and the election officials for their service and to all those who were nominated and participated in this election. We eagerly anticipate the contributions of our new leadership and what we can accomplish as a project moving forward. . Quotes: . “Excited to start my two-year term on the Kubeflow Steering Committee on behalf of Red Hat! Thanks to everyone who’s supported me along the way, and congratulations to all the new committee members. Johnu, Andrey, and I have collaborated on various Kubeflow subprojects for many years. I look forward to working with them and the rest of the committee more closely. Kubeflow is an umbrella of projects that provide an excellent foundation for AI/ML applications in the cloud-native world. Its contributor velocity, community momentum, and industry adoption have proliferated. Together with the Kubeflow community, ecosystem projects, supporting organizations, and partners, I am confident that we will steer the project towards a successful CNCF journey and continue thriving.” - Yuan (Terry) Tang (Red Hat) . “I am thrilled to join the Kubeflow Steering Committee for the two-year term. Being an active member of this community for almost six years, it was great to see how the project evolves towards open governance and widespread user adoption. Thanks to everyone for your support, collaboration, and contributions throughout these years. Kubeflow stands as the foundational framework to run AI/ML workloads on Kubernetes. It bridges ML, Big Data, and Cloud Native ecosystems to facilitate a new generation of AI applications. Kubeflow components empower users at every stage of ML lifecycle from model development and training to fine-tuning and deployment. I am excited about the future of CNCF and Kubeflow together, to build an open CloudNative AI/ML platform accessible to everyone.” - Andrey Velichkevich (Apple) . “I am really excited to join the first Kubeflow Steering Committee formed post CNCF incubation. Kubeflow is one of the most popular enterprise-ready MLOps platforms used in production at various companies. It has become the de facto platform to run complex ML pipelines, managing model lifecycles on a large scale. Having been part of Kubeflow leadership since its inception, it is delightful to see its growing ecosystem proving its relevance in the current times. Thanks to the entire Kubeflow user and developer community, who have immensely contributed to its success over the years. I am excited to drive the future of this vibrant community and look forward to the next phase of the Kubeflow journey in the CNCF ecosystem.” -Johnu George (Nutanix) . Additional Information: . Kubeflow Project Steering Committee Elections 2023 | List of Eligible Voters and Candidates 2023 | [Election] - 2023 KSC Member Election Results | [Elections] Election Phase - Kubeflow Project Steering Committee Elections - Begins today | [Election] Testimonial Phase Kubeflow Steering Committee Election - Now Open | [Elections] Nominations Phase - 2023 Kubeflow Project Steering Committee Elections - Opens Today | [Elections] 2023 Kubeflow Project Steering Committee Elections - Eligible Voters and Candidates | [Elections] 2023 Kubeflow Project Steering Committee Elections - Timeline and Update | . Get Involved . Getting involved in the Kubeflow Community offers numerous opportunities for learning, networking, and contributing to open-source development. Participating in the Kubeflow community you’ll be joining a vibrant ecosystem of developers, data scientists, AI enthusiasts and others who are pushing the boundaries of machine learning operations with Kubeflow. By actively participating and sharing your ideas, you can make a meaningful impact and be part of this dynamic community. Check out the weekly community call, get involved in discussions on the [mailing list] .",
            "url": "https://blog.kubeflow.org/election/2024/01/31/kubeflow-project-steering-committee-announced.html",
            "relUrl": "/election/2024/01/31/kubeflow-project-steering-committee-announced.html",
            "date": " • Jan 31, 2024"
        }
        
    
  
    
        ,"post10": {
            "title": "Kubeflow Community Holds First Election for Kubeflow Steering Committee",
            "content": "The Kubeflow Community, known for its dedication to making machine learning workflows on Kubernetes simple, portable, and scalable, has recently announced a significant milestone in its journey. For the first time, they are holding elections for the Kubeflow Steering Committee (KSC)! . The KSC will be crucial in guiding the project’s direction, ensuring it continues to meet the needs of its growing user base. Candidates for this committee are drawn from the community’s diverse members, embodying the spirit of open-source collaboration that Kubeflow cherishes. This election marks a new chapter in Kubeflow’s history, reflecting the community’s commitment to a democratic project governance model. It serves as a testament to the community’s growth, maturity, and dedication to inclusivity and shared decision-making. . The election process for the KSC is structured to ensure that every phase is transparent and fair. As per the announced timeline, we are currently in the nomination phase, where eligible community members are encouraged to nominate themselves or others for a position on the Committee. This stage will determine the group of candidates who will move forward to the testimonial phase and then the voting phase. It’s an exciting time for the community, as members step up to the challenge and show their readiness to drive the project’s future direction. . Elections like this in the open source ecosystem highlight the collaborative spirit that drives these initiatives. They provide the opportunity for anyone, regardless of their role in the project, to step up and help guide its future. Let’s all join in congratulating the Kubeflow Community on this milestone and look forward to the outcomes of this exciting election! . Get Involved: . We are an open and welcoming community of software developers, data scientists, AI enthusiasts, organizations and more! Getting involved in the Kubeflow Community is an exciting journey that offers numerous opportunities for learning, networking, and contributing to open-source development. By actively participating and sharing your ideas, you can make a meaningful impact and be part of this dynamic community. Check out the weekly community call, get involved in discussions on the mailing list or chat with others on the Slack Workspace! . Important Dates: . Exceptions Phase: 4 December 2023 at 0900 Pacific Time (Starts) - 10 December 2023 at 12:00pm Pacific Time (Ends) | Nomination Phase: 11 December 2023 at 0900 Pacific Time (Starts) - 24 December 2023 at 12:00pm Pacific Time (Ends) | Testimonial Phase: 25 December 2023 at 0900 Pacific Time (Starts) - 7 January 2024 at 12:00pm Pacific Time (Ends) | Voting Phase: 8 January 2024 at 0900 Pacific Time (Starts) - 29 January 2024 at 12:00pm Pacific Time (Ends) | Announcement of Election Results: 30 January 2024 | . Additional Information: . Kubeflow Project Steering Committee Elections 2023 | List of Eligible Voters and Candidates 2023 | [Elections] Nominations Phase - 2023 Kubeflow Project Steering Committee Elections - Opens Today | [Elections] 2023 Kubeflow Project Steering Committee Elections - Eligible Voters and Candidates | [Elections] 2023 Kubeflow Project Steering Committee Elections - Timeline and Update | .",
            "url": "https://blog.kubeflow.org/election/2023/12/12/kubeflow-community-holds-first-election-for-kubeflow-steering-committee.html",
            "relUrl": "/election/2023/12/12/kubeflow-community-holds-first-election-for-kubeflow-steering-committee.html",
            "date": " • Dec 12, 2023"
        }
        
    
  
    
        ,"post11": {
            "title": "Kubeflow 1.8: Kubernetes MLOps delivered via Python workflows",
            "content": "Kubeflow v1.8’s powerful workflows uniquely deliver Kubernetes-native MLOps, which dramatically reduce yaml wrangling. ML pipelines are now constructed as modular components, enabling easily chainable and reusable ML workflows. The new Katib SDK reduces manual configuration and simplifies the delivery of your tuned model. v1.8 also introduces the PVC Viewer for easier persistent storage management eliminating the need for Kubernetes CLI storage commands. . Kubeflow 1.8 adds ARM processor support which simplifies adoption for Apple Silicon users and IoT servers. 1.8 delivers updated Tensorflow, PyTorch, Cuda notebook image examples with a detangled makefile process. Kubeflow’s performance and security are improved via significant upgrades to the underlying components and packages. . Selected and Highlighted deliveries . Kubeflow Pipelines . Kubeflow 1.8 includes KFP 2.0.3 and KFP SDK 2.4.0. The KFP Python SDK 2.4.0 release contains features present that were previewed in the v2 namespace with now full official support along with considerable additional functionality. A selection of these features include: . An improved and unified Python-based authoring experience for components and pipelines | Support for using pipelines as components (pipeline in pipeline) | Various additional configurations for tasks | Compilation to an Argo-independent pipeline definition that enables pipelines to be compiled once and run anywhere | An improved KFP CLI | Refreshed user documentation and reference documentation | . This pipeline release is focused on cross-platform portability, features such as reading and writing to a Kubernetes PVC has been moved into the extension package kfp-kubernetes, more information can be found at the platform specific features page on the Kubeflow Website. . Additionally the project kfp-tekton which allows users to run pipelines with a Tekton backend, is also updated version 2.0.3 and the sdk will compile to the same pipeline spec, sdk users can use the same pipeline definition to run on both Argo and Tekton backends. . Katib . The updated Katib pythonic SDK in Kubeflow v1.8 helps developers to improve model quality by simplifying the configuration and tracking of model and environmental parameters. In this release we are happy to announce new KatibConfig API (#2176) along with multiple commits improving the experience in the UI and bug fixes in the SDK. A selection of these features and bug fixes include: . Upgrade Tensorflow version to v2.13.0 (#2216) | Start waiting for certs to be ready before sending data to channel (#2215) | Remove a katib-webhook-cert Secret from components (#2214) | . More information on the release can be found in the release notes . Training Operator . Many model developers are experiencing challenges with GPU utilization, especially with large language models. The new training operator functionality provides efficient configuration, utilization and scaling for training. v1.8 improves support for gang schedulers such as volcano, scheduler-plugins and koord-schedule (#1747). Additionally the implementation of suspend semantics allows the external controller to delete or not create Pods depending on the state of the flag (#1859). V1.8 also includes workflow updates and bug fixes that make model training more straightforward to use. A selection of these features and bug fixes include: . Make scheduler-plugins the default gang scheduler. (#1747) | Auto-generate RBAC manifests by the controller-gen (#1815) | Set correct env variables for PytorchJob to support torchrun, allowing to support different distributed training launch methods (#1840) | Add default Intel MPI env variables to MPIJob (#1804) | Fully consolidate tfjob-operator to training-operator (#1850) | . More information on the release can be found in the release notes . KServe . Kubeflow v1.8 introduces KServe 0.11. In this release, we’ve introduced Large Language Model (LLM) runtimes, made enhancements to the KServe control plane, and updated the Python SDK with revised support for the Open Inference Protocol, as well as improved dependency management using poetry. A selection of these features and bug fixes include: . TorchServe 0.8.0 for LLM support (#2980) | Implement v2/open inference endpoints for kserve python runtimes (2655) | Adding an end-to-end example for deploying a large language model on KServe (2836) | . More information on the KServe 0.11 release can be found in the release notes . Simplifying Storage for ML Workflows . Kubeflow’s web apps enhancements expose valuable information to data scientists, which simplifies advanced workflows. For example, Kubernetes storage management, especially for persistent volumes, is critical for rapid data science iteration but can require complex, manual configuration. The newly introduced PVC Viewer makes storage management easy for end users and eliminates the need to learn low-level CLI commands. Users can click and drag to add/remove files and file directories from persistent volumes. The volumes can easily be attached to MLOps objects, like notebooks, which makes experimenting with different data sets faster. . . Support for ARM Machines . Kubeflow 1.8 brings forward a significant upgrade (#7343) by officially introducing support for ARM architectures. This move ensures that developers can leverage Kubeflow’s powerful machine learning capabilities on a wider variety of devices and platforms, including the Apple Silicon and a multitude of IoT devices. This expansion highlights Kubeflow’s adaptability, ensuring seamless machine learning workflows across various hardware architectures. . Security - Rootless Kubeflow Updates and Fixes . In our ongoing commitment to bolstering security, version 1.8 brings both vital security fixes and innovative features. A notable endeavor by the Security Team has been the development of features that lay the groundwork for a rootless Kubeflow #2455 in the future. v1.8 offers users the flexibility to run Kubeflow either with the optional istio-cni or without it. A selection of these features and bug fixes include: . Fix performance issue within a mysql request that could cause a denial of service (#9680) | Fix issue on which Profile controller and KFAM allow unauthenticated in-cluster traffic(#7032) | Adding oauth2-proxy as optional alternative to oidc-authservice (#2409) | . Future versions will continue to refine this rootless functionality, along with plans to make istio-cni the default option along with istio ambient mesh and incorporating Pod Security Standards. Please join the Security Team . Notebooks . Kubeflow Notebook 1.8 brings many enhancements to the notebook container images. . This chart shows how the official images are now related to each other . . Of special note is that all notebook images (except the CUDA ones) are now built for both AMD64 and ARM64 architectures (#7357). Additionally, it is now much easier to build custom images by running make docker-build-dep from within the image’s folder. We have also updated to the following package versions: . Python 3.11.6 | JupyterLab 3.6.6 | Tensorflow 2.13.0 and CUDA 11.8 (for Tensoflow CUDA images) | PyTorch 2.1.0 and CUDA 12.1 (for PyTorch CUDA images) | . Platform dependencies . Kubeflow 1.8 includes hundreds of commits. The Kubeflow release process includes several rounds of testing by the Kubeflow working groups and Kubeflow distributions. Kubeflow’s configuration options provide a high degree of flexibility. After considering all of the testing options, the 1.8 Release Team narrowed the critical dependencies for consistent testing and documentation to the following. . Component | Knative | Istio | Kubernetes | Cert-Manager | Kustomize | Dex | Argo | Tekton | Oidc | . Version used in 1.8 | 1.10.2 (Serving) 1.10.1 (Eventing) | 1.17.3 | 1.25/1.26 | 1.12.2 | 5.0.3 | 2.36.0 | 3.3.10 | 0.47.5 | e236439 | . The 1.8 documentation includes overall installation instructions from the Manifest Working Group, and detailed feature reviews from each Kubeflow working group. Most of the working groups have broken their changelogs into subsections that highlight core features, UI enhancements, miscellaneous updates, bug fixes and breaking changes. . What’s next . The community continues to see a large increase in activity since the announcement that Kubeflow is now part of the CNCF Incubator. The community holds regular meetings and holds elections for the Kubeflow steering committee in the next few weeks. . During the 1.8 release cycle, the community continues to work in terms of security, issue triage and documentation. Our next releases will focus on helping build components for training LLMs and scalability. . How to get started with 1.8 . For trying out Kubeflow 1.8 we recommend our installation page where you can choose between a selection of Kubeflow distributions. For more advanced users we also provide the manifest installation guide. We continue to test and improve the documentation. If you find an issue, please feel free to report and/or open a PR to help improve it for others. . Join the Community . We would like to thank everyone for their contribution to Kubeflow 1.8, especially Daniela Plasencia for his work as the v1.8 Release Manager. As you can see, the Kubeflow community is vibrant and diverse, solving real-world problems for organizations worldwide. . Want to help? . Want to help? The Kubeflow community Working Groups hold open meetings and are always looking for more volunteers and users to unlock the potential of machine learning. If you’re interested in becoming a Kubeflow contributor, please feel free to check out the resources below. We look forward to working with you! . Visit our Kubeflow website or Kubeflow GitHub Page | Join the Kubeflow Slack channel | Join the kubeflow-discuss mailing list | Attend a weekly community meeting | .",
            "url": "https://blog.kubeflow.org/kubeflow-1.8-release/",
            "relUrl": "/kubeflow-1.8-release/",
            "date": " • Oct 23, 2023"
        }
        
    
  
    
        ,"post12": {
            "title": "Kubeflow User Survey 2023",
            "content": "In April 2023, the Kubeflow user survey opened, gathering community feedback. The survey aimed to comprehend the adoption of Kubeflow and collect input on the benefits, gaps, and requirements for machine learning use cases. . The survey consisted of 21 questions, including multiple-choice and freeform formats. It ran from April 11th to May 26th and received 90 responses. The 2023 survey yielded more targeted and actionable freeform answers, which provided further insights into users’ requirements and potential enhancements for Kubeflow. Furthermore, we received a significant amount of positive feedback regarding the factors that contribute to the success of Kubeflow and its community. One survey respondent expressed their appreciation, stating, “I love that it helps teams do high-quality ML work while providing flexibility. I love that it is open-source. I love that the community keeps working hard and making it better.” Another respondent highlighted their liking for Kubeflow due to “the broad potential of some of the projects (like Pipelines components) and the different integrations that make for more of an end-to-end experience.” . We thank everyone for participating in the survey and we will use both the improvements and positive feedback to steer our efforts in enhancing Kubeflow, ensuring it remains a user-centric platform for all. . Key Takeaways . 84% of the users deploy more than one Kubeflow component | The top 3 Kubeflow components are Pipelines (90%), Notebooks (76%), and Katib (47%) | The top contrib component used by the user is KServe (62%) | Documentation (55%) is the biggest gap in Kubeflow, followed by tutorials (50%) and tied for third place, installation (39%) and upgrades (39%) | Monitoring models (45%) is the biggest gap in the users’ ML lifecycle, followed by model registry (44%) and initial setup (39%) | 52% of the users use the raw manifest installation to install Kubeflow | The top distribution used to install Kubeflow is AWS (28%) followed by Google Cloud (17%) | 74% of the users deploy Kubeflow on the cloud and 45% on-prem | 49% of the users are running Kubeflow in production | 17% of the users are contributing back to Kubeflow | 49% of the users are keeping up with the latest Kubeflow 1.7 release and 43% are running a version behind Kubeflow 1.6 | . Survey Respondents . . The Kubeflow user survey drew responses from 90 members of the community mostly made up of members from the United States (43%), Europe (34%), and Asia-Pacific (10%). The majority of the respondents were from the Tech industry (49%), followed by Finance (13%) and Consulting (11%). . While there is diversity in roles that make up the community, most of the community members had the title MLOps Engineer (18%), ML Engineer (17%), and Architect (15%). . In the last year’s 2022 User Survey, we started to see an increase in the number of users adopting Kubeflow in their production environment and actively contributing to the Kubeflow project as their expertise with the project grew. This year, we see the trend continuing with the highest percentage of Kubeflow running in production (49%) and the highest number of users contributing to the project (17%). As the adoption of Kubeflow continues to rise, this will have a significant impact on the positive growth and development of the project. . . Documentation and Tutorials . . Documentation and tutorial have been a challenge in the community for a while, and the trend continues this year with documentation (55%) voted as the number one biggest gap in Kubeflow, followed by Tutorials (49%) as the second biggest gap. . In the previous surveys, the feedback on documentation and tutorials was very generic. However, this year, the users are asking for specific documentation to help them adopt Kubeflow better. Some of the documentation and tutorials users are looking for are the following: . Tutorials and examples of how to leverage the new features delivered with the new releases | An architecture diagram of Kubeflow of how each component works | How to setup Transport Layer Security (TLS) with raw manifest installation | A user onboarding documentation on how to add contributors to the platform and map credentials | Documentation to show integration with other tools like MLflow | Examples of all the capabilities of KFP packages | Documentation to cover the RBAC and authorization in Kubeflow | Upgrade guide for every release | . In addition to the specific ask, the major themes in what type of documentation improvements users are asking for are . Documentation for advanced users and advanced use cases | Documentation for best practices around Kubeflow | More diverse end-to-end examples, not just the mnist example | Frequent updates to documentation as many are out of date | . To address the challenges of Kubeflow documentation, we are looking for help from our community. If you are interested in improving current Kubeflow documentation and work on future improvements, we invite you to join us at the next community meeting to introduce yourself to the community. . Installation and Upgrades . . One of the top three voted answers to the biggest gap in Kubeflow was installation tied with upgrades. The survey result showed that 52% of the users use the raw manifest installation to install Kubeflow. With the sole support of Kustomize as a choice of installation tool for Kubeflow, many users are looking for more diverse support for tooling, especially Helm. . One of the biggest reasons for Helm support requests is due to complexity of installation. One survey respondent shared, “Kustomize does not quite provide the same experience and requires a lot more familiarity with the underlying systems and manifests to properly configure…,” which makes installation too difficult for a small team trying to adopt Kubeflow. . For some, support for a different installation tool isn’t the answer. We also received much feedback on the installation needing to be more lightweight. Due to the complexity of the stack and integration of many components, people are struggling to customize their Kubeflow instance due to tight coupling and requirements of large resources. . In addition to installation, upgrades are also on top of the mind for many of the users. While there are some upgrade guides for specific distributions, many of the users are installing Kubeflow without distribution. Without upgrade guides provided for the raw manifest installation, upgrades are taking a great amount of effort for the users. . Monitoring Models and Model Registry . . In last year’s survey, monitoring surged to the top as the biggest gap in the machine learning lifecycle. This year, the trend continued with monitoring (45%) voted as the number one concern, closely followed by model registry (44%). . For both gaps, users are looking for a built-in solution that addresses monitoring and model registry needs, with specific requests for support of Grafana and MLflow integrations to enable a seamless experience with Kubeflow pipelines. With a lack of support for both of these tools, users are expressing concerns about the challenges they face in managing and tracking their models. . Security . In the previous 2022 Kubeflow User Survey, security emerged as one of the top three voted answers, highlighting it as a significant gap in Kubeflow. Since then, the community has placed a strong emphasis on security, leading to the formation of the Kubeflow Security Team comprising members with a security-focused mindset. Their primary objective is to address security concerns and ensure that Kubeflow remains a robust and secure platform for all its users. For further details, visit the Kubeflow security team and consider joining the next security team meeting. . What’s Next? . Currently, all working groups are working towards the 1.8 release, planned to release on October 4th, 2023. Upon successful delivery of the 1.8 release, each working group will shift their focus towards prioritizing features for the next subsequent release. During the initial planning phase of the new release, each working group leads will reevaluate the survey results to determine their priorities driven by your survey feedback. If you are interested in the future direction of each working group, join them in discussing their roadmap at their future meetings. All Kubeflow community meetings can be found in the Kubeflow community calendar. . As for improvements to the documentation and tutorials, Kubeflow community is actively seeking help from our community members. If you are interested in improving current Kubeflow documentation and work on future improvements, we invite you to join us at the next community meeting to introduce yourself to the community and express your interest in joining this effort. . Join the Community . We would like to thank everyone for their participation in the survey. As you can see from the survey results, the Kubeflow Community is vibrant and diverse, solving real-world problems for organizations worldwide. . Want to help? The Kubeflow Community Working Groups hold open meetings and are always looking for more volunteers and users to unlock the potential of machine learning. If you’re interested in becoming a Kubeflow contributor, check out the Kubeflow community page to learn more. We look forward to working with you! .",
            "url": "https://blog.kubeflow.org/kubeflow-user-survey-2023/",
            "relUrl": "/kubeflow-user-survey-2023/",
            "date": " • Jul 26, 2023"
        }
        
    
  
    
        ,"post13": {
            "title": "Kubeflow v1.7 simplifies Kubernetes native MLOps via enhanced UI, Katib Tuning API and new training frameworks",
            "content": "Kubeflow v1.7 users are capitalizing on their python knowledge to build seamless workflows without the need of Kubernetes CLI commands and without building container images for each iteration. With new UIs in multiple components, developers can correlate configuration parameters with logs which allow them to quickly analyze the results. When coupled with Kubeflow’s pythonic workflows and Kubernetes operating efficiencies, these enhancements save model developers material amounts of time and toil. . This release includes hundreds of new commits, and the following sections provide details on the top features. You will also find updates on platform dependencies, changelogs, breaking changes, ecosystem partners, the new Security Team and Kubeflow’s transition to the CNCF. We encourage active users to contribute to our next release, and provide links to join the community at the end. . Selected and Highlighted deliveries . Katib . Katib includes new enhancements to the UI and SDK. The new Katib UI provides simplified fine-grained configuration and log correlation. Additionally sorting and filtering have been added, allowing a more organized view of your many experiments. In addition, these features minimize the necessity of manually employing low-level commands to locate and correlate logs with hyperparameter experiment configurations. This simplifies the process of in-depth performance analysis and subsequent iterations of model parameters. . In 1.7, the Katib SDK provides new features including a Tune API and the ability to retreive trial metrics from the Katib database. Model developers or data scientists can execute the Tune API to start a HyperParameter Experiment without any knowledge of underlying systems e.g. Kubernetes, Docker. It automatically converts user training scripts to a Katib Experiment. The Katib changelog provides details on over 100 updates and bug fixes including these SDK and UI top features: . . Narrow down Katib RBAC rules (#2091) | Support Postgres as a Katib DB (#1921) | [SDK] Create Tune API in the Katib SDK (#1951) | [SDK] Get Trial Metrics from Katib DB (#2050) | More Suggestion container fields in Katib Config (#2000) | Katib UI: Enable pagination/sorting/filtering (#2017 and #2040) | Katib UI: Add authorization mechanisms (#1983) | . . Training operator . Kubeflow’s unified distributed training operator enhancements include configuration options for fine tuned resource scaling (processor, memory, storage). It now includes HPA support for Pytorch Elastic workloads where users can specify target metric/utilization in Job Spec. This is used for automatic scale up/down of Pytorch Job matching demand while ensuring the elastic policy configured by the user. These enhancements simplify user workflows significantly and reduce operational toil and costs. The Job Spec is flexible and supports multiple scheduler types: Kubernetes, volcano, custom. Major 1.7 training operator features include: . . PodGroup enhancements(#1574) | Integration with other training frameworks - Paddlepaddle(#520) | Enhancements on Pytorch Elastic training (#1645, #1626) | Support coscheduling plugin (#1722) | [SDK] Create Unify Training Client(#1719) | . . Pipelines . In Kubeflow 1.7, the Pipelines Working Group (KFP) has continued its efforts towards KFP v2 with their latest 2.0.0-alpha.7 release. This release includes the following key enhancements: . . Pipelines as components: Pipelines can themselves be used as components in other pipelines, just as you would use any other single-step component in a pipeline. (#8179, #8204, #8209, #8220) | Sub-DAG visualization that allows pipeline users to dive deep into sub-graph components of their pipeline. (#8326) | Miscellaneous bug and vulnerability fixes. | . . Model developers recognize the time-saving pythonic workflows in Kubeflow Pipelines, which speed iteration by not requiring the generation of new images for pre-prod experimentation. The new V2 UI and SDK in Kubeflow 1.7 provide valuable details on each pipeline step. This simplifies the correlation and analysis of parameters, metadata and artifacts during iteration. . Kubeflow web apps (Notebooks, Volumes, TensorBoards) and Controllers . Kubeflow 1.7 delivers new web apps enhancements that expose more information to the end users and improve their UI interactions. . A valuable new delivery is that all of the main tables have filtering #6754 and sorting #6742 functionalities as well as showing objects from all namespaces at once #6778. This allows the end users to navigate through their tools and apps (notebooks, tensorboards, volumes etc) more efficiently. . Additionally, 1.7 provides an update to the Notebooks form page #6826 as well as a dedicated page #6769 #6788 for the different types of tools managed by Kubeflow. These detailed pages allow users to view logs, events and configuration yamls and they also link from one another (i.e. going to a volume’s details page via a notebook details page). Previously these functions were only available through the Kubernetes API, which would require the user to have increased privileges and to have a more in-depth knowledge of Kubernetes CLI commands. With these new features the user has a simpler, more organized and more secure way of accessing crucial Kubeflow resource information. . Other notable features are small improvements on our user stories around PodDefaults. Aside from additional use-cases, like defining sidecar and init containers #6749, Kubeflow’s TensorBoard stack now integrates with PodDefaults #6874 #6924. These enhancements enable the re-use of the user’s existing PodDefaults to gain S3 access from both Notebook and TensorBoard servers. . Platform dependencies, breaking changes, add-ons . Kubeflow 1.7 includes hundreds of commits. The Kubeflow release process includes several rounds of testing by the Kubeflow working groups and Kubeflow distributions. Kubeflow’s configuration options provide a high degree of flexibility. After considering all of the testing options, the 1.7 Release Team narrowed the critical dependencies for consistent testing and documentation to the following. . . K8s | Istio | KNative | Kustomize | Cert Mgr | DEX | Argo | Tekton | Oidc-authservice | . 1.25 / 1.24 | 1.16 | 1.8.1 | 3.2 or 5.0 | 1.10.1 | 2.31.2 | 3.3.8 | 1.5 | e236439 | . . Another valuable platform enhancement is the support of additional processor architectures including IBM’s Power (#6684). This effort provides the foundation to add other processor types as well. . The 1.7 documentation includes overall installation instructions from the Manifest Working Group, and detailed feature reviews from each Kubeflow working group. Most of the working groups have broken their changelogs into subsections that highlight core features, UI enhancements, miscellaneous updates, bug fixes and breaking changes. . Working Group Changelogs including breaking changes . Notebooks | Training Operator | Katib | Pipelines | KServe | . The community has continued its work to identify core components and add-ons. Significant enhancements in add-ons include the continued integration with KServe’s v.10 release, as well as a new serving option from BentoML. The BentoML team has done a tremendous job in supporting the Kubeflow community in 1.7 and their documentation is excellent. Details on BentoML are available here. . What’s next . The community continues to see a large increase in activity since the announcement that Kubeflow will be donated to the CNCF by Google. The community holds regular meetings to review progress on the checklist items needed for the CNCF due diligence (meeting notes). . During the 1.7 release cycle, the community formed a Security Team, which is working to improve the security profile of Kubeflow components and their dependencies. The Security Team has completed these three deliveries: . . Set-up a github directory, slack channel and regular meeting schedule with notes | Created an inventory of container images used by Kubeflow | Created a list of common vulnerabilities and errors (CVEs) in the container images. | . Going forward, the Security Team will work to develop on-going policies and to remedy security issues. For example, fixing CVEs is an on-going maintenance requirement and this function is currently provided by Kubeflow distribution providers as a value added delivery. Some distributions and end-users are working to fix CVEs in the upstream projects and the Security Team is looking for help on defining and delivering those deliveries and expectations. . The Kubeflow team is working on integration efforts with the Ray and MLflow communities.The Ray integration progress has moved closer to user testing and users can find more information on this tracking issue. The MLflow integration is progressing and its integration is tracked here. . How to get started with 1.7 . For trying out Kubeflow 1.7 we recommend our installation page where you can choose between a selection of Kubeflow distributions. For more advanced users we recommend the manifest installation guide. . Join the Community . We would like to thank everyone for their contribution to Kubeflow 1.7, especially Dominik Fleischmann for his work as the v1.7 Release Manager. As you can see, the Kubeflow community is vibrant and diverse, solving real-world problems for organizations worldwide. . Want to help? The Kubeflow community Working Groups hold open meetings and are always looking for more volunteers and users to unlock the potential of machine learning. If you’re interested in becoming a Kubeflow contributor, please feel free to check out the resources below. We look forward to working with you! . . Visit our Kubeflow website or Kubeflow GitHub Page | Join the Kubeflow Slack channel | Join the kubeflow-discuss mailing list | Attend a weekly community meeting | .",
            "url": "https://blog.kubeflow.org/kubeflow-1.7-release/",
            "relUrl": "/kubeflow-1.7-release/",
            "date": " • Mar 29, 2023"
        }
        
    
  
    
        ,"post14": {
            "title": "Kubeflow has applied to become a CNCF incubating project",
            "content": "With support from the Kubeflow Project Steering Group, Google has just announced the application for Kubeflow to become an incubating project in the Cloud Native Computing Foundation (CNCF). . We in the Kubeflow community are excited about how far the project has come since its inception in 2017, and are blown away by the diverse community of contributors and users we have built together. Today, Kubeflow has close to 200 contributors from over 30 organizations. Over the past five years, the project has seen almost 20,000 GitHub stars, tens of thousands of commits, and hosted several user and contributor gatherings across the world. . What is Kubeflow . With the help of the project community, Kubeflow has developed into an end-to-end, extendable ML platform for Kubernetes, with mature components to address key stages of the ML lifecycle: . model development (Kubeflow Notebooks) | model training (Kubeflow Pipelines and Kubeflow Training Operator) | model serving (KServe) | automated machine learning (Katib) | . Kubeflow’s powerful development experience helps data scientists all around the world build, train, and deploy their ML models, and gives MLOps teams the tools they need to deploy and scale advanced workflows in a variety of infrastructures. . Project History . In May 2020, with the v1.0 release, Kubeflow reached maturity across a core set of its stable applications. In 2022, we also graduated Kubeflow Serving as an independent project, KServe, which now, is incubating in Linux Foundation AI &amp; Data. . The six releases since v1.0 have truly been a community effort. The community, collaborating together in 16 Working Groups, brought to project a number of key improvements in ML workflow simplicity, resource efficiency, security, and tooling. The wider Kubeflow ecosystem has also added 10 Kubeflow distributions across multiple Cloud and on-prem environments. . Our application for Kubeflow to become a CNCF incubating project is the next big step for the project. We’re excited to see Kubeflow get closer to its roots in the cloud-native ecosystem, and for the community to continue driving innovation in the MLOps and data science experiences. . What’s Next . The pull request we’ve opened today to join CNCF as an incubating project is only the first step. Google and the Kubeflow community will work with the CNCF and their Technical Oversight Committee (TOC), to meet the incubation stage requirements. While the due diligence and eventual TOC decision will take some time, the Kubeflow project will continue developing and releasing throughout this process. . If Kubeflow is accepted into CNCF, the project will transfer to the CNCF its source code, trademark, website, and other collaboration assets and social media accounts. As part of the donation, we will also work with the Kubeflow community to update the project governance to ensure diverse representation of the broader community. Open source comes with the responsibility to contribute, sustain, and improve the projects to make our ecosystem better. We at Google will continue to support Kubeflow and participate in the Kubeflow community as project maintainers, contributors, and users. . A huge thank you to everyone who has contributed to Kubeflow over the years! We are excited for what lays ahead. . By Thea Lamkin, Senior Program Manager, Google, on behalf of the Kubeflow Project Steering Group .",
            "url": "https://blog.kubeflow.org/kubeflow-applied-cncf-incubating/",
            "relUrl": "/kubeflow-applied-cncf-incubating/",
            "date": " • Oct 24, 2022"
        }
        
    
  
    
        ,"post15": {
            "title": "Kubeflow v1.6 delivers support for Kubernetes v1.22 and introduces an alpha release of the Kubeflow Pipeline v2 functionality",
            "content": "The Kubeflow Community is excited to announce the availability of the Kubeflow v1.6 software release, which includes support for Kubernetes v1.22 and introduces an alpha release of the Kubeflow Pipeline v2 (KFP v2) functionality. . Kubeflow v1.6 also adds new hyperparameter support for the population based algorithm in Katib, and provides a combined Python SDK for PyTorch, MXNet, MPI, XGBoost in Kubeflow’s distributed Training Operator. For model serving, v1.6 has new ClusterServingRuntime and ServingRuntime CRDs, and a new Model Spec was introduced to the InferenceService Predictor Spec, which provides a new way to specify models in KServe 0.8. Additionally, v1.6 cleans up a few CVEs in the central dashboard and enables the PodDefaults webhook to pick-up new certificate updates. For the Kubernetes upgrade, the community developed and tested each Kubeflow component manifest with Kubernetes v1.22. As v1.22 introduced some breaking changes, the upgrade was a team effort, and this Kubeflow release management process will be useful in the community’s future updates of Kubernetes and other software dependencies. The software is available here. . In parallel to developing these new software features, the Kubeflow Community also completed its annual user survey. The survey generated many good user insights into requirements for Kubeflow, and you can see more on the survey results. Of the many highlights, we would like to identify the growing request for model monitoring as shown in the chart below. . . We believe the increased focus on monitoring identifies Kubeflow’s maturing user base. The Community is making efforts to explain Kubeflow’s current functionality and build into the users’ new model monitoring requirements. In addition, the user survey results and Kubeflow Working Group roadmaps will be discussed in the upcoming Kubeflow Summit. You can learn more about the Kubeflow Summit here. . To round back to the current software delivery, the following takes a deeper look at the v1.6 highlights in more detail. . Kubeflow v1.6 details . From a new feature introduction standpoint, Kubeflow v1.6 includes an experimental release of the KFP v2’s new front-end, back-end and SDK, which deliver a modern UI and DAG, first class support for metadata, and a simplified component authoring experience. This slide deck, KFP v2 Introduction, provides a good overview of v2. . . The KFP v2 alpha introduces: . An Argo-agnostic approach for creating and running pipelines | A brand new DAG visualization, which uses the Pipeline Template and MLMD in this pipeline spec | Streamlined Component Authoring | . . You can learn more about KFP v2 and the related breaking changes in these docs: . KFP v2 Introduction | The SDK reference | Breaking changes including an SDK change from kfp.v2 to kfp | . . For those users who are still relying on the KFP v1 functionality, Kubeflow v1.6 and its KFP v2 component are fully tested and supported with the mature features in the legacy Kubeflow Pipelines v1.8 SDK. This provides the same functionality that thousands of KFP v1 users leverage in production today. . . In addition to the KFP v2 alpha, Kubeflow 1.6 includes feature enhancements and operational improvements for Katib users. These include support for: . The population based training algorithm | Enhanced validation checks for configurations, which will save time debugging parameter misconfiguration | Security fixes | MetricsUnavailable Status support, which will make debugging easier | . . In Kubeflow v1.6, the Training Operator Working Group added these valuable enhancements: . Python SDK for PyTorch, MXNet, MPI, XGBoost | The Clientset (Golang) is also generated for PyTorch, MXNet, MPI, XGBoost | Gang scheduling support for MPI | . . Kubeflow v1.6 has several functional and operational improvements to Kubeflow notebooks, central dashboard, webapps and controllers. The following highlights the closed PRs that were considered medium-sized or larger. You can review the full list of the closed PRs here. . Support for K8s 1.22 | PodDefaults webhook picking up new certificates | Show objects from all names spaces in Central Dashboard, Jupyter, Tensorboard, Volumes Mgr | Updated the hosting container registry of images, note - this could be a breaking change for users with custom manifests | CVE fixes for Central Dashboard | . . The Manifest Working Group contributed several enhancements in 1.6. These enhancements include the testing of each Kubeflow’s component manifest for: . Compatibility with K8s 1.22 | The manifests that can be applied | The Pods of components that can become ready | The basic objects that can be created | . . For model serving, Kubeflow v1.6 incorporates the KServe v0.8.0 release, which includes these enhancements: . ClusterServingRuntime and ServingRuntime CRDs | A new Model Spec is introduced to the InferenceService Predictor Spec as a new way to specify models | Support for Knative 1.0+ | gRPC for transformer to predictor network communication | Multi-namespace support for the ModelMesh alternative backend | . . KServe has these breaking changes to the Python SDK: . KFModel is renamed to Model | KFServer is renamed to ModelServer | KFModelRepository is renamed to ModelRepository | . To learn more please see the KServe v0.8 release notes and Release blog post. . Join the community . We would like to thank everyone for their contribution to Kubeflow 1.6, especially Anna Jung for her work as the v1.6 Release Manager. The Kubeflow community is incredibly pleased to have Amazon Web Services extending their support by offering AWS promotional credits. We hope this sponsorship will enable many Kubeflow Working groups to sustainably host their testing and CI/CD infrastructure on AWS, which is essential for maintaining the community’s high development velocity. As you can see, the Kubeflow community is vibrant and diverse, solving real-world problems for organizations worldwide. . Want to help? The Kubeflow community Working Group hold open meetings and are always looking for more volunteers and users to unlock the potential of machine learning. If you’re interested in becoming a Kubeflow contributor, please feel free to check out the resources below. We look forward to working with you! . Visit our Kubeflow website or Kubeflow GitHub Page | Join the Kubeflow Slack channel | Join the kubeflow-discuss mailing list | Attend a weekly community meeting | .",
            "url": "https://blog.kubeflow.org/kubeflow-1.6-release/",
            "relUrl": "/kubeflow-1.6-release/",
            "date": " • Aug 24, 2022"
        }
        
    
  
    
        ,"post16": {
            "title": "Kubeflow User Survey 2022",
            "content": "In May 2022, the Kubeflow user survey was opened to gather community feedback. The goal of the survey was to understand the adoption of Kubeflow and to gather input on the benefits, gaps, and requirements for machine learning use cases. . The survey was comprised of 24 questions (multiple choice and freeform). It ran from May 6th to June 7th and received 151 responses. Out of the 151 responses, 91 responses provided optional feedback on how to improve Kubeflow and the Community. In the 2022 survey, we received an increased number of freeform answers, which provided additional insights into users’ needs. . Key Takeaways . 85% of the users deploy more than one Kubeflow component | The top 3 Kubeflow components are Pipelines (89%), Notebooks (75%), KServe (Formally KFServing) (63% - the combined result of distinct users using KServe or KFServing or both). | Data preprocessing and transformations are the most challenging (44%) and time-consuming (73%) steps in the ML lifecycle | 59% of the users identify model monitoring as the biggest gap in their ML lifecycle and 32% of the users identify model monitoring as the most challenging | 44% of the users are running Kubeflow in production | 90% of the users rely on the Kubeflow community for the latest tutorials | The top 5 ML tools used by the Kubeflow users are Tensorflow (66%), Scikit-Learn (61%), PyTorch (60%), Keras (44%), and MLflow (43%) | 47% of the users are keeping up with the latest Kubeflow 1.5 release, and 42% are running Kubeflow 1.4 | Lack of documentation and tutorials are the biggest challenges in Kubeflow adoption | Installation and upgrading of Kubeflow are a challenge for the users | Users like to see security issues, especially CVEs in images addressed promptly | Namespace isolation is the top feature request from the community | . Survey Respondents . The Kubeflow user survey drew responses from 151 members of the community with experience running Kubeflow in production (44%), in the lab (23%), have experience upgrading Kubeflow cluster (18%), contributing to Kubeflow (10%), and just starting (5%). . The majority of the respondents were from the Tech industry (48%), followed by Finance (15%) and Healthcare (8%). . The top job titles were ML Engineers (47%), Architect (26%), and Data Scientist / Analyst (23%). . . . Documentation and Tutorials . Lack of documentation and tutorials are the biggest challenges in Kubeflow adoption. With new features and releases of components, the community is seeking better examples and tutorials that would help them adopt the new features. . In addition, many are still finding Kubeflow to be complicated. Users asked for additional clarity on the inner workings of Kubeflow and how to integrate with many other ML components, including other Kubeflow components. . A few of the requests from the community include . Up-to-date versioned documentation | A hosted playground platform to demo and learn features of Kubeflow | Comparison between Kubeflow and other ML tools | Tips and knowledge around running Kubeflow in production | End-to-end tutorials on how to get the most out of all components that make up Kubeflow release | Better example documentation from Kubeflow distributions owners | Q&amp;A Forum | . . Installation and Upgrades . One of the top three voted answers to the biggest gap in Kubeflow was installation (tied with security). In addition, &gt; 25% of the freeform answers mentioned feedback about the installation process and/or installation tools that users would like to see supported. . Users are looking for easier installation with better support to upgrade their Kubeflow components and cluster. . Security . “… with the current state of Kubeflow, none of our customers is able to use Kubeflow without extensive modifications, as there are widely gaping security issues and tons of CVEs across all images that need to be patched before going anywhere near production.” . One of the top three voted answers to the biggest gap in Kubeflow was security (tied with installation). In addition, 16% of the freeform answers mentioned the need to improve security in Kubeflow, with top concerns being CVEs in images and user isolation. . . Kubernetes Versions . “Follow k8s release cycles ahead of time, there must be support for the latest k8s versions before the oldest get officially EOL’d. Note also major changes in 1.25!” . The community raised valid concerns about Kubeflow’s lack of support for the latest Kubernetes versions. With Kubernetes 1.21 reaching its end of life on June 28th, 2022, the users found the lack of 1.22 support in the 1.5 release problematic and requested Kubeflow to keep up with the release of its dependencies. . Namespace Isolation . About 15% of the free-from responses contained the word “namespace” in relation to resource isolation that users like to see supported by Kubeflow. Many users are seeking namespaced isolation across various Kubeflow resources, which include pipelines, experiments, artifacts, and metadata. . From all the list of features requested in the survey, namespace isolation is the top request from the community. . Comparison to 2021 User Survey . Challenges with Monitoring . In both 2021 and 2022, Kubeflow users identified that data preprocessing and transformation are the most time-consuming and challenging steps in the machine learning lifecycle. While the top answers stayed the same, there were big changes to the follow-up rankings for the most challenging question. . In 2021, the top 5 challenges were data preprocessing and transformation, pipeline building, feature engineering, hyperparameter tuning, and distributed training. . In 2022, the top 5 challenges were data preprocessing and transformation, feature engineering, model monitoring, distributed training, and pipeline building. The biggest difference from last year is the change in ranking for hyperparameter tuning and seeing model monitoring moving up in the ranks. . 2022 2021 As users find model monitoring one of the challenges in the machine learning lifecycle, it has also been identified as the biggest gap in users’ ML activities. . In 2021, the biggest gap was identified as connecting data pipelines to ML pipelines. In 2022, the biggest gap was identified as model monitoring. While the top three answers haven’t changed, data shows that monitoring is at the top of the mind of users as they find it to be most challenging and where the biggest gaps exist. . 2022 2021 Full Survey Results . Full survey results can be found in the initial survey results and in the summary of the free-from answers. . What’s Next? . Survey results will be discussed with the Kubeflow community during the Kubeflow summit in October. More details about the summit will be shared via the kubeflow-discuss mailing list. Join the mailing list to keep up to date with Kubeflow summit news. . Join the Community . We would like to thank everyone for their participation in the survey. As you can see from the survey results, the Kubeflow Community is vibrant and diverse, solving real-world problems for organizations worldwide. . Want to help? The Kubeflow Community Working Groups hold open meetings and are always looking for more volunteers and users to unlock the potential of machine learning. If you’re interested in becoming a Kubeflow contributor, please feel free to check out the resources below. We look forward to working with you! . Visit our Kubeflow website or Kubeflow GitHub Page | Join the Kubeflow Slack workspace | Join the kubeflow-discuss mailing list | Attend a weekly community meeting | .",
            "url": "https://blog.kubeflow.org/kubeflow-user-survey-2022/",
            "relUrl": "/kubeflow-user-survey-2022/",
            "date": " • Aug 19, 2022"
        }
        
    
  
    
        ,"post17": {
            "title": "Kubeflow 1.5.1 Release Announcement",
            "content": "The Kubeflow Community is excited to announce the release of Kubeflow 1.5.1. The Kubeflow 1.5.1 release provides a valuable enhancement to the caching feature of Kubeflow Pipeline (KFP) for any Kubernetes distribution with strict security policies for user application certificates. Caching is one of the crucial features of KFP which enables skipping a step that has already been executed which saves time and money. This release unblocks the caching feature to be used on distributions like AWS, the Mercedes Benz cluster, etc. . Kubeflow 1.5.1 includes the enhancements for the Kubeflow Pipelines cache server and the cert generation mechanism #2165. These enhancements improve the operations of Kubernetes clusters with strict security policies, especially in clusters where non-Kubelet applications cannot create API server type certificates. Kubeflow Pipelines introduced dependency on cert-manager for this enhancement. . A number of Kubeflow distribution providers are supporting Kubeflow 1.5.1. These cloud provider and vendor distributions include additional enhancements and professional support, which many users find valuable. Please find a list of the Kubeflow distribution providers here. . Special thanks to Kartik Kalamadi, Suraj Kota, Kimonas Sotirchos and James Liu for driving this effort! .",
            "url": "https://blog.kubeflow.org/kubeflow-1.5.1-release/",
            "relUrl": "/kubeflow-1.5.1-release/",
            "date": " • Jul 18, 2022"
        }
        
    
  
    
        ,"post18": {
            "title": "Kubeflow v1.5 improves ML model accuracy, reduces infrastructure costs and optimizes MLOps",
            "content": "The Kubeflow v1.5 software release improves ML model accuracy, lowers infrastructure costs, and simplifies operations by providing a more consistent user experience between components. . Lower infrastructure costs and improve model accuracy . Several enhancements in Kubeflow v1.5 lower infrastructure costs and help improve model accuracy. For example, new elastic training augments the Kubeflow training operator for PyTorch and enables PyTorch workers to be scaled up and down, providing fault tolerant and elastic training. This allows the training jobs to continue without restarting from scratch even if a worker fails. Elastic training can also enable the use of ephemeral or spot instances, which can save infrastructure costs. See the elastic training proposal for more details. . In addition, v1.5 extends notebook monitoring and culling. Kernel monitoring shuts down notebook servers that have been inactive when a configurable timer expires. Kubeflow v1.5 introduces greater precision in assessing notebook server UI activity. . Finally, v1.5 includes early validation of hyperparameter tuning. This feature improves model accuracy by reducing the overfitting of the model to the data sets used during hyperparameter tuning. It can also reduce the infrastructure used as it will stop hyperparameter tuning when configurable thresholds are reached. . Simplified operations . v1.5 simplifies operations in several ways including easier support of high availability for the Katib Controller via the new hyperparameter leader election. v1.5 also adds the MPI framework to the Unified Training Operator, which deploys a single operator for handling the most popular frameworks: Tensorflow, Pytorch, MXNet, XGBoost and MPI. Kubeflow Pipelines (KFP) has incorporated support for the Emissary Executor as the default executor, which means that users are not required to manually configure this option to Emissary. This enhancement also enables KFP to support the newer Kubernetes versions that require a Container Runtime Interface (CRI) and provides support for Docker containers. . In addition, KServe v0.7 is integrated with Kubeflow v1.5. For users who are still using KFServing v0.6.1, v1.5 does support the installation of the KFServing v0.6.1 release. For users migrating from KFServing to KServe, there is helpful documentation, which you can find here. From a feature standpoint, KServe v0.7 has added a beta of ModelMesh, which enables easier scaling of model serving and provides an architecture that overcomes pod and IP limitations that limit the number of models that can run on a single node and/or cluster. . Note on K8s 1.22: Although many of the Kubeflow services (KFP, AutoML, Training Operator, KServe) have started testing with K8s 1.22, v1.5 was not thoroughly tested with K8s 1.22. Additionally, critical work is still outstanding for Kubeflow Notebooks and other central services. While the Community always encourages testing, K8s 1.22 is not formally supported with Kubeflow v1.5. K8s 1.22 support is on the roadmap for Kubeflow’s next release and if you are interested in K8s 1.22 support, please add a comment on this tracking issue. . Streamlined user experience . In v1.5, the Kubeflow User Interface (UI) dashboards for Notebooks, Tensorboards and Volume Manager more closely match KFP’s dashboard. These enhancements include how dashboard fields are displayed and how operations are performed. This gives users a consistent look and feel when working in the UI. Additionally, the Kubeflow notebooks manager web app form and its configuration template have been updated for allowing users to fully define the PVC objects that will be created for a notebook. This will give end users the ability to modify crucial parts of the PVCs, such as StorageClasses, and to more easily support popular storage offerings, including NFS. In addition, AutoML has also updated its SDK, CI framework and parameter settings across frameworks (goptuna, optuna, hyperopt). . Installation, tutorials and documentation . The Kubeflow v1.5 release process includes improvements to Kubeflow’s installation, tutorial and documentation. For installation, the Manifest Working Group’s documentation provided the starting place for the Kubeflow distributions, and this collaboration has increased the maturity of the code and documentation for Kubeflow’s GitOps installation pattern. The v1.5 release process included a tracking issue for the critical web pages that are updated in each release. Additionally, there are many Working Group documentation updates and we invite you to open issues when you find documentation that needs improvement. Kubeflow v1.5 also includes two tutorials, which are described below: . Tutorial 1 - E2E MNIST with Kubeflow Tutorial,which provides an end-to-end test sequence i.e. start a notebook, run a pipeline, execute training, hyperparameter tuning and model serving. . Tutorial 2 - Deploy and run your first interfere service with KServe Tutorial Run your first interface service on Kubeflow 1.5 i.e. define, create and check an InferenceService, post an interfere request and receive response. Optionally run performance tests. . Join the community . The v1.5 Release Team would like to thank everyone for their efforts on Kubeflow v1.5, especially the users, code contributors, working group leads, and distribution team leads. As you can see from the extensive contributions to Kubeflow 1.5, the Kubeflow Community is vibrant and diverse, and solving real world problems for organizations around the globe. . Excited to contribute your great ideas? The Kubeflow Community Working Groups hold open meetings, public lists, and are always looking for more volunteers and users to unlock the potential of machine learning. If you’re interested in becoming a Kubeflow contributor, please feel free to check out the resources below. We look forward to working with you! . Visit our Kubeflow website or Kubeflow Github page | Join the Kubeflow Slack Channel | Join the kubeflow-discuss mailing list | Attend a weekly community meeting | .",
            "url": "https://blog.kubeflow.org/kubeflow-1.5-release/",
            "relUrl": "/kubeflow-1.5-release/",
            "date": " • Apr 1, 2022"
        }
        
    
  
    
        ,"post19": {
            "title": "Unified Training Operator release announcement",
            "content": "The Kubeflow Training Operator Working Group introduced several enhancements in the recent Kubeflow 1.4 release. The most significant was the introduction of the new unified training operator that enables Kubernetes custom resources (CR) for many of the popular training frameworks: Tensorflow, Pytorch, MXNet and XGboost. In addition, the tf-operator repository has been renamed to training-operator. . This single operator provides several valuable benefits: . Better resource utilisation - For releases prior to 1.4, each framework had a separate corresponding controller managing its distributed job. The unified training operator manages all distributed jobs across frameworks, which improves resource utilization and performance. | Less maintenance overhead - Unified training operator reduces the maintenance efforts in managing distributed jobs across the framework. By default, all supported schemas(TFJob, PyTorchJob, MXNetJob, XGBoostJob) are enabled. However, specific schemas can be enabled using the flag ‘enable-scheme’. Setting this flag enables the user to enable the framework(s) that are necessary for the deployment environment. | Easy adoption of new operators - Common code is abstracted from all framework implementations, which makes it easy for adopting new operators with less code. The common infrastructure code can be reused for many of the new operator efforts. Reference: Paddle operator proposal, DGL operator proposal | Better developer experience - Common features can be shared across frameworks without code duplication thereby, creating a developer friendly environment. For example, Prometheus Monitoring and Job Scheduling features are common, making them available to all frameworks without any extra code. | The unified training operator’s manifests include an enhanced training operator, which manages custom resource definitions for TFJob, PyTorchJob, MXNet Job and XGBoostJob. All individual operator repositories, including pytorch-operator, mxnet-operator, xgboost-operator, will be archived soon. Please check out the latest release for more details and give it a try! . Release highlights . Kubeflow 1.4 release includes the following major changes to training. . Universal Training Operator changes . Unified Training Operator for TF, PyTorch, MXNet, XGBoost #1302 #1295 #1294 #1293 #1296 | More common code refactoring for reusability #1297 | API code restructuring to consistent format #1300 | Prometheus counters for all frameworks #1375 | Python SDK for all frameworks #1420 | API doc for all frameworks #1370 | Restructuring of examples across all frameworks #1373 #1391 | . Common package updates . Make training container port customizable to support profiling #131 | Optimize the TTL setting of all Jobs #137 | More appropriate use of expectation for Jobs #139 | . MPI Operator updates . Scalability improvements to reduce pressure on kube-apiserver #360 | V2beta1 MPIJob API #366 #378 | Intel MPI Support #389 #403 #417 #425 | . MPI Operator roadmap . The MPI framework integration with the unified training operator is under development and is planned for delivery in the next release i.e. post 1.4. Currently, it needs to be separately installed using MPIJob manifests. . Acknowledgement . The unified training operator is the outcome of efforts from all existing Kubeflow training operators and aims to provide a unified and simplified experience for both users and developers. We’d like to thank everyone who has contributed to and maintained the original operators. . PyTorch Operator: list of contributors and maintainers | MPI Operator: list of contributors and maintainers | XGBoost Operator: list of contributors and maintainers | MXNet Operator: list of contributors and maintainers | . Join the WG-Training . If you want to help, or are looking for issues to work on, feel free to check the resources below! . Slack: #wg-training . Community: wg-training . Issues: https://github.com/kubeflow/training-operator/issues .",
            "url": "https://blog.kubeflow.org/unified-training-operator-1.3-release/",
            "relUrl": "/unified-training-operator-1.3-release/",
            "date": " • Oct 13, 2021"
        }
        
    
  
    
        ,"post20": {
            "title": "Kubeflow's 1.4 release lays the foundation for advanced ML metadata workflows",
            "content": "The Kubeflow 1.4 release lays several important building blocks for the use of advanced metadata workflows. A quick summary of 1.4’s top deliveries includes: . Advanced metadata workflows with improved metric visualization and pipeline step caching in Kubeflow Pipelines (KFP) via the KFP Software Development Kit (SDK) | A new KFServing model user interface that displays ML model status, configuration, yaml, logs, and metrics | New Optuna Suggestion Service with multivariate TPE algorithm and Sobol’s Quasirandom Sequence support for hyperparameter tuning | A new, unified training operator that supports all deep learning frameworks with a Python SDK, enhanced monitoring and advanced scheduling support | . Kubeflow 1.4 enables the use of metadata in advanced machine learning (ML) workflows, especially in the Kubeflow Pipelines SDK. With the Pipelines SDK and its new V2-compatible mode, users can create advanced ML pipelines with Python functions that use the MLMD as input/output arguments. This simplifies metrics visualization. . Another enhancement to Pipelines is the option to use the Emissary executor for non-Docker Kubernetes container runtime requirements. In addition, 1.4 can support metadata-based workflows to streamline the creation of TensorBoard visualizations and to serve ML models. . Core improvements to code, process, and documentation . For the Kubeflow Working Groups, 1.4 was primarily a maintenance release, which enabled the Community to concentrate on core improvements to code, process, and documentation. In the 2021 Kubeflow User Survey, users requested documentation improvements (please see the figure below). The Kubeflow 1.4 release cycle included the 1.4 Docs Sprint that generated nearly fifty (50) PRs. These PRs were tracked in this issue and this Kanban board, and we encourage more users to contribute by reading and improving the Kubeflow documentation. . . The 1.4 release improvements simplify future feature development by reducing redundant code, increasing CI/CD, and automating testing. An important delivery was the new Unified Training Operator for Tensorflow, PyTorch, MXNet, and XGBoost PR#1302. 1.4 also initiated the Community’s adoption of a defined release process in its new Kubeflow Release Handbook. The Handbook defines the stages of the release and contributors’ roles, which has helped to improve responsibilities and quality. . Simplified installation . As shown in the Kubeflow User Survey (see the figure above), users have also asked for installation improvements. In Kubeflow 1.3, the Community refactored the Kubeflow deployment pattern to use manifests files (in yaml or json), which are stored in Git repositories, and then deployed using the Kustomize installation tool. This flexible installation pattern simplifies customization by overlaying manifests. This pattern is now being exploited in 1.4. . In 1.4, the Community provides an upstream set of base manifests in the Kubeflow manifest repo. Third parties have built custom installation guides or distributions with overlays that extend the base manifests. In 1.4, the third party overlays were removed from the Kubeflow manifest repo and moved to the repository of their choosing. This pattern provides third parties more flexibility to upgrade and document their overlays. You can see a full set of installation guides and distributions here. . In addition, on-prem Kubeflow users can use the base installation manifests which utilize open source solutions like Istio, Dex, and AuthService for authentication. The Community and the Manifests Working Group are actively working to provide extra overlays and patches to accommodate more advanced use cases and installations. For example, we recently configured Knative to work with the AuthService and Dex. . Dependencies, change logs, tracking issues and roadmaps . Kubeflow has many software dependencies. In 1.4, the top dependencies used in testing are defined below: . Kubeflow Dependency | Version | . Kubernetes | 1.19.0 | . Istio | 1.9.6 | . Knative | 0.22.1 | . Kustomize | 3.2.0 | . This chart provides links to important details from the Working Groups, including their 1.4 tracking issues, change logs, and roadmaps. Please note that the Working Groups use version numbers that are specific to their project. As a result, many Kubeflow components, which have been incorporated and tested in Kubeflow 1.4, may have a different version number than 1.4. . Working Group | Changelog / Release Notes | Roadmap | . Notebooks | 1.4, releases/tag/v1.4.0 | kubeflow/issues/5978 | . Training Operators | 1.3, Training Operator Changelog | Training Operators Roadmap | . Katib | V0.12, Katib Release Notes PR for v0.12 | Katib Roadmap | . Kubeflow Pipelines | v1.7 Release Notes, Changelog | Pipelines Roadmap | . KFServing | v0.6.1, kfserving/releases/tag/v0.6.1 | KFServing Roadmap | . Kubeflow 1.4 video update and tutorials . The Kubeflow Working Group representatives have recorded a presentation on Kubeflow 1.4’s new features, which you can find on the Kubeflow YouTube channel. Additionally, Kubeflow 1.4’s new features are easy to try in these tutorials: . AutoML Tutorial with metadata based workflows to build TensorBoards and to serve models | Run Katib from your local laptop by following this example. | KFP Tutorial using Pipelines SDK v2 to orchestrate your ML workflow as a pipeline | KFServing Tutorial | Training Operator Tutorial | . What’s coming . The Kubeflow Community is working on Kubeflow 1.5 planning and the Kubeflow Conformance Program proposal. Please watch blog.kubeflow.org for updates on these topics and more. . Join the community . We would like to thank everyone for their efforts on Kubeflow 1.4, especially the users, code contributors and working group leads. As you can see from the extensive contributions to Kubeflow 1.4, the Kubeflow Community is vibrant and diverse, and solving real world problems for organizations around the world. . Want to help? The Kubeflow Community Working Groups hold open meetings, public lists, and are always looking for more volunteers and users to unlock the potential of machine learning. If you’re interested in becoming a Kubeflow contributor, please feel free to check out the resources below. We look forward to working with you! . Visit our Kubeflow website or Kubeflow GitHub page | Join the Kubeflow Slack channel | Join the kubeflow-discuss mailing list | Attend a weekly community meeting | .",
            "url": "https://blog.kubeflow.org/kubeflow-1.4-release/",
            "relUrl": "/kubeflow-1.4-release/",
            "date": " • Oct 12, 2021"
        }
        
    
  
    
        ,"post21": {
            "title": "KServe: The next generation of KFServing",
            "content": "By Dan Sun and Animesh Singh on behalf of the Kubeflow Serving Working Group . KFServing is now KServe . We are excited to announce the next chapter for KFServing. In coordination with the Kubeflow Project Steering Group, the KFServing GitHub repository has now been transferred to an independent KServe GitHub organization under the stewardship of the Kubeflow Serving Working Group leads. . The project has been rebranded from KFServing to KServe, and we are planning to graduate the project from Kubeflow Project later this year. . . Developed collaboratively by Google, IBM, Bloomberg, NVIDIA, and Seldon in 2019, KFServing was published as open source in early 2019. The project sets out to provide the following features: . A simple, yet powerful, Kubernetes Custom Resource for deploying machine learning (ML) models on production across ML frameworks. | Provide performant, standardized inference protocol. | Serverless inference according to live traffic patterns, supporting “Scale-to-zero” on both CPUs and GPUs. | Complete story for production ML Model Serving including prediction, pre/post-processing, explainability, and monitoring. | Support for deploying thousands of models at scale and inference graph capability for multiple models. | . KFServing was created to address the challenges of deploying and monitoring machine learning models on production for organizations. After publishing the open source project, we’ve seen an explosion in demand for the software, leading to strong adoption and community growth. The scope of the project has since increased, and we have developed multiple components along the way, including our own growing body of documentation that needs it’s own website and independent GitHub organization. . What’s Next . Over the coming weeks, we will be releasing KServe 0.7 outside of the Kubeflow Project and will provide more details on how to migrate from KFServing to KServe with minimal disruptions. KFServing 0.5.x/0.6.x releases are still supported in next six months after KServe 0.7 release. We are also working on integrating core Kubeflow APIs and standards for the conformance program. . For contributors, please follow the KServe developer and doc contribution guide to make code or doc contributions. We are excited to work with you to make KServe better and promote its adoption by more and more users! . . KServe Key Links . Website | Github | Slack(#kubeflow-kfserving) | . Contributor Acknowledgement . We’d like to thank all the KServe contributors for this transition work! . Andrews Arokiam . | Animesh Singh . | Chin Huang . | Dan Sun . | Jagadeesh . | Jinchi He . | Nick Hill . | Paul Van Eck . | Qianshan Chen . | Suresh Nakkiran . | Sukumar Gaonkar . | Theofilos Papapanagiotou . | Tommy Li . | Vedant Padwal . | Yao Xiao . | Yuzhui Liu . | .",
            "url": "https://blog.kubeflow.org/release/official/2021/09/27/kfserving-transition.html",
            "relUrl": "/release/official/2021/09/27/kfserving-transition.html",
            "date": " • Sep 27, 2021"
        }
        
    
  
    
        ,"post22": {
            "title": "Blog: Running Kubeflow at Intuit: Enmeshed in the service mesh",
            "content": "Deploying Kubeflow 1.3 in an enterprise with existing Kubernetes infrastructure, a Service Mesh (Istio), and Argo, presents a host of challenges. This blog will address how those challenges were overcome while retaining the best practices that both the organization and Kubeflow prescribe. . Lay of the land at Intuit . Intuit has invested heavily in building out a robust Kubernetes infrastructure that powers all of Intuit’s products: TurboTax, QuickBooks, and Mint. There are thousands of services that run on over a hundred Kubernetes clusters. Managing these clusters is the Intuit Kubernetes Service (IKS) control plane. The IKS control plane provides services such as namespace management, role management, and isolation, etc. Connecting the services is an advanced, Istio-based service mesh, which complements Intuit’s API Gateway. In combination, they provide robust authentication, authorization, rate limiting, and other routing capabilities. . The Intuit ML Platform is built on this ecosystem and provides model training, inference, and feature management capabilities, leveraging the best of Intuit’s Kubernetes infrastructure and AWS SageMaker. This is the backdrop against which we started exploring Kubeflow to provide advanced orchestration, experimentation, and other services. . Kubeflow and Istio . Our first challenge with running Kubeflow was the compatibility of Kubeflow’s Istio with Intuit’s existing Service Mesh built on top of Istio. Two key problems emerged: version compatibility and operational maintenance. . Kubeflow v1.3 defaults to Istio (v1.9), and luckily it is compatible with the older versions of Istio (v1.6), which is what Intuit runs on. Running two Istio versions is impractical, as that would defeat the benefit of a large, interconnected existing service mesh. Hence, we wanted Kubeflow to work seamlessly with Intuit’s service mesh running Istio v1.6. . If you are new to Istio, you might want a primer on these key Traffic Management Components and Security Components: . VirtualService | DestinationRule | Gateway | EnvoyFilter | AuthorizationPolicy | Step 1: Remove default Istio configurations and Argo from Kubeflow . The first step to running Kubeflow was to remove the Istio and Argo bundled with Kubeflow so that it could be integrated with the Intuit service mesh. . To Remove Kubeflow’s default Istio . We have used Kustomize to build the manifest we need for our Kubeflow installation and we are using ArgoCD to deploy the Kubeflow Kubernetes manifests. . . ├── base # Base folder for the kubeflow out of the box manifests │ ├── kustomization.yaml │ ├── pipelines # Folder for Kubeflow Pipelines module │ │ ├── kustomization.yaml │ ├── other modules # Similar to the Pipelines module you can bring other modules as well │ ├── kustomization.yaml ├── envs # Folder for all the Kubeflow environments │ ├── prod │ │ ├── kustomization.yaml │ ├── dev │ ├── kustomization.yaml . base -&gt; kustomization.yaml . apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - github.com/kubeflow/manifests/common/kubeflow-roles/base?ref=v1.3.0 - github.com/kubeflow/manifests/common/kubeflow-namespace/base?ref=v1.3.0 - github.com/kubeflow/manifests/common/oidc-authservice/base?ref=v1.3.0 - github.com/kubeflow/manifests/apps/admission-webhook/upstream/overlays/cert-manager?ref=v1.3.0 - github.com/kubeflow/manifests/apps/profiles/upstream/overlays/kubeflow?ref=v1.3.0 - github.com/kubeflow/manifests/apps/centraldashboard/upstream/overlays/istio?ref=v1.3.0 - pipelines . base -&gt; pipelines -&gt; kustomization.yaml . apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization bases: - github.com/kubeflow/pipelines/manifests/kustomize/base/installs/multi-user?ref=1.5.0 - github.com/kubeflow/pipelines/manifests/kustomize/base/metadata/base?ref=1.5.0 - github.com/kubeflow/pipelines/manifests/kustomize/base/metadata/options/istio?ref=1.5.0 # To remove the default Argo from Pipelines module # - github.com/kubeflow/pipelines/manifests/kustomize/third-party/argo/installs/cluster?ref=1.5.0 - github.com/kubeflow/pipelines/manifests/kustomize/third-party/mysql/base?ref=1.5.0 - github.com/kubeflow/pipelines/manifests/kustomize/third-party/mysql/options/istio?ref=1.5.0 - github.com/kubeflow/pipelines/manifests/kustomize/third-party/minio/base?ref=1.5.0 - github.com/kubeflow/pipelines/manifests/kustomize/third-party/minio/options/istio?ref=1.5.0 - github.com/kubeflow/pipelines/manifests/kustomize/third-party/metacontroller/base?ref=1.5.0 # Identifier for application manager to apply ownerReference. # The ownerReference ensures the resources get garbage collected # when application is deleted. commonLabels: application-crd-id: kubeflow-pipelines # !!! If you want to customize the namespace, # please also update base/cache-deployer/cluster-scoped/cache-deployer-clusterrolebinding.yaml namespace: kubeflow . Note: we had to create a separate folder for pipelines because we didn’t want to use Argo, which comes with the Pipelines module. If you can use default Argo, then you can simply use https://github.com/kubeflow/manifests/apps/pipeline/upstream/env/platform-agnostic-multi-user-pns?ref=v1.3.0 instead of the pipelines folder. . If you don’t want to use ArgoCD, you can build the manifest using the kustomize build command, which is essentially what ArgoCD does. The configuration above has been tested for Kustomize 3.8.x and 4.0.x, and it works with both. . Step 2: Kustomize the Kubeflow manifests . Given the managed Kubernetes ecosystem at Intuit, protocols for service to service communication and namespace isolation is opinionated, and we had to make the following changes: . Enable Kubeflow namespace for Istio injection by adding the label istio-injection: enabled in the namespace specification. This label is then used by Istio to add the sidecar into the namespace. | Enable sidecar injection to all the deployments and statefulsets in Kubeflow by adding the annotation sidecar.istio.io/inject: &quot;true&quot;, along with some Intuit-specific custom labels and annotations to the Deployments and StatefulSets. | Intuit’s security policies forbid the direct use of external container registries. Intuit’s internal container registry runs regular vulnerability scans and certifies Docker images for use in various environments. The internal container registry also has an allow list that enables external registries to be proxied and held to the same, high-security standards. We enabled it for all Kubeflow containers. | Changes in VirtualService to route all the traffic from one central gateway instead of using Kubeflow gateway. | We have used Kustomize to modify the Kubeflow application manifest. . For adding labels, we have used LabelTransformer apiVersion: builtin kind: LabelTransformer metadata: name: deployment-labels labels: &lt;Intuit custom labels&gt; istio-injected: &quot;true&quot; fieldSpecs: - path: spec/template/metadata/labels kind: Deployment create: true - path: spec/template/metadata/labels kind: StatefulSet create: true . | For adding annotations, we have used AnnotationsTransformer . apiVersion: builtin kind: AnnotationsTransformer metadata: name: deployment-annotations annotations: &lt;Intuit custom annotations&gt; sidecar.istio.io/inject: &quot;true&quot; fieldSpecs: - path: spec/template/metadata/annotations kind: Deployment create: true - path: spec/template/metadata/annotations kind: StatefulSet create: true . | For replacing docker image URLs, we used ImageTagTransformer . apiVersion: builtin kind: ImageTagTransformer metadata: name: image-transformer-1 imageTag: name: gcr.io/ml-pipeline/cache-deployer newName: docker.intuit.com/gcr-rmt/ml-pipeline/cache-deployer . It will be helpful for any organization which has a proxy for accessing the internet, cloning all the container images local to your org is the way to go as the internet will not be required to access those images. . | For transforming VirtualServices . - op: remove path: /spec/hosts/0 - op: replace path: /spec/gateways/0 value: &lt;custom gateway&gt; - op: add path: /spec/hosts/0 value: &lt;kubflow host name&gt; - op: add path: /spec/exportTo value: [&quot;.&quot;] . | Putting it all together . envs -&gt; prod/dev -&gt; kustomization.yaml . apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ../base transformers: - transformers/image-transformers.yaml - transformers/label-transformers.yaml - transformers/annotations-transformers.yaml patchesJson6902: # patch VirtualService with explicit host # add multiple targets like below for all the VirtualServices which you need - path: patches/virtual-service-hosts.yaml target: group: networking.istio.io version: v1alpha3 kind: VirtualService name: centraldashboard . | You might face issues with the metadata_envoy service, in our case we were getting the following error [debug][init] [external/envoy/source/common/init/watcher_impl.cc:27] init manager Server destroyed unable to bind domain socket with id=0 (see --base-id option) 2021-01-29T23:32:26.680310Z error Epoch 0 exited with error: exit status 1 . After looking up, we found that, when you run this docker image with Istio Sidecar injection, this problem occurs. The reason is, both these containers are essentially envoyproxy containers and the default base-id for both containers is set to 0. . So to make it work, we had to change CMD in this Dockerfile . CMD [&quot;/etc/envoy.yaml&quot;, &quot;--base-id&quot;, &quot;1&quot;] . | Step 3: Custom changes needed for SSO . There are two major components around authentication using SSO: . Authservice: It is a StatefulSet that runs the oidc-auth service. It runs in the istio-system namespace and directly talks to an OIDC service for authentication | Authn-filter: It’s an EnvoyFilter that filters the traffic to authservice and checks the Kubeflow auth header and redirects to authservice if the request is not authorized, check the presence of header called kubeflow-userid | Note: Intuit SSO supports OIDC, so we did not need to use dex for the integration. If your org’s SSO does not support OIDC, then you can use dex in the middle; details can be found here. . For our installation, we needed the authservice to be mesh-enabled, and it made more sense to move authservice to the kubeflow namespace as well, which was already enabled for Istio sidecar injection. . After enabling Istio mesh on authservice, some more changes were required in the default manifest for it to work. The authservice pod was not able to communicate with the Intuit SSO HTTPS URL, because outbound traffic from the main container pod is intercepted by Istio sidecar to enforce mtls (default behavior). So, we had to exclude the HTTPS port (443) to disable mtls. This can be done using the annotation traffic.sidecar.istio.io/excludeOutboundPorts: &quot;443&quot;. . Step 4: Setting up ingress . We exposed the istio-ingressgateway service as LoadBalancer using the following mechanism: . Setting up public hosted zone in Route 53, add hostname you would like to use, like example.com | Setting up an ACM certificate for the hostname you want to use for the Kubeflow installation, the hostname can be kubeflow.example.com | Updating the service manifest by adding a few annotations: # Note that the backend talks over HTTP. service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http # TODO: Fill in with the ARN of your certificate. service.beta.kubernetes.io/aws-load-balancer-ssl-cert: &lt;cert arn from step 2&gt; service.beta.kubernetes.io/aws-load-balancer-security-groups: &lt;to restrict access within org&gt; # Only run SSL on the port named &quot;https&quot; below. service.beta.kubernetes.io/aws-load-balancer-ssl-ports: &quot;https&quot; external-dns.alpha.kubernetes.io/hostname: kubeflow.example.com . | After applying the new manifest, AWS will automatically add the appropriate A and TXT entries in your hosted zone (example.com) and Kubeflow will be accessible at kubeflow.example.com. . To secure the Gateway with https, you can also change the gateway port and add the key and certificate in the Gateway. . More about these annotations can be found at Terminate HTTPS traffic on Amazon EKS and SSL support on AWS blog. . Step 5: Using an external Argo installation . Kubfelow uses Argo workflows internally to run the pipeline in a workflow fashion. Argo generates artifacts after the workflow steps and all we need to do is configure the artifact store if we are planning to use the external Argo: . Install Argo workflows in your cluster, it gets installed in a namespace called argo. | Remove all the Argo-related manifests from Kubeflow. | To override the artifact store, you need to change the ConfigMap workflow-controller-configmap which comes with the Kubeflow manifest. It uses minio as the store but you can configure it to use S3 as well. More details can be found from the ArgoWorkflow Controller Configmap GitHub page. | The latest version of Argo has the option to override artifact store for namespace as well. | Debugging tricks . Check if EnvoyFilter is getting applied: you should have the istioctl cmd tool: . istioctl proxy-config listeners &lt;pod name&gt; --port 15001 -o json . See if the envoy filter is getting listed in the output. More about Istio proxy debugging can be found here. . | Check istio-ingressgateway: . # Port forward to the first istio-ingressgateway pod kubectl -n istio-system port-forward $(kubectl -n istio-system get pods -listio=ingressgateway -o=jsonpath=&quot;{.items[0].metadata.name}&quot;) 15000 # Get the http routes from the port-forwarded ingressgateway pod (requires jq) curl --silent http://localhost:15000/config_dump | jq &#39; &#39;&#39;.configs.routes.dynamic_route_configs[].route_config.virtual_hosts[]| {name: .name, domains: .domains, route: .routes[].match.prefix}&#39; &#39;&#39; # Get the logs of the first istio-ingressgateway pod # Shows what happens with incoming requests and possible errors kubectl -n istio-system logs $(kubectl -n istio-system get pods -listio=ingressgateway -o=jsonpath=&quot;{.items[0].metadata.name}&quot;) --tail=300 # Get the logs of the first istio-pilot pod # Shows issues with configurations or connecting to the Envoy proxies kubectl -n istio-system logs $(kubectl -n istio-system get pods -listio=pilot -o=jsonpath=&quot;{.items[0].metadata.name}&quot;) discovery --tail=300 . | Check the authservice connectivity: istio-ingressgateway pod should be able to access authservice. You can check that using the following command: . kubectl -n istio-system exec $(kubectl -n istio-system get pods -listio=pilot -o=jsonpath=&quot;{.items[0].metadata.name}&quot;) -- curl -v http://authservice.istio-system.svc.cluster.local:8080 . Also, make sure authservice can reach dex: . In our case, authservice is in the kubeflow namespace so we made changes accordingly using the command below: . kubectl -n kubeflow exec authservice-0 -- wget -q -S -O &#39;-&#39; &lt;oidc auth url&gt;/.well-known/openid-configuration . It should look something similar to: . { &quot;issuer&quot;: &quot;http://dex.kubeflow.svc.cluster.local:5556/dex&quot;, &quot;authorization_endpoint&quot;: &quot;http://dex.kubeflow.svc.cluster.local:5556/dex/auth&quot;, &quot;token_endpoint&quot;: &quot;http://dex.kubeflow.svc.cluster.local:5556/dex/token&quot;, &quot;jwks_uri&quot;: &quot;http://dex.kubeflow.svc.cluster.local:5556/dex/keys&quot;, &quot;userinfo_endpoint&quot;: &quot;http://dex.kubeflow.svc.cluster.local:5556/dex/userinfo&quot;, &quot;response_types_supported&quot;: [ &quot;code&quot; ], &quot;subject_types_supported&quot;: [ &quot;public&quot; ], &quot;id_token_signing_alg_values_supported&quot;: [ &quot;RS256&quot; ], &quot;scopes_supported&quot;: [ &quot;openid&quot;, &quot;email&quot;, &quot;groups&quot;, &quot;profile&quot;, &quot;offline_access&quot; ], &quot;token_endpoint_auth_methods_supported&quot;: [ &quot;client_secret_basic&quot; ], &quot;claims_supported&quot;: [ &quot;aud&quot;, &quot;email&quot;, &quot;email_verified&quot;, &quot;exp&quot;, &quot;iat&quot;, &quot;iss&quot;, &quot;locale&quot;, &quot;name&quot;, &quot;sub&quot; ] } . | Check connectivity between services: try using curl or wget from one service to another. Usually one or the other is always available, otherwise, you can always install using the apt-get command. Example use case: from the ml-pipeline deployment pod you can check if pipeline APIs are accessible. . kubectl -n kubeflow exec $(kubectl -n kubeflow get pods -lapp=ml-pipeline-ui -o=jsonpath=&quot;{.items[0].metadata.name}&quot;) -- wget -q -S -O &#39;-&#39; ml-pipeline.kubeflow.svc.cluster.local:8888/apis/v1beta1/pipelines . | Asks for the Kubeflow Community . The challenges that we encountered at Intuit are not unique and will be faced by any enterprise that wants to adopt Kubeflow. . It would be nice to have Kubeflow play well with the available Kubernetes infrastructure in an enterprise, rather than mandating its own set of infrastructure. Here are some suggestions/bugs for improving the ecosystem, some of which Intuit will work with the community to build out: . We saw Kubeflow manifest repo went through major folder restructuring for v1.3 but we think there is still room for improvements. | Multi-Cluster / Multi-Region support. #5467 | Upgrade seems to be an issue in general, should figure out a way to manage this better. #5440 | Multi-tenancy with group support. #4188 | Installing Kubeflow in any custom namespace. #5647 | Existing metadata service is not performant, we did try some settings with more resources and horizontal scaling. The community is already working on KFP v2.0, which might address a lot of concerns around metadata service. | References . Kubeflow Pipelines (KFP) v2 System Design | Traffic Management Components | Istio 1.6 Architecture | Istio 1.3 Architecture | Terminate HTTPS traffic on Amazon EKS | SSL support on AWS | Intuit’s Modern SaaS Platform | Stitching a Service Mesh Across Hundreds of Discrete Networks | Multicluster Istio configuration and service discovery using Admiral | Genius of Admiral | .",
            "url": "https://blog.kubeflow.org/running-kubeflow-at-intuit/",
            "relUrl": "/running-kubeflow-at-intuit/",
            "date": " • May 3, 2021"
        }
        
    
  
    
        ,"post23": {
            "title": "The Kubeflow 1.3 software release streamlines ML workflows and simplifies ML platform operations",
            "content": "The Kubeflow 1.3 release delivers simplified ML workflows and additional Kubernetes integrated features to optimize operational and infrastructure efficiencies. In addition to new User Interfaces (UIs), which improve ML workflows for pipeline building, model tuning, serving and monitoring, 1.3 also enables “headless” GitOps-inspired installation patterns. The latest version of Kubeflow provides users with a mature foundation and delivers a modern ML platform with best-in-class Key Performance Indicators (KPIs). . The Kubeflow user community is growing quickly, which was demonstrated in our recent survey results. When compared to last year’s survey, the 2021 Survey showed a 50% increase in responses and a whopping 300% increase in users supporting production deployments. As shown below, the user survey responses, especially from ML engineers, architects and data scientists, have identified where the Kubeflow contributors should focus their efforts. . Kubeflow User Survey Results - March 2021 . . Streamlined ML workflows delivered via new UIs . Data scientists will like the new and updated user interfaces (UIs) for Katib, TensorBoard, Persistent Volumes, Pipelines and Kale. These new UIs address many of the ML tasks that are time consuming and technically challenging. The UIs reduce the need for a data scientist to learn kfctl or docker CLI commands. . Below please find details on the UIs’ benefits for ML workflows: . Katib (Video Tour) The Katib UI is integrated with the central dashboard and streamlines hyperparameter tuning by presenting a visualization graph and a table that compares each trial’s performance along with its hyperparameters. You can also review the details of each trial’s algorithm, metrics collector and yaml. (Project 1) | . | TensorBoard (Video tour) The TensorBoard UI streamlines the TensorBoard configuration tasks, especially for logging of training jobs which are running in Notebooks or Pipelines. It simplifies accessibility to metrics, which helps you to improve model accuracy , identify performance bottlenecks, and reduce unproductive training jobs. | . | Volume Manager (Video tour) The Volume Manager enables you to manage your data and persistent volumes. For the volumes in your namespace, it streamlines the creation and deletion of volumes, which then can be easily attached to your notebooks. PR 5684 | . | Kale (Video tour) The updated Kale UI, a JupyterLab extension, simplifies your hyperparameter tuning trial set-up. The UI walks you through these steps: enter your hyperparameters as a list or a range, pick your search algorithm (Grid, Random, Bayesian) and the parameter to be optimized i.e. minimize loss. Then with a click of a button, your Katib trials are set-up, snapshotted, tracked, and run. | . | Kubeflow Pipelines (KFP) The KFP UI has been reorganized for a more unified experience (PR 4925), and includes the ability to manage recurring runs via new “JobsList” and “AllJobslist” pages (PR 5131) and simplified view of dependency graphs. | . | . Beyond the UIs, data scientists can also tie Notebooks with Serving more closely than ever before. In addition to the aforementioned integration with TensorBoard, Kubeflow Notebooks also now support first class deployments with TensorFlow 2.0, PyTorch, VS Code and RStudio. . KFServing enhancements include simplified canary rollouts with traffic splitting at the Knative revisions level. It also delivers extended ML framework support for: . TorchServe predict and PyTorch Captum Explain | PMMLServer, PR 1141 | LightGBM | . Infrastructure and operational efficiencies . ML engineers will like 1.3’s delivery of operational and infrastructure efficiencies, which are coupled with streamlined installation patterns and upgraded Istio version support. The following chart provides a summary of the top production features in 1.3. . Feature | Benefits | . Multi-model serving (Alpha) | More models on same infra and workaround cluster limits i.e. # of pods &amp; ip addresses | . Pod affinity | Avoid unnecessary usage on GPU or large CPU nodes | . gRPC support | Fewer messages, less bandwidth for KFServing workloads | . Katib trial templates | Simplifies hyperparameter tuning set-up for custom model types | . Katib early stopping | Stops hyperparameter tuning trials that are unproductive | . Pipelines step caching | Re-use results from previously run steps | . Multi-user pipelines | User and resource isolation for non-GCP environments. | . Manifests refactoring | Simplifies Kubeflow installation and upgrades | . Istio upgradability | Improved security, day 2 operations, compatibility and support | . We are pleased to announce that the user documentation on Kubeflow.org has also been updated (PR 2546). Additional detailed documentation, especially on the valuable working group deliveries, can be found here: . Kubeflow Pipeline 1.3 Project (PR 12) | Kubeflow Pipelines SDK with Tekton | Operationalize, scale and infuse trust in AI models using KFServing | Kubeflow Katib: Scalable, portable and cloud native system for AutoML | . Simplified installation and improved documentation . ML Engineers, who are installing Kubeflow, have a clear path to installation success as Kubeflow 1.3 includes new manifests and upgraded Istio support. For more information on installation patterns for each distribution, please visit the Getting Started page on Kubeflow.org. If you are supporting a distribution or just interested in low-level details, please review the Kubeflow 1.3 Manifest readme. . Kubeflow 1.3 tutorials . Kubeflow 1.3 new features are easy to try on these tutorials: . Open Vaccine Tutorial Use the new UIs to build an ML Pipeline, tune your model, and then deploy and monitor it. This tensorflow-based example was modified from a Kaggle tutorial for building a Covid 19 vaccine from bases in an mRNA molecule. The tutorial is easy to run on AWS and GCP in about 1 hour. | . | Model Risk Management Tutorial This model produces a SR11-7 compliance report for financial institutions who are regulated by the Federal Reserve. The example provides reporting on bias in a home mortgage lending model. The tutorial is easy to run on AWS and GCP in about 1 hour. | . | . Join the community . We would like to thank everyone for their efforts on Kubeflow 1.3, especially the code contributors and working group leads. As you can see from the extensive contributions to Kubeflow 1.3, the Kubeflow Community is vibrant and diverse, and solving real world problems for organizations around the world. . Want to help? The Kubeflow Community Working Groups hold open meetings, public lists, and are always looking for more volunteers and users to unlock the potential of machine learning. If you’re interested in becoming a Kubeflow contributor, please feel free to check out the resources below, we look forward to working with you! . Visit our Kubeflow website or Kubeflow GitHub Page | Join the Kubeflow Slack channel | Join the kubeflow-discuss mailing list | Attend a weekly community meeting | .",
            "url": "https://blog.kubeflow.org/kubeflow-1.3-release/",
            "relUrl": "/kubeflow-1.3-release/",
            "date": " • Apr 23, 2021"
        }
        
    
  
    
        ,"post24": {
            "title": "Kubeflow Continues to Move into Production",
            "content": "Kubeflow Users are maturing and the community is growing, forty eight percent of users are supporting deployments in production. . The Spring 2021 Kubeflow Community User Survey collected input from Kubeflow users on the benefits, gaps and requirements for machine learning use cases. It is the largest survey to date with 179 responses—a 50% increase from the Kubeflow 1.0 Community User Survey a year ago. The Survey respondents span a spectrum of skill sets. While 42% are machine learning (ML) engineers, and 24% are ML architects, the titles of the respondents vary from DevOps Engineers to data scientists and product managers. . . Kubeflow Continues to Move Into Production . Forty eight percent of users are supporting deployments that are in production, up from 15% last year. Further, one question that many folks have is “do people upgrade a production deployment or just install a new cluster and start over?” It appears that the latter is far more common: just 8% have upgraded their environment. . . Similar to previous years, Kubeflow Pipelines and Notebooks are the most popular components, but other components are now being widely deployed as well. Interest in TensorBoard has grown, joining KFServing, Katib (AutoML), and Distributed Training as top additional services. . . Although the usage patterns for Kubeflow components are mixed, the vast majority of users need at least two Kubeflow components in their ML Platform. . . TensorFlow is the leading ML Framework, followed by Scikit-learn, PyTorch, Keras, and XGBoost. However, with Kubeflow’s built-in extensibility, the type of ML tools people use in Kubeflow go beyond just training frameworks, and include MLFlow, Airflow, and Spark. . . VS Code and Jupyter Lead Dev Environments . From an Integrated Development Environment (IDE) perspective, most users are developing models in Jupyter Notebooks and Visual Studio Code, and about one third are using PyCharm. . . From a gap perspective, the users would like improved documentation, tutorials, and installation, along with more automation, support and security. . . Using Kubeflow Goes Beyond Just Training . Users identified that data preprocessing and transformation are both the most time consuming and challenging steps. We also received feedback that pipeline building and feature engineering are both time consuming and challenging. Distributed training, model serving and monitoring appear to be more technically challenging than time consuming. . . ML model delivery commonly requires multiple teams to work together i.e. data engineers, data scientists, ML engineers and devops engineers. ML workflows often include manual processes and there can be gaps in the handoffs between these groups. In particular, connecting data pipelines to ML pipelines is an example of a process that could be better automated, along with pipeline building and model monitoring. . . The vast majority of Kubeflow users are self-reliant in solving complex problems: . . And many are using tutorials created by Cloud Service Providers (i.e. Google, AWS and Azure) and MiniKF from Arrikto. . . The majority of ML models have a fairly short life: ~50% run in production for 3 months or less. On the other end of the spectrum, 25% of the models remain in production for 6 months or longer. . . And ~70% of all models take up to 15 iterations to produce a final model suitable for production. . . Users have a wide range of success with their models: ~43% are getting more than half of their models to deliver business value. On the other side, 39% are getting a very small percentage (10%) of their models into production and delivering business value. . . User Requests . We provided a section for free-form responses and we received a great deal of feedback. Here are some good examples of user requests: . Metadata storage and versioning | More robust access control and permission granularity for model/data sharing | More visibility on your roadmap | Installation patterns, stability, multi tenancy | More real life case studies | Updated and more in-depth documentation | . Key Takeaways . In addition to learning about how users are operating Kubeflow in production clusters, the Community Survey has given us important data that we can use to enhance our processes: . Improvements to the release management process, which is being driven by better inter-Working Group collaboration. This, coupled with core upgrades to Istio and a clean-up of the installation manifests, will improve the testing, documentation, and installation patterns. | Automation of pipeline building and feature engineering tasks, especially with continued integrations of Kubeflow with Kale and Feast, which are enabling new end-to-end workflows and tutorials. | Enhancements for data preprocessing and transformation as well as streamlined connections of Data Pipelines to ML Pipelines. Additionally, a renewed effort to develop a Spark operator. | Several new UIs are under development i.e. Katib, Model Management, Volumes Management and TensorBoard Management, which will help the user experience. | . For more details on the Community’s and specific Working Group’s deliveries, please review the Kubeflow 1.3 Release Blog post. . Join the Community . We would like to thank everyone for their participation in the survey. As you can see from the survey results, the Kubeflow Community is vibrant and diverse, solving real world problems for organizations around the world. . Want to help? The Kubeflow Community Working Groups hold open meetings, public lists, and are always looking for more volunteers and users to unlock the potential of machine learning. If you’re interested in becoming a Kubeflow contributor, please feel free to check out the resources below. We look forward to working with you! . Visit our Kubeflow website or Kubeflow GitHub Page | Join the Kubeflow Slack channel | Join the kubeflow-discuss mailing list | Attend a weekly community meeting | .",
            "url": "https://blog.kubeflow.org/kubeflow-continues-to-move-to-production",
            "relUrl": "/kubeflow-continues-to-move-to-production",
            "date": " • Mar 19, 2021"
        }
        
    
  
    
        ,"post25": {
            "title": "Elastic Training with MPI Operator and Practice",
            "content": "With increase in the size of dataset and deep learning models, distributed training emerges as the mainstream approach for training neural network models in industry. While it is feasible now to launch a massive distributed training job on Kubernetes with Kubeflow, advanced features like elastic workload and other cost mitigation approaches remain leashed when we talk about deep learning jobs on Kubernetes. . To address issues on cost and resource utilization, the TKE (Tencent Kubernetes Engine) AI team designs and implements Elastic Training in Kubeflow community. . Here we present how the elastic training is performed on Kubernetes. Validated with experiments under circumstances, elastic training lowers cost for distributed training on cloud. . Background . Let’s first recap training deep learning models. When we talk about ‘training’, it refers generally to iteratively optimizing parameters in a neural network model with its gradient descent. Accelerated with GPUs, the training can speed up for 10-100 times. . When manufacturers try to integrate more computational resources like GPUs into a single machine, to hold training experiments with more and more data or model parameters, the cost grows exponentially. Therefore, after initially proposed by Mu Li on OSDI’14, distributed training takes over training on a single machine when researchers play with massive dataset or large model. . For distributed training in data-parallelism, Horovod is widely adopted given its excellent support on deep learning frameworks like TensorFlow and PyTorch, communication optimization and easier programming pattern. In Horovod, all training processes are equal participants, each of which process the gradient calculation and communication. . . Because of significant acceleration of training speed as well as the programming pattern that are easier to understand, data-parallelism distributed training, represented by Horovod, is getting more and more attention. However, there still remain some issues: . The cost of training on the cloud is still the hurdle. While researchers no longer face the complexity when training on cloud thanks to Kubernetes and Kubeflow, the cost of training on cloud quells some users. | Compared with training on a single machine, multi-node distributed training accumulates the probability of training failure. The entire training experiment fails when any of its training process issues an error. This problem becomes even severer when some experiments take days or even weeks. | When collocating training tasks with other workloads (with higher priority), the resources demand fluctuates as the request for these other workloads may change periodically. This unbalance of resources availability throws cold water on the idea of using hybrid-deployment to maximize resource utilization. | . Elastic Training . Researchers and engineers proposed Elastic Training as the key to solve the puzzle. . Traditionally, the resource configuration for a distributed training job is fixed. Elastic training breaks this rule and enables users to change the number of instances participating in a distributed training job, bringing the following benefits to clusters with distributed training jobs: . Fault Tolerance: any worker instance can fail as long as at least one is surviving. | Resources Utilization: when the resources stress piles, the cluster is able to reduce the replicas of workloads with lower priority (distributed training workloads), releasing resource to other workloads (such as prediction service), ensuring SLA for business; after resources released from workloads, elastic training job is able to absorb these resource by scaling up workload replicas. | Training on Cloud: there is a type of resource on the cloud that is called “spot” or “preemptible” instances; it comes with unexpected low price tags but may be retrieved after guaranteed hour expires. | . Elastic training appears a perfect match to public cloud. Combined with spot instances, we cut the cost for GPUs from ¥16.21/hour to ¥1.62/hour, reducing the overall cost for the training job by nearly 70%. Under the same budget, elastic training employs more GPUs and accelerates the training speed by 5 to 10 times. . Elastic Horovod . As the major player in distributed training framework, Horovod v0.20.0 offers its solution to elastic training, Elastic Horovod. Here we quotes the architecture differences between Elastic Horovod and existing Horovod from RFC Elastic Horovod: . . All collective operations are coordinated within a hvd.elastic.run function. | State is synchronized between workers before the user’s training function is executed. | Worker failure or worker added events will result in a reset event on other workers. | Reset events act as barriers to: Determine whether the job should continue based on worker exit codes. | Blacklist failing hosts. | Launch workers on new hosts. | Update the rank information on existing workers. | . | State is synchronized following a reset event. | . When launching an elastic training job, horovodrun requires a discover_hosts.sh script to detect available hosts and slots in real time. In the following section, we refer this script as discover_hosts.sh. Nevertheless the script needs not to be named as discover_hosts.sh. An example of discover_hosts.sh can be found here. . Elastic Horovod on Kubernetes . MPI-Operator is designed to deploy Horovod jobs on Kubernetes. While the operator releases multiple versions, the general idea stays unchanged. It includes: . . MPIJob Controller creates a launcher pod and worker pods according to the replicas configuration in MPIJobs | For each MPIJob, the controller creates a ConfigMap, which delivers two files: hostfile and kubexec.sh | With all worker pods ready, mpirun on launcher pod (granted with pod/exec permission) uses kubexec.sh to launch processes on worker pods | . Launching an Elastic Horovod job is not feasible as there exist several incompatibilities between Elastic Horovod and MPIJob Controller. We take controller-v1 as the example: . No built-in discover_hosts.sh available on launcher pod | After worker replica number is turned down, worker pods that are no longer wanted will not be deleted by the controller, leaving the size of the distributed training unchanged | After worker replica number is turned up, the controller does not update rule in the Role binded to the launcher pod, preventing the launcher pod from creating processes on newly created pods | . To address these compatibility issues, we pushed multiple pull requests regarding Horovod and MPI-Operator, including mpi-operator-pull-335 and horovod-pull-2199. As providing an MPI-Operator-specific discover_hosts.sh is most critical to the launcher pod for Elastic Horovod, we consider two scenarios for converting worker pods with a Running phase into a discover_hosts.sh script. . A dynamic discover_hosts.sh composed by MPIJob controller and synchronized to the launcher pod via ConfigMap MPIJob controller has a podLister, which can be used to list worker pods readily | the controller filters worker pods with status.phase == Running and encode the result into the discover_hosts.sh | the ConfigMap is updated when discover_hosts.sh is modified and the change will be propagated to the launcher pod by Kubernetes | . | A static discover_hosts.sh using kubectl in the launcher pod to list all running worker pods from APIServer | . Scenario 2 changes the delivery image instead of the controller. However, as we cannot limit how frequently users will execute the discover_hosts.sh script, it poses a potential threat to the APIServer, especially when the count of worker pods is massive. . An fixture to scenario 2 is to replace the kubectl with a podLister process, removing extra stress from the APIServer. In this way, we install two processes in launcher pod but lack a proper mechanism to keep the podLister alive. Once the podLister dies, there leaves no elasticity for the training job. . Therefore we choose the first scenario and map the disocver_hosts.sh under /etc/mpi/. We also fixed the other compatibility issues after the worker replica configuration changes. For users choose non-elastic mode, just simply ignore /etc/mpi/discover_hosts.sh. . Concerns comes to scenario 1 as well. There is a delay between the ConfigMap and what horovodrun sees from the discover_hosts.sh in the launcher pod. This delay, on one hand, can be tweaked by cluster admin and on the other hand, can be considered as tiny compared to the training elapsed time or the time for Elastic Horovod to handle worker changes. . Demo . We present a demo to show how to operate an Elastic Horovod job with MPI Operator. . bash-5.0$ kubectl create -f ./tensorflow-mnist-elastic.yaml mpijob.kubeflow.org/tensorflow-mnist-elastic createdbash-5.0$ kubectl get po NAME READY STATUS RESTARTS AGE tensorflow-mnist-elastic-launcher 1/1 Running 0 14s tensorflow-mnist-elastic-worker-0 1/1 Running 0 14s tensorflow-mnist-elastic-worker-1 1/1 Running 0 14s . The job is created with two workers. After the training begins, we change MPIJob.Spec.MPIReplicaSpecs[&quot;Worker&quot;].Replicas to 3, adding another worker. Let’s check how the discover_hosts.sh changes: . bash-5.0$ kubectl exec tensorflow-mnist-elastic-launcher -- /etc/mpi/discover_hosts.sh tensorflow-mnist-elastic-worker-0:1 tensorflow-mnist-elastic-worker-1:1 bash-5.0$ cat ./patch_r3.yaml spec: mpiReplicaSpecs: &quot;Worker&quot;: replicas: 3 bash-5.0$ kubectl patch mpijob tensorflow-mnist-elastic --patch &quot;$(cat patch_r3.yaml)&quot; --type=merge mpijob.kubeflow.org/tensorflow-mnist-elastic patched bash-5.0$ kubectl exec tensorflow-mnist-elastic-launcher -- /etc/mpi/discover_hosts.sh tensorflow-mnist-elastic-worker-0:1 tensorflow-mnist-elastic-worker-1:1 tensorflow-mnist-elastic-worker-2:1 . We reduce the replica count to 1, retrieving 2 worker instances. . bash-5.0$ cat ./patch_r1.yaml spec: mpiReplicaSpecs: &quot;Worker&quot;: replicas: 1 bash-5.0$ kubectl patch mpijob tensorflow-mnist-elastic --patch &quot;$(cat patch_r1.yaml)&quot; --type=merge mpijob.kubeflow.org/tensorflow-mnist-elastic patched bash-5.0$ kubectl get po NAME READY STATUS RESTARTS AGE tensorflow-mnist-elastic-launcher 1/1 Running 0 4m48s tensorflow-mnist-elastic-worker-0 1/1 Running 0 4m48s tensorflow-mnist-elastic-worker-1 1/1 Terminating 0 4m48s tensorflow-mnist-elastic-worker-2 1/1 Terminating 0 2m21s . The elastic training persists. . Thu Mar 11 01:53:18 2021[1]&lt;stdout&gt;:Step #40 Loss: 0.284265 Thu Mar 11 01:53:18 2021[0]&lt;stdout&gt;:Step #40 Loss: 0.259497 Thu Mar 11 01:53:18 2021[2]&lt;stdout&gt;:Step #40 Loss: 0.229993 Thu Mar 11 01:54:27 2021[2]&lt;stderr&gt;:command terminated with exit code 137 Process 2 exit with status code 137. Thu Mar 11 01:54:27 2021[0]&lt;stderr&gt;:command terminated with exit code 137 Process 0 exit with status code 137. Thu Mar 11 01:54:57 2021[1]&lt;stderr&gt;:[2021-03-11 01:54:57.532928: E /tmp/pip-install-2jy0u7mn/horovod/horovod/common/operations.cc:525] Horovod background loop uncaught exception: [/tmp/pip-install-2jy0u7mn/horovod/third_party/compatible_gloo/gloo/transport/tcp/pair.cc:575] Connection closed by peer [10.244.2.27]:54432 WARNING:root:blacklist failing host: tensorflow-mnist-elastic-worker-2 WARNING:root:blacklist failing host: tensorflow-mnist-elastic-worker-1 Thu Mar 11 01:54:58 2021[1]&lt;stdout&gt;:Step #50 Loss: 0.207741 Thu Mar 11 01:55:00 2021[1]&lt;stdout&gt;:Step #60 Loss: 0.119361 Thu Mar 11 01:55:02 2021[1]&lt;stdout&gt;:Step #70 Loss: 0.131966 . As we can see, Elastic Horovod on MPI-Operator now supports tweaking worker replicas dynamically. As a future work, we aim to support Horizontal Pod Autoscaler to MPIJob as well as other features like designated worker deletion. . Conclusion . When the concept of cloud native and distributed training fuse to elastic training on Kubernetes, it lowers the cost and gives robustness and flexibility. As a team, we are working with PyTorch, Horovod and other communities to propel elastic training. We wish to further introduce our work on elasticity with PS/Worker training mode, optimization for resource and job priority and other topics on cloud native AI. .",
            "url": "https://blog.kubeflow.org/elastic%20training/operators/2021/03/15/elastic-training.html",
            "relUrl": "/elastic%20training/operators/2021/03/15/elastic-training.html",
            "date": " • Mar 15, 2021"
        }
        
    
  
    
        ,"post26": {
            "title": "Kubeflow Katib: Scalable, Portable and Cloud Native System for AutoML",
            "content": "As machine learning (ML) architectures are increasing in complexity, it is becoming important to find the optimal hyperparameters and architecture for ML models. Automated machine learning (AutoML) has become a crucial step in the ML lifecycle. Katib provides AutoML features in Kubeflow in a Kubernetes native way. . Katib is an open source project which is agnostic to ML frameworks. It can tune hyperparameters in applications written in any language of the user’s choice and natively supports many ML frameworks, such as TensorFlow, Keras, PyTorch, MPI, MXNet, XGBoost, scikit-learn, and others. Katib improves business results by efficiently building more accurate models and lowering operational and infrastructure costs. Katib can be deployed on local machines, or hosted as a service in on-premise data centers, or in private/public clouds. . Katib offers a rich set of features accessible via APIs. By using these APIs, Katib is natively integrated to Kubeflow Notebooks and Pipelines. Katib supports Hyperparameter optimization (HP), Neural Architecture Search (NAS), and Early Stopping. Early Stopping feature can be used without any significant changes in the current Katib Experiments. . Furthermore, Katib is a unique system which supports all Kubernetes workloads and Kubernetes custom resource definition (CRD) to perform Katib Experiments. Since Katib can execute various Kubernetes resources, users are able to run not only ML models optimization Experiments. They can also enhance any software, code or program to make it more efficient with optimization algorithms provided by Katib. . We are continually working on the new Katib UI to provide a better User Experience and native integration with Kubeflow central dashboard. Please check this presentation to know more about the new UI. . All of the above mentioned features allow users to easily integrate Katib in their ML infrastructure pipeline. . System Architecture . . There are 3 main concepts in Katib which are Kubernetes CRDs: . Experiment - a single optimization run with objective, search space, and search algorithm. . | Suggestion - set of hyperparameters, which are produced by a user’s selected search algorithm. Katib creates Trials to evaluate them. . | Trial - one iteration of the hyperparameters tuning process. Trial runs the worker job which corresponds to the training job. Since Trial is an abstraction on top of the worker job, any Kubernetes resource can be used to perform the training job. For example, TFJob, MPIJob or even Tekton Pipeline. . | . By using above resources Katib follows the following steps, which are marked in the diagram above: . Once an Experiment is submitted, the Experiment controller creates an appropriate Suggestion object. . | The Suggestion controller creates an AutoML algorithm service based on this Suggestion object. When the algorithm service is ready, the Suggestion controller calls the service to get new parameters and appends them to the Suggestion object. . | The Experiment controller finds that Suggestion object has been updated and creates a corresponding Trial object for each set of parameters. . | The Trial controller generates a worker job for each Trial object and watches for the status of each job. The worker job based on the Trial template. . | Once the worker job has been completed, the metrics collector gets the metrics from the job and persists them in the database. . | The Experiment controller sends the metrics results to the algorithm service and gets new parameters from the Suggestion object. . | Custom Kubernetes resources support . Katib version 0.10 implements a new feature to support any Kubernetes CRDs or Kubernetes workloads as a Katib Trial template. Therefore, there is no need to manually modify the Katib controller to use CRD as a Trial. As long as the CRD creates Kubernetes Pods, allows injecting the sidecar container on these Pods, and has success and failure status, the CRD can be used in Katib. . Here are the motivations behind this feature: . Katib Trial template supports only a limited type of Kubernetes resource (BatchJob, TFJob and PyTorchJob). . | Many Katib users might have their own CRDs which they want to use as a Trial template. Thus, the approach of updating the Katib controller for the new CRD is not scalable. . | Some CRDs might have Go packages versions which are incompatible with the Katib controller packages. For such cases, it is impossible to build a Katib controller image. . | Users have to build and maintain a custom image version for the Katib controller if they want to implement a new CRD in Katib. . | . The above problems led to the creation of a scalable and portable solution for the Trial template. This solution allows users to modify Katib components and to add their CRDs without changing the Katib controller image. . Katib now supports Tekton Pipeline and MPIJob in addition to BatchJob, TFJob and PyTorchJob. In the case of Tekton Pipeline, a user is able to build a complex workflow inside the Trial’s worker job. The user also can implement data preprocessing and postprocessing with all of the Tekton Pipeline features. Eventually, Katib’s metrics collector parses and saves the appropriate metrics from the training processes to the database. . Support new Kubernetes CRD in Katib . To support new Kubernetes CRD, Katib components need to be modified before installing in the Kubernetes cluster. To make this modification, it is necessary to know: . what API group, version, and kind the Kubernetes CRD has, and . | which Kubernetes resources the CRD’s controller creates. . | . Check the Kubernetes guide to know more about CRDs. . Follow these two simple steps to integrate new CRD in Katib: . Modify the Katib controller Deployment’s arguments with the new flag: . --trial-resources=&lt;object-kind&gt;.&lt;object-API-version&gt;.&lt;object-API-group&gt; . For example, to support Tekton Pipeline: . . . . containers: - name: katib-controller image: docker.io/kubeflowkatib/katib-controller command: [&quot;./katib-controller&quot;] args: - &quot;--webhook-port=8443&quot; - &quot;--trial-resources=Job.v1.batch&quot; - &quot;--trial-resources=TFJob.v1.kubeflow.org&quot; - &quot;--trial-resources=PyTorchJob.v1.kubeflow.org&quot; - &quot;--trial-resources=MPIJob.v1.kubeflow.org&quot; - &quot;--trial-resources=PipelineRun.v1beta1.tekton.dev&quot; . . . . | Modify the Katib controller ClusterRole’s rules with the new rule to give Katib an access to all Kubernetes resources that are created by the CRD’s controller. To know more about ClusterRole, please check the Kubernetes guide. . For example, for the Tekton Pipeline, Trial creates Tekton PipelineRun, then Tekton PipelineRun creates Tekton TaskRun. Therefore, Katib controller ClusterRole should have an access to the pipelineruns and taskruns: . kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: katib-controller rules: - apiGroups: - tekton.dev resources: - pipelineruns - taskruns verbs: - &quot;*&quot; - apiGroups: - kubeflow.org resources: - tfjobs - pytorchjobs - mpijobs verbs: - &quot;*&quot; . . . . | Install Katib by following the getting started guide. . | At this point, the Kubernetes CRD can be used in the Katib Trial template. Check this guide to know more about Tekton and Katib integration. . Early Stopping . Early Stopping is now supported in the Katib 0.10 release. Early Stopping is one of the essential steps for doing HP tuning. It helps to avoid overfitting when the model is training during Katib Experiments. . Using Early Stopping helps to save compute resources and to reduce the Experiment execution time by stopping the Experiment’s Trials when the target metric(s) no longer improves before the training process is complete. . The major advantage of using Early Stopping in Katib is that the training container package doesn’t need to be modified. Basically, the Experiment’s YAML has to be extended with the new entity - earlyStopping, which is similar to the algorithm YAML section: . apiVersion: &quot;kubeflow.org/v1beta1&quot; kind: Experiment metadata: namespace: kubeflow name: median-stop spec: algorithm: algorithmName: random earlyStopping: algorithmName: medianstop algorithmSettings: - name: min_trials_required value: &quot;3&quot; - name: start_step value: &quot;5&quot; objective: type: maximize goal: 0.99 objectiveMetricName: Validation-accuracy additionalMetricNames: - Train-accuracy parallelTrialCount: 2 maxTrialCount: 15 maxFailedTrialCount: 3 . . . . Currently, Katib supports the Median Stopping Rule. The Medium Stopping rule stops a running Trial at the step S if the Trial’s best objective value is lower than the median value of all succeeded Trials’ objectives reported up to that step S. Readers interested in learning more about the Median Stopping Rule can check the Google Vizier: A Service for Black-Box Optimization paper. . To know more about using Early Stopping in Katib please follow the official guide. . Getting Involved . First of all, thanks a lot to our contributors (Alfred Xu (Nvidia), Andrey Velichkevich (Cisco), Anton Kirillov (Mesosphere), Ce Gao (Tencent Cloud), Chenjun Zou (Alibaba), Elias Koromilas (InAccel), Hong Xu (IBM), Johnu George (Cisco), Masashi Shibata, Vaclav Pavlin (Red Hat), Yao Xiao (AWS), Yuan Tang (Ant Group)) who helped with the 0.10 release. Our community is growing and we are inviting new users and AutoML enthusiasts to contribute to the Katib project. The following links provide information about getting involved in the community: . Subscribe to the calendar to attend the AutoML WG community meeting. . | Check the AutoML WG meeting notes. . | Join the AutoML WG Slack channel. . | Check the Katib adopters list. . | Learn more about Katib in the presentations and demos list. . | . Please let us know about the active use-cases, feature requests and questions in the AutoML Slack channel or submit a new GitHub issue. To know more about the new Katib UI or to track the current integration process please check the GitHub project. We are planning to arrange a webinar and tutorial session for using AutoML in Kubeflow soon. Please join the kubeflow-discuss mailing list to know more about it. . Special Thanks to Amit Saha (Cisco), Ce Gao (Tencent Cloud), Johnu George (Cisco), Jorge Castro (Arrikto), Josh Bottum (Arrikto) for their help on this blog. .",
            "url": "https://blog.kubeflow.org/katib/",
            "relUrl": "/katib/",
            "date": " • Mar 10, 2021"
        }
        
    
  
    
        ,"post27": {
            "title": "Operationalize, Scale and Infuse Trust in AI Models using KFServing",
            "content": "By Animesh Singh and Dan Sun . With inputs from : KFServing WG, including Yuzhui Liu, Tommy Li, Paul Vaneck, Andrew Butler, Srinivasan Parthasarathy etc. . Machine Learning has become a key technology in a wide range of industries and organizations. One key aspect in ML landscape is that more and more models are getting produced, but are they actually getting deployed? And if they are getting deployed, are there enough robust operational mechanisms in place to understand model predictions, and monitor for drift, accuracy, anamoly, bias etc.? One key aspect of deploying models in production is being able to monitor the predictions for various metrics, and explaining the decisions the model is making, and producing quality metrics, more so in regulated industries like finance, healthcare, government sector etc. Additionally based on those metrics do we have a technology in place to understand the metrics and take corrective actions e.g. doing canary rollouts? . KFServing, a project which originated in the Kubeflow community, has been hard at work solving production model serving use cases by providing performant, high abstraction interfaces for common ML frameworks like Tensorflow, XGBoost, ScikitLearn, PyTorch, and ONNX. It encapsulates the complexity of autoscaling, networking, health checking, and server configuration to bring cutting edge serving features like GPU Autoscaling, Scale to Zero, and Canary Rollouts to model deployments. We just released KFServing v0.5 with other various features to address the model operationalization and trust needs. Additionally, the team has been hard at work to make AI explainability a core construct of the deployed models, by integrating with various industry leading technologies. . KFServing Beta API and V2 (next gen) Inference Protocol . KFServing 0.5 has promoted the control plane API from v1alpha2 to stable v1beta1 and started to support the data plane V2 inference protocol. The v1beta1 control plane API enables a simple, data scientist-friendly interface, while providing the flexibility of specifying container and pod template fields for pre-packaged model servers. The V2 inference protocol pushes a standard and easy-to-use high performance REST/gRPC API across multiple model servers, such as Triton and MLServer, to increase the portability of the model ensuring the client/server can operate seamlessly. . KFServing 0.5 also introduces an optional model agent for request/response logging, request batching, and model pulling. The model agent sits alongside as a sidecar to the model server. Pre-packaged Model servers plugged onto KFServing can benefit from these common model serving features, as well as the model servers built using custom frameworks. . . What’s New? . TorchServe integration: TorchServe now is used as implementation for KFServing PyTorch model server, it also enables model explanability with Captum, see TorchServe examples here. . | Triton Inference Server V2 inference REST/gRPC protocol support, see examples of serving BERT and TorchScript models on GPUs. . | Tensorflow gRPC support. . | SKLearn/XGBoost model server now uses MLServer which supports v2 inference protocol. . | New model servers added for pmml and lightgbm. . | You can now specify container or pod template level fields on the pre-packaged model servers (e.g., env variables, readiness/liveness probes). . | Allow specifying timeouts on the component spec. . | Simplified canary rollout, you no longer need to specify both the default and canary specs on the InferenceService spec; KFServing now automatically tracks the last rolled out revision and automatically splits the traffic between the latest ready revision and last rolled out revision. . | The transformer to predictor call now defaults to using AsyncIO, which significantly improves the latency/throughput for high concurrent workload use cases. . | . KFServing Multi-Model Serving to enable massive scalability . With machine learning approaches becoming more widely adopted in organizations, there is a trend to deploy a large number of models. The original design of KFServing deploys one model per InferenceService. But when dealing with a large number of models, its ‘one model, one server’ paradigm presents challenges on a Kubernetes cluster to deploy hundreds of thousands of models. To scale the number of models, we have to scale the number of InferenceServices, something that can quickly challenge the cluster’s limits. . Multi-model serving is an alpha feature added in 0.5 to increase KFServing’s scalability. To learn more about multi-model serving motivations and implementation deatils, dive into the details in KFServing github. Please assume that the interface is subject to change. The experimental feature must be enabled from the inference service configmap. . . Multi-model serving will work with any model server that implements KFServing’s V2 protocol. More specifically, if the model server implements the load and unload endpoint, then it can use KFServing’s TrainedModel. Currently, the supported model servers are Triton, SKLearn, and XGBoost. Click on Triton or SKLearn for examples on how to run Multi-Model Serving. . KFServing on OpenShift . RedHat OpenShift is a market leader for enterprise Kubernetes distribution, and by enabling KFServing for OpenShift we have ensured that enterprises running battle hardened OpenShift platform can leverage KFServing to bring serverless model inferencing on OpenShift, including how to leverage OpenShift Service Mesh. Please follow the details here to get KFServing running on OpenShift . . LFAI Trusted AI Projects on AI Fairness, AI Explainability and Adversarial Robustness in KFServing . Trust and responsibility should be core principles of AI. The LF AI &amp; Data Trusted AI Committee is a global group working on policies, guidelines, tools and projects to ensure the development of trustworthy AI solutions, and we have integrated LFAI AI Explainability 360, Adversarial Robustness 360 in KFServing to provide production level trusted AI capabilities. Please find more details on these integration in the following links . AI Explainability 360-KFServing Integration . AI Fairness 360-KFServing Integration . Adversarial Robustness Toolbox-KFServing Integration . . Metrics driven automated rollouts using Iter8 in KFServing . Iter8-KFServing enables metrics-driven experiments, progressive delivery, and automated rollouts for ML models served over Kubernetes and OpenShift clusters. Iter8 experiments can be used to safely expose competing versions of a model to traffic while gathering and assessing metrics to intelligently shift traffic to the winning version of your model. Discover how to set it up and get it running in the Iter8-KFServing repository . . Join us to build Trusted Model Inferencing Platform on Kubernetes . Please join us on the KFServing GitHub repository, try it out, give feedback, and raise issues. Additionally, you can connect with us via the following: . To contribute and build an enterprise-grade, end-to-end machine learning platform on OpenShift and Kubernetes, please join the Kubeflow community and reach out with any questions, comments, and feedback! . | If you want help deploying and managing Kubeflow on your on-premises Kubernetes platform, OpenShift, or on IBM Cloud, please connect with us. . | Check out the OpenDataHub if you are interested in open source projects in the Data and AI portfolio, namely Kubeflow, Kafka, Hive, Hue, and Spark, and how to bring them together in a cloud-native way. . | . Contributor Acknowledgement . We’d like to thank all the KFServing contributors for the awesome work! . Animesh Singh . | Jinchi He  . | Clive Cox . | Ellis Tarn . | Pu Gang . | Qianshan Chen . | Yuzhui Liu . | Peter Salanki . | Jagadeesh . | David Goodwin . | Ilan Filonenko . | Hanbae Seo . | Ryan Dawson . | Paul Van Eck . | Weng Yao . | Theofilos Papapanagiotou . | Tom Duffy . | Andrew Butler . | Adrian Gonzalez-Martin . | Nick Hill . | Yao Xiao . | AnyISalIn . | Aaron Choo . | Michas Szacillo . | Dan Sun . | Geeta Chauhan . | Tommy Li . | .",
            "url": "https://blog.kubeflow.org/release/official/2021/03/08/kfserving-0.5.html",
            "relUrl": "/release/official/2021/03/08/kfserving-0.5.html",
            "date": " • Mar 8, 2021"
        }
        
    
  
    
        ,"post28": {
            "title": "Kubeflow 1.2 release announcement",
            "content": "Special Message from Kubeflow Founders . Three years (!!) ago, we (Jeremy Lewi, Vish Kannan and David Aronchick) stood on stage at Kubecon to introduce Kubeflow for the first time. We could not have possibly imagined what would have come about - thousands of GitHub stars, tens of thousands of commits and a community that has built the most flexible and scalable platform for machine learning. And, best of all, it’s not backed by an enormous company that requires you to “upgrade” in order to use it; we gave it all away for free! Here’s to everything you all have done and we could not be more excited about the NEXT three years (and the three years beyond that). Thank you! . Announcing Kubeflow v1.2 release . The Kubeflow Community’s delivery of the Kubeflow 1.2 software release includes ~100 user requested enhancements to improve model building, training, tuning, ML pipelining and serving. This post includes a Release Highlights Section, which details significant 1.2 features as contributed by the Kubeflow application working groups (WG), SIGs, and ecosystem partners. The Kubeflow 1.2 changelog provides a quick view of the 1.2 deliveries. . The Release was validated, tested and documented by the developers, and the Release is now being validated, tested and documented by users, cloud providers and commercial support partners on popular platforms i.e. AWS, Azure, GCP, IBM, etc. The Community is working on a more sustainable approach to owning and maintaining test infrastructure. . For Release 1.2, AWS has built and contributed a shared test-infra, which provides WG owners with enough permissions to identify problems, and test proposed solutions to completion. Currently, most WGs (AutoML, Training-Operators, KFServing, Deployments, Manifests) have already migrated their tests on this solution. As a result, the test-infra blocking time has fallen significantly, which is good for users and contributors. . Getting Involved . The Community continues to grow and we invite new users and contributors to join the Working Groups and Community Meetings. The following provides some helpful links to those looking to get involved with the Kubeflow Community: . Join the Kubeflow Slack channel | Join the kubeflow-discuss mailing list | Attend a Weekly Community Meeting | Review the Working Group Meeting Notes in Release Highlights Section (as the Notes include great discussions and meeting times) | . If you have questions and/or run into issues, please leverage the Kubeflow Slack channel and/or submit bugs via Kubeflow on GitHub. . What’s next . The Community has started discussions on Kubeflow 1.3. Arrikto has agreed to lead the 1.3 Release Management process and the Community will continue to capture input from users and contributors as features are defined, developed and delivered. Onward and upward! . Special thanks to Constantinos Venetsanopoulos (Arrikto), Animesh Singh (IBM), Jiaxin Shan (ByteDance), Yao Xiao (AWS), David Aronchick (Azure), Dan Sun (Bloomberg), Andrey Velichkevich (Cisco), Matthew Wicks (Eliiza), Willem Pienaar (Feast), Yuan Gong (Google), James Wu (Google), Jeremy Lewi (Google), Josh Bottum (Arrikto), Chris Pavlou (Arrikto), Kimonas Sotirchos (Arrikto), Rui Vasconcelos (Canonical), Jeff Fogarty (US Bank) , Karl Shriek (AlexanderThamm), and Clive Cox (Seldon) for their help on 1.2 and this post. . Release Highlights Section . Working Group: AutoML / Katib . Working Group Meeting Notes: Katib Working Group Meeting Notes . Overall benefit: Better model accuracy, Better infrastructure utilization . Overall description: Katib 0.10 with the new v1beta1 API has been released in Kubeflow 1.2. Automated configuration of Hyperparameters to deliver more accuracy models that use less infrastructure, AutoML / Katib simplified the process of finding the optimized set of parameters for your model with Early Stopping techniques. Possibility to orchestrate complex pipeline during Katib Experiment with custom Kubernetes CRD support. . Feature Name: Early Stopping . Feature Description: Save your cluster resources by using Katib Early Stopping techniques. Allow to use the Median Stopping Rule algorithm. . Feature Benefit: You don’t need to modify your training source code to use the feature! Early Stopping can be used with every Katib algorithm. . Feature Name: Support custom CRD in the new Trial template. . Feature Description: You are able to follow two simple steps to integrate your custom Kubernetes resource in Katib. Flexible way to send your hyperparameters in the new Trial template design, which is a valid YAML. . Feature Benefit: Define Tekton Pipeline in your Katib experiment. You are able to pass hyperparameters even if your model config is a JSON scikit learn Pipeline. . Feature Name: Resume Experiments . Feature Description: Implementation of the various methods to resume Katib Experiments. Save the Experiment’s Suggestion data in the custom volume. Use Katib config to modify your volume settings. . Feature Benefit: Free your cluster resources after your Experiment is finished. . Feature Name: Multiple Ways to Extract Metrics . Feature Description: You can specify metrics strategies for your Experiment. Katib computes the Experiment objective based on these values. You are able to view detailed metric info for each Trial. . Feature Benefit: Get correct optimisation results when your model produces necessary value at the final training step. . Working Group: KFServing . Working Group Meeting Notes: KFServing Working Group Meeting Notes . Overall benefit: KFServing enables serverless inferencing on Kubernetes and provides performant, high abstraction interfaces for common machine learning (ML) frameworks like TensorFlow, XGBoost, scikit-learn, PyTorch, and ONNX to solve production model serving use cases. . Overall description: Kubeflow 1.2 includes KFServing v0.4.1, where the focus has been on enabling KFServing on OpenShift and additionally providing more features, such as adding batcher module as sidecar, Triton inference server renaming and integrations, upgrading Alibi explainer to 0.4.0, updating logger to CloudEvents V1 protocol and allowing customized URL paths on data plane. Additionally, the minimum Istio is now v1.3.1, and KNative version has been moved to KNative 0.14.3. More details can be found here and here . Feature Name: Add batcher module as sidecar #847 @zhangrongguo . Feature Description: KFServer Batcher accepts user requests, batch them and then send to the “InferenceService”. Batcher Feature Description . Feature Benefit: Faster response time to inference requests, and Improve infrastructure utilization . Feature Name: Alibi explainer upgrade to 0.4.0 #803 @cliveseldon . Feature Description: The enhancements include a KernelSHAP explainer for black-box model SHAP scores and documentation for the LinearityMeasure algorithm. This delivery includes a new API for explainer and explanation objects, which provide a variety of improvements, but are breaking changes. . Feature Benefit: This delivery improves the ability to understand which features impact model accuracy along with improving operations. . Feature Name/Description : Triton inference server rename and integrations #747 @deadeyegoodwin . Working Group: Pipelines . Working Group Meeting Notes: http://bit.ly/kfp-meeting-notes . Overall benefit: Simplify process of creating a model when you have new data and new code . Overall description: Kubeflow Pipelines is a platform for building and deploying portable, scalable machine learning (ML) workflows based on containers. The Kubeflow Pipelines platform consists of: . A user interface (UI) for managing and tracking experiments, jobs, and runs. | An engine for scheduling multi-step ML workflows. | An SDK for defining and manipulating pipelines and components. | Notebooks for interacting with the system using the SDK. | . The following are the goals of Kubeflow Pipelines: . End-to-end orchestration: enabling and simplifying the orchestration of machine learning pipelines. | Easy experimentation: making it easy for you to try numerous ideas and techniques and manage your various trials/experiments. | Easy re-use: enabling you to re-use components and pipelines to quickly create end-to-end solutions without having to rebuild each time | . Kubeflow Pipelines is stabilizing over a few patch releases. At the same time, we made a lot of progress at standardizing the pipeline IR (intermediate representation) which will serve as a unified pipeline definition for different execution engines. . Feature Name: Kubeflow Pipelines with Tekton backend available . Feature Description: After an extensive effort, we have Kubeflow Pipelines running on Tekton end-to-end and available in open source. Additionally it’s available as default with Kubeflow deployment on IBM Cloud, and can be deployed on OpenShift. . Feature Benefit: Tekton support . If you are an existing user of Tekton, or are a fan of Tekton, or running OpenShift Pipelines, get Kubeflow Pipelines running on top of it. More details here https://developer.ibm.com/blogs/kubeflow-pipelines-with-tekton-and-watson/ . . Feature Name: stabilizing Kubeflow Pipelines 1.0.x . Feature Description: We are stabilizing Kubeflow Pipelines over a few patch releases: Kubeflow Pipelines 1.0.4 Changelog ~20 fixes and ~5 minor features. . Working Group: Notebooks . Working Group Meeting Notes: coming soon . Overall benefit: Interactive, experimental coding environment for model development . Overall description: Notebooks provide an advanced, interactive coding environment that users and teams can share and leverage kubernetes namespaces for isolation and resource utilization . Feature Name: Affinity/Toleration configs, #5237 . Feature Description: Adds the ability for Kubeflow administrators to set groups of Affinity/Toleration configs which users can pick from a dropdown. . Feature Benefit: Allows more fine-grained selection of how Notebook pods are scheduled. . Feature Name: Refactor Notebooks Web App . Feature Description: The details of the refactoring are defined in these deliveries: . Common code between the different python backends, #5164 | Create an Angular Library with common frontend code, #5252 | Refactor the JWA backend to utilize common code, #5316 | Initialize the Jupyter web app frontend in crud-web-apps, #5332 | . Feature Benefit : Refactoring will enable an easier future integration with other web apps - Tensorboard, volume manager. . Feature Name: Stop and Restart Notebooks while maintaining state, #4857 #5332 . NOTE: The artifacts for the updated Notebooks web app will be available in 1.2.1 or later . Feature Description: Implementation of a “shut down server” button in the central dashboard that scales the stateful set for the server down to zero and a “start server” button that scales it back up again. . Feature Benefit: Save work, save infrastructure resources . Working Group: Training-Operators . Working Group Meeting Notes: coming soon . Overall benefit: Faster model development using operators that simplify distributed computing . Feature Name: The Training Operator contributors provided the following fixes and improvements in Kubeflow 1.2: . Update mxnet-operator manifest to v1 (#1326, @Jeffwan) | Correct XGBoostJob CRD group name and add singular name (#1313, @terrytangyuan) | Fix XGBoost Operator manifest issue (#1463, @Jeffwan) | Move Pytorch operator e2e tests to AWS Prow (#305, @Jeffwan) | Support BytePS in MXNet Operator (#82, @jasonliu747) | Fix error when conditions is empty in tf-operator (#1185, @Corea) | Fix successPolicy logic in MXNet Operator (#85, @jasonliu747) | . SIG: Model Management . Overall benefit: The ability to find model versions and their subcomponents including metadata . SIG Meeting Notes: Model Management SIG Meeting Notes . Overall description: The SIG was initiated to define and develop a Kubeflow solution for model management, which will make it easier to organize and find models and their artifacts. In addition, several contributors are submitting proposals on how to define data types for ML model and data, with the goal of driving wider metadata standards, and interoperability of models between ML platforms, clouds, and frameworks. The proposals are working to define an ontology for model and data types and tooling to search and organize that metadata. . Proposals from Kubeflow Pipelines contributors, the Model Management SIG, Seldon and a MLSpec from David Aronchick (Azure) are under discussion. Please find links to those proposals below: . ML Data in Kubeflow Pipelines | ML Spec from David Aronchick | Model Management Proposal from Karl Schriek, SIG Tech Lead | Seldon’s Proposal for Initial Metadata Types | . EcoSystem: Seldon . Overall benefit: Deploy, Scale, Update models built with Kubeflow. . Overall description: Seldon handles scaling of production machine learning models and provides advanced machine learning capabilities out of the box including Advanced Metrics, Request Logging, Explainers, Outlier Detectors, A/B Tests, and Canaries. . Kubeflow 1.2 comes with Seldon’s 1.4 release. This release of Seldon adds further capabilities for model deployment and inferencing including the addition of batch and streaming interfaces to a deployed model. It also allows for fine grained control of how a deployed model interfaces with Kubernetes with the addition of KEDA and Pod Disruption Budget options. Finally, it begins a process of compatibility with KFServing by allowing the usage of the V2 Dataplane supported by Seldon, KFServing and NVIDIA Triton. . Version: 1.4.0 . Feature Name: Stream and Batch support . Feature Description: Streaming support for native Kafka integration. Batch prediction support from and to cloud storage. . Feature Benefit: Allows Seldon users to interact with their models via RPC, Streaming or Batch as needed. . Feature Name: Extended kubernetes control via KEDA and PDBs . Feature Description: Allows fine grained control of deployed models via autoscaling with KEDA metrics and addition of pod disruption budgets. . Feature Benefit: Manage models at scale in a production cluster. . Feature Name: Alpha V2 Dataplane . Feature Description: Run custom python models using an updated python server along with support for the V2 Dataplane (NVIDIA Triton, KFServing, Seldon) . Feature Benefit: Utilize a standard powerful protocol that is supported cross project. . EcoSystem: Kale . Overall benefit: Kubeflow Workflow tool that simplifies ML pipeline building and versioning directly from a Notebook or IDE i.e. VSCode . Kale GitHub repo: https://github.com/kubeflow-kale/kale . Kale Tutorials: https://www.arrikto.com/tutorials/ . Overall description: Kale lets you deploy Jupyter Notebooks that run on your laptop or on the cloud to Kubeflow Pipelines, without requiring any of the Kubeflow SDK boilerplate. You can define pipelines just by annotating Notebook’s code cells and clicking a deployment button in the Jupyter UI. Kale will take care of converting the Notebook to a valid Kubeflow Pipelines deployment, taking care of resolving data dependencies and managing the pipeline’s lifecycle . Feature Name: Dog Breed Classification example . Feature Description: Tutorial for simplified pipeline to build a model for Image Classification . Feature Benefit: Faster understanding of ML workflows to deliver models with hyperparameter tuning . Feature Name: Katib integration with Kale . Feature Description: Automated hyperparameter tuning and reproducible katib trials using pipelines . Feature Benefit: Better model accuracy and easy reproducibility and debugging . Feature Name: Pipeline Step Caching for Katib Trials using Kale’s integration with Rok . Feature Description: Kale recognizes when a pipeline step has been run before and fetches complete results from Rok and inserts into pipeline processing . Feature Benefit: Faster hyperparameter tuning, reduced infrastructure utilization . EcoSystem: Feast . Overall benefit: Feast allows teams to register, ingest, serve, and monitor machine learning features in production. . Working Group Meeting Notes: https://tinyurl.com/kf-feast-sig . Overall description: The latest release of Feast was a concerted effort by the Feast community to make Feast available in more environments than Google Cloud. We’ve removed all hard couplings to managed services and made it possible to run Feast both on AWS and locally. . Version: Feast 0.8 . Feature Name: Support for AWS . Feature Description: Feast 0.8 now comes with support for deployment on AWS, with native support for job management on EMR, and support for both S3 and Kinesis as data sources. . Feature Benefit: Finally makes it possible for Kubeflow users on AWS to run Feast . Feature Name: Batch-only ingestion . Feature Description: Allows teams to ingest data into stores without passing the data through a stream. . Feature Benefit: Allows for a more performant ingestion compared to the stream-first approach. . Feature Name: Local-only mode . Feature Description: Makes it possible to run Feast without any external infrastructure, using only Docker Compose or Minikube . Feature Benefit: Lowers the barrier to entry for new users, and makes it easier to test and develop Feast . EcoSystem: On-Prem SIG . Description: The on-prem SIG was officially created during this release with the intent to develop best practices for Kubeflow deployment in on-prem installations. With the new release, the SIG has also secured testing infrastructure in order to provide a well-tested reference architecture. . SIG Meeting Notes: https://bit.ly/2LyTh14 . Slack channel: https://kubeflow.slack.com/archives/C01C9NPD15H . Platform: AWS . Description: Better reliability, better testing coverage by enabling E2E tests for Kubeflow AWS deployment, better Kubeflow notebook user experience. . Platform: IBM . Description: Pipelines and Security have been the key focus for Kubeflow on IBM Cloud for this release. On the Pipelines side, Kubeflow Pipelines with Tekton is available for deployment on IBM Cloud Kubernetes Service and is included by default with Kubeflow deployment on IBM Cloud. On the security side, we have enabled integration with IBM Cloud AppId as an authentication provider instead of Dex. When using AppID, it delegates the identity provider to IBM Cloud with builtin identity providers (Cloud Directory, SAML, social log-in with Google or Facebook etc.) or custom providers. Additionally for securing the Kubeflow authentication with HTTPS we have provided integration instructions using the IBM Cloud Network Load Balancer. . Platform: GCP . Description: Better UX and reliability for installation and upgrade. Upgrade Cloud Config Connector in management cluster to latest. . Platform: Azure . Description: We added instructions for deploying Kubeflow with multi-tenancy backed by Azure Active Directory. Additionally, we documented the steps to replace the Metadata store with a managed Azure MySQL datatabase instance. . Platform: OpenShift . Description: Our focus for this release was to create the OpenShift stack that can install Kubeflow components on OpenShift 4.x . We architected the stack so users can pick and choose components they would like to install by adding or removing kustomizeConfig entries in the kfdef. Components currently supported are istio, single user pipeline, Jupyter notebooks with a custom Tensorflow notebook image, profile controller with custom image, Katib, pytorch and Tensorflow job operators and Seldon. You can install Kubeflow 1.2 on Openshift from the Open Data Hub community operator in OpenShift Catalog using the OpenShift kfdef. . Platform: MicroK8s . Description: Kubeflow is a built-in add-on to MicroK8s, and now includes Istio v1.5 as default. . Platform: MiniKF . Description: MiniKF is currently testing with Kubeflow 1.2 and will provide an updated MiniKF version based after validation testing and documentation has completed. Please find more information on MiniKF here: https://www.arrikto.com/get-started/ . You can also find tutorials that will guide you through end-to-end data science examples here: https://www.arrikto.com/tutorials .",
            "url": "https://blog.kubeflow.org/release/official/2020/11/18/kubeflow-1.2-blog-post.html",
            "relUrl": "/release/official/2020/11/18/kubeflow-1.2-blog-post.html",
            "date": " • Nov 18, 2020"
        }
        
    
  
    
        ,"post29": {
            "title": "Record metadata on Kubeflow from Notebooks",
            "content": "Lineage Tracking . This blog post will first guide you through the metadata SDK API, to create a notebook and log several actions to the metadata DB. Afterwards, you will be able to navigate to the Kubeflow UI and the resulting lineage graph, which gives you a graphical representation of the dependencies between the objects you logged using the SDK. | . Install the Kubeflow-metadata library . !pip install kubeflow-metadata --user # Install other packages: !pip install pandas --user # Then restart the Notebook kernel. . import pandas from kubeflow.metadata import metadata from datetime import datetime from uuid import uuid4 . METADATA_STORE_HOST = &quot;metadata-grpc-service.kubeflow&quot; # default DNS of Kubeflow Metadata gRPC serivce. METADATA_STORE_PORT = 8080 . Create a new Workspace and Run in a workspace . A Workspace groups a set of pipelines or notebooks runs, and their related artifacts and executions | Store is an object that provides a connection to the Metadata gRPC service | The Run object captures a pipeline or notebook run in a workspace | . ws1 = metadata.Workspace( # Connect to metadata service in namespace kubeflow in k8s cluster. store=metadata.Store(grpc_host=METADATA_STORE_HOST, grpc_port=METADATA_STORE_PORT), name=&quot;xgboost-synthetic&quot;, description=&quot;workspace for xgboost-synthetic artifacts and executions&quot;, labels={&quot;n1&quot;: &quot;v1&quot;}) . r = metadata.Run( workspace=ws1, name=&quot;xgboost-synthetic-faring-run&quot; + datetime.utcnow().isoformat(&quot;T&quot;) , description=&quot;a notebook run&quot;, ) . Create an execution in a run . An Execution is a specific instance of a run, and you can bind specific input/output artifacts to this instance. Execution also serves as object for logging artifacts as its input or output | . exec = metadata.Execution( name = &quot;execution&quot; + datetime.utcnow().isoformat(&quot;T&quot;) , workspace=ws1, run=r, description=&quot;execution for training xgboost-synthetic&quot;, ) print(&quot;An execution was created with id %s&quot; % exec.id) . An execution was created with id 290 . Log a data set and a model . A Log_input log an artifact as an input of this execution. Here exec.log_input accept an artifact class as an argument, a DataSet is an artifact. Every artifacts has different paramenters such as name, uri, query. The way to create DataSet artifact is calling ready-to-use APIs metadata.DataSet and provide arguments | . date_set_version = &quot;data_set_version_&quot; + str(uuid4()) data_set = exec.log_input( metadata.DataSet( description=&quot;xgboost synthetic data&quot;, name=&quot;synthetic-data&quot;, owner=&quot;someone@kubeflow.org&quot;, uri=&quot;file://path/to/dataset&quot;, version=&quot;v1.0.0&quot;, query=&quot;SELECT * FROM mytable&quot;)) print(&quot;Data set id is {0.id} with version &#39;{0.version}&#39;&quot;.format(data_set)) . Data set id is 171 with version &#39;data_set_version_cbebc757-0d76-4e1e-bbd9-02b065e4c3ea&#39; . A Log_output log an artifact as a output of this execution. Here exec.log_output accept an artifact class as an argument, a Model is an artifact. Every artifacts has different paramenters such as name, uri, hyperparameters. The way to create Model artifact is calling ready-to-use APIs metadata.Model and provide arguments | . model_version = &quot;model_version_&quot; + str(uuid4()) model = exec.log_output( metadata.Model( name=&quot;MNIST&quot;, description=&quot;model to recognize handwritten digits&quot;, owner=&quot;someone@kubeflow.org&quot;, uri=&quot;gcs://my-bucket/mnist&quot;, model_type=&quot;neural network&quot;, training_framework={ &quot;name&quot;: &quot;tensorflow&quot;, &quot;version&quot;: &quot;v1.0&quot; }, hyperparameters={ &quot;learning_rate&quot;: 0.5, &quot;layers&quot;: [10, 3, 1], &quot;early_stop&quot;: True }, version=model_version, labels={&quot;mylabel&quot;: &quot;l1&quot;})) print(model) print(&quot; nModel id is {0.id} and version is {0.version}&quot;.format(model)) . kubeflow.metadata.metadata.Model(workspace=None, name=&#39;MNIST&#39;, description=&#39;model to recognize handwritten digits&#39;, owner=&#39;someone@kubeflow.org&#39;, uri=&#39;gcs://my-bucket/mnist&#39;, version=&#39;model_version_50b419e2-af69-4c0e-a251-78246d4c0578&#39;, model_type=&#39;neural network&#39;, training_framework={&#39;name&#39;: &#39;tensorflow&#39;, &#39;version&#39;: &#39;v1.0&#39;}, hyperparameters={&#39;learning_rate&#39;: 0.5, &#39;layers&#39;: [10, 3, 1], &#39;early_stop&#39;: True}, labels={&#39;mylabel&#39;: &#39;l1&#39;}, id=172, create_time=&#39;2019-12-04T00:44:49.444411Z&#39;, kwargs={}) Model id is 172 and version is model_version_50b419e2-af69-4c0e-a251-78246d4c0578 . Log the evaluation of a model . Metrics captures an evaluation metrics of a model on a data set | . metrics = exec.log_output( metadata.Metrics( name=&quot;MNIST-evaluation&quot;, description=&quot;validating the MNIST model to recognize handwritten digits&quot;, owner=&quot;someone@kubeflow.org&quot;, uri=&quot;gcs://my-bucket/mnist-eval.csv&quot;, data_set_id=str(data_set.id), model_id=str(model.id), metrics_type=metadata.Metrics.VALIDATION, values={&quot;accuracy&quot;: 0.95}, labels={&quot;mylabel&quot;: &quot;l1&quot;})) print(&quot;Metrics id is %s&quot; % metrics.id) . Metrics id is 173 . Add Metadata for serving the model . serving_application = metadata.Execution( name=&quot;serving model&quot;, workspace=ws1, description=&quot;an execution to represent model serving component&quot;, ) # Noticed we use model name, version, uri to uniquely identify existing model. served_model = metadata.Model( name=&quot;MNIST&quot;, uri=&quot;gcs://my-bucket/mnist&quot;, version=model.version, ) m=serving_application.log_input(served_model) print(&quot;Found the mode with id {0.id} and version &#39;{0.version}&#39;.&quot;.format(m)) . Found the mode with id 172 and version &#39;model_version_50b419e2-af69-4c0e-a251-78246d4c0578&#39;. . Plot the lineage graph . The figure above shows an example of the lineage graph from our xgboost example. Follow below steps for you to try out: | . Follow the guide to setting up your Jupyter notebooks in Kubeflow | Go back to your Jupyter notebook server in the Kubeflow UI. (If you’ve moved away from the notebooks section in Kubeflow, click Notebook Servers in the left-hand navigation panel to get back there.) | In the Jupyter notebook UI, click Upload and follow the prompts to upload the xgboost example notebook. | Click the notebook name (build-train-deploy.ipynb.ipynb) to open the notebook in your Kubeflow cluster. | Run the steps in the notebook to install and use the Metadata SDK. | Click Artifact Store in the left-hand navigation panel on the Kubeflow UI. | Select Pipelines -&gt; Artifacts | Navigate to xgboost-synthetic-traing-eval | Click on Lineage explorer |",
            "url": "https://blog.kubeflow.org/jupyter/2020/10/01/lineage.html",
            "relUrl": "/jupyter/2020/10/01/lineage.html",
            "date": " • Oct 1, 2020"
        }
        
    
  
    
        ,"post30": {
            "title": "Data Science Meets Devops: MLOps with Jupyter, Git, & Kubernetes",
            "content": "The Problem . Kubeflow is a fast-growing open source project that makes it easy to deploy and manage machine learning on Kubernetes. . Due to Kubeflow’s explosive popularity, we receive a large influx of GitHub issues that must be triaged and routed to the appropriate subject matter expert. The below chart illustrates the number of new issues opened for the past year: . . Figure 1: Number of Kubeflow Issues To keep up with this influx, we started investing in a Github App called Issue Label Bot that used machine learning to auto label issues. Our first model was trained using a collection of popular public repositories on GitHub and only predicted generic labels. Subsequently, we started using Google AutoML to train a Kubeflow specific model. The new model was able to predict Kubeflow specific labels with average precision of 72% and average recall of 50%. This significantly reduced the toil associated with issue management for Kubeflow maintainers. The table below contains evaluation metrics for Kubeflow specific labels on a holdout set. The precision and recall below coincide with prediction thresholds that we calibrated to suit our needs. . Label Precision Recall . area-backend | 0.6 | 0.4 | . area-bootstrap | 0.3 | 0.1 | . area-centraldashboard | 0.6 | 0.6 | . area-components | 0.5 | 0.3 | . area-docs | 0.8 | 0.7 | . area-engprod | 0.8 | 0.5 | . area-front-end | 0.7 | 0.5 | . area-frontend | 0.7 | 0.4 | . area-inference | 0.9 | 0.5 | . area-jupyter | 0.9 | 0.7 | . area-katib | 0.8 | 1.0 | . area-kfctl | 0.8 | 0.7 | . area-kustomize | 0.3 | 0.1 | . area-operator | 0.8 | 0.7 | . area-pipelines | 0.7 | 0.4 | . area-samples | 0.5 | 0.5 | . area-sdk | 0.7 | 0.4 | . area-sdk-dsl | 0.6 | 0.4 | . area-sdk-dsl-compiler | 0.6 | 0.4 | . area-testing | 0.7 | 0.7 | . area-tfjob | 0.4 | 0.4 | . platform-aws | 0.8 | 0.5 | . platform-gcp | 0.8 | 0.6 | . Table 1: Evaluation metrics for various Kubeflow labels. Given the rate at which new issues are arriving, retraining our model periodically became a priority. We believe continuously retraining and deploying our model to leverage this new data is critical to maintaining the efficacy of our models. . Our Solution . Our CI/CD solution is illustrated in Figure 2. We don’t explicitly create a directed acyclic graph (DAG) to connect the steps in an ML workflow (e.g. preprocessing, training, validation, deployment, etc…). Rather, we use a set of independent controllers. Each controller declaratively describes the desired state of the world and takes actions necessary to make the actual state of the world match. This independence makes it easy for us to use whatever tools make the most sense for each step. More specifically we use . Jupyter notebooks for developing models. | GitOps for continuous integration and deployment. | Kubernetes and managed cloud services for underlying infrastructure. | . . Figure 2: illustrates how we do CI/CD. Our pipeline today consists of two independently operating controllers. We configure the Trainer (left hand side) by describing what models we want to exist; i.e. what it means for our models to be “fresh”. The Trainer periodically checks whether the set of trained models are sufficiently fresh and if not trains a new model. We likewise configure the Deployer (right hand side) to define what it means for the deployed model to be in sync with the set of trained models. If the correct model is not deployed it will deploy a new model. For more details on model training and deployment refer to the Actuation section below. . Background . Building Resilient Systems With Reconcilers . A reconciler is a control pattern that has proven to be immensely useful for building resilient systems. The reconcile pattern is at the heart of how Kubernetes works. Figure 3 illustrates how a reconciler works. A reconciler works by first observing the state of the world; e.g. what model is currently deployed. The reconciler then compares this against the desired state of the world and computes the diff; e.g the model with label “version=20200724” should be deployed, but the model currently deployed has label “version=20200700”. The reconciler then takes the action necessary to drive the world to the desired state; e.g. open a pull request to change the deployed model. . . Figure 3. Illustration of the reconciler pattern as applied by our deployer. Reconcilers have proven immensely useful for building resilient systems because a well implemented reconciler provides a high degree of confidence that no matter how a system is perturbed it will eventually return to the desired state. . There is no DAG . The declarative nature of controllers means data can flow through a series of controllers without needing to explicitly create a DAG. In lieu of a DAG, a series of data processing steps can instead be expressed as a set of desired states, as illustrated in Figure 4 below: . . Figure 4: illustrates how pipelines can emerge from independent controllers without explicitly encoding a DAG. Here we have two completely independent controllers. The first controller ensures that for every element ai there should be an element bi. The second controller ensures that for every element bi there should be an element ci. This reconciler-based paradigm offers the following benefits over many traditional DAG-based workflows: . Resilience against failures: the system continuously seeks to achieve and maintain the desired state. | Increased autonomy of engineering teams: each team is free to choose the tools and infrastructure that suit their needs. The reconciler framework only requires a minimal amount of coupling between controllers while still allowing one to write expressive workflows. | Battle tested patterns and tools: This reconciler based framework does not invent something new. Kubernetes has a rich ecosystem of tools that aim to make it easy to build controllers. The popularity of Kubernetes means there is a large and growing community familiar with this pattern and supporting tools. | . GitOps: Operation By Pull Request . GitOps, Figure 5, is a pattern for managing infrastructure. The core idea of GitOps is that source control (doesn’t have to be git) should be the source of truth for configuration files describing your infrastructure. Controllers can then monitor source control and automatically update your infrastructure as your config changes. This means to make a change (or undo a change) you just open a pull request. . . Figure 5: To push a new model for Label Bot we create a PR updating the config map storing the id of the Auto ML model we want to use. When the PR is merged, Anthos Config Management(ACM) automatically rolls out those changes to our GKE cluster. As a result, subsequent predictions are made using the new model. (Image courtesy of Weaveworks) Putting It Together: Reconciler + GitOps = CI/CD for ML . With that background out of the way, let’s dive into how we built CI/CD for ML by combining the Reconciler and GitOps patterns. . There were three problems we needed to solve: . How do we compute the diff between the desired and actual state of the world? | How do we affect the changes needed to make the actual state match the desired state? | How do we build a control loop to continuously run 1 &amp; 2? | Computing Diffs . To compute the diffs we just write lambdas that do exactly what we want. So in this case we wrote two lambdas: . The first lambda determines whether we need to retrain based on the age of the most recent model. | The second lambda determines whether the model needs to be updated by comparing the most recently trained model to the model listed in a config map checked into source control. | We wrap these lambdas in a simple web server and deploy on Kubernetes. One reason we chose this approach is because we wanted to rely on Kubernetes’ git-sync to mirror our repository to a pod volume. This makes our lambdas super simple because all the git management is taken care of by a side-car running git-sync. . Actuation . To apply the changes necessary, we use Tekton to glue together various CLIs that we use to perform the various steps. . Model Training . To train our model we have a Tekton task that: . Runs our notebook using papermill. | Converts the notebook to html using nbconvert. | Uploads the .ipynb and .html files to GCS using gsutil | This notebook fetches GitHub Issues data from BigQuery and generates CSV files on GCS suitable for import into Google AutoML. The notebook then launches an AutoML job to train a model. . We chose AutoML because we wanted to focus on building a complete end to end solution rather than iterating on the model. AutoML provides a competitive baseline that we may try to improve upon in the future. . To easily view the executed notebook we convert it to html and upload it to GCS which makes it easy to serve public, static content. This allows us to use notebooks to generate rich visualizations to evaluate our model. . Model Deployment . To deploy our model we have a Tekton task that: . Uses kpt to update our configmap with the desired value. | Runs git to push our changes to a branch. | Uses a wrapper around the GitHub CLI (gh) to create a PR. | The controller ensures there is only one Tekton pipeline running at a time. We configure our pipelines to always push to the same branch. This ensures we only ever open one PR to update the model because GitHub doesn’t allow multiple PRs to be created from the same branch. . Once the PR is merged Anthos Config Mesh automatically applies the Kubernetes manifests to our Kubernetes cluster. . Why Tekton . We picked Tekton because the primary challenge we faced was sequentially running a series of CLIs in various containers. Tekton is perfect for this. Importantly, all the steps in a Tekton task run on the same pod which allows data to be shared between steps using a pod volume. . Furthermore, since Tekton resources are Kubernetes resources we can adopt the same GitOps pattern and tooling to update our pipeline definitions. . The Control Loop . Finally, we needed to build a control loop that would periodically invoke our lambdas and launch our Tekton pipelines as needed. We used kubebuilder to create a simple custom controller. Our controller’s reconcile loop will call our lambda to determines whether a sync is needed and if so with what parameters. If a sync is needed the controller fires off a Tekton pipeline to perform the actual update. An example of our custom resource is illustrated below: . apiVersion: automl.cloudai.kubeflow.org/v1alpha1 kind: ModelSync metadata: name: modelsync-sample namespace: label-bot-prod spec: failedPipelineRunsHistoryLimit: 10 needsSyncUrl: http://labelbot-diff.label-bot-prod/needsSync parameters: - needsSyncName: name pipelineName: automl-model pipelineRunTemplate: spec: params: - name: automl-model value: notavlidmodel - name: branchName value: auto-update - name: fork value: git@github.com:kubeflow/code-intelligence.git - name: forkName value: fork pipelineRef: name: update-model-pr resources: - name: repo resourceSpec: params: - name: url value: https://github.com/kubeflow/code-intelligence.git - name: revision value: master type: git serviceAccountName: auto-update successfulPipelineRunsHistoryLimit: 10 . The custom resource specifies the endpoint, needsSyncUrl, for the lambda that computes whether a sync is needed and a Tekton PipelineRun, pipelineRunTemplate, describing the pipeline run to create when a sync is needed. The controller takes care of the details; e.g. ensuring only 1 pipeline per resource is running at a time, garbage collecting old runs, etc… All of the heavy lifting is taken care of for us by Kubernetes and kubebuilder. . Note, for historical reasons the kind, ModelSync, and apiVersion automl.cloudai.kubeflow.org are not reflective of what the controller actually does. We plan on fixing this in the future. . Build Your Own CI/CD pipelines . Our code base is a long way from being polished, easily reusable tooling. Nonetheless it is all public and could be a useful starting point for trying to build your own pipelines. . Here are some pointers to get you started: . Use the Dockerfile to build your own ModelSync controller | Modify the kustomize package to use your image and deploy the controller | Define one or more lambdas as needed for your use cases You can use our Lambda server as an example | We wrote ours in go but you can use any language and web framework you like (e.g. flask) | . | Define Tekton pipelines suitable for your use cases; our pipelines(linked below) might be a useful starting point Notebook Tekton task - Run notebook with papermill and upload to GCS | PR Tekton Task - Tekton task to open GitHub PRs | . | Define ModelSync resources for your use case; you can refer to ours as an example ModelSync Deploy Spec - YAML to continuously deploy label bot | ModelSync Train Spec - YAML to continuously train our model | . | If you’d like to see us clean it up and include it in a future Kubeflow release please chime in on issue kubeflow/kubeflow#5167. . What’s Next . Lineage Tracking . Since we do not have an explicit DAG representing the sequence of steps in our CI/CD pipeline understanding the lineage of our models can be challenging. Fortunately, Kubeflow Metadata solves this by making it easy for each step to record information about what outputs it produced using what code and inputs. Kubeflow metadata can easily recover and plot the lineage graph. The figure below shows an example of the lineage graph from our xgboost example. . . Figure 6: screenshot of the lineage tracking UI for our xgboost example. Our plan is to have our controller automatically write lineage tracking information to the metadata server so we can easily understand the lineage of what’s in production. . Conclusion . . Building ML products is a team effort. In order to move a model from a proof of concept to a shipped product, data scientists and devops engineers need to collaborate. To foster this collaboration, we believe it is important to allow data scientists and devops engineers to use their preferred tools. Concretely, we wanted to support the following tools for Data Scientists, Devops Engineers, and SREs: . Jupyter notebooks for developing models. | GitOps for continuous integration and deployment. | Kubernetes and managed cloud services for underlying infrastructure. | . To maximize each team’s autonomy and reduce dependencies on tools, our CI/CD process follows a decentralized approach. Rather than explicitly define a DAG that connects the steps, our approach relies on a series of controllers that can be defined and administered independently. We think this maps naturally to enterprises where responsibilities might be split across teams; a data engineering team might be responsible for turning weblogs into features, a modeling team might be responsible for producing models from the features, and a deployments team might be responsible for rolling those models into production. . Further Reading . If you’d like to learn more about GitOps we suggest this guide from Weaveworks. . To learn how to build your own Kubernetes controllers the kubebuilder book walks through an E2E example. .",
            "url": "https://blog.kubeflow.org/mlops/",
            "relUrl": "/mlops/",
            "date": " • Aug 1, 2020"
        }
        
    
  
    
        ,"post31": {
            "title": "Kubeflow 1.1 improves ML Workflow Productivity, Isolation & Security, and GitOps",
            "content": "The Kubeflow Community’s delivery of Kubeflow 1.1 offers users valuable ML workflow automation with Fairing and Kale along with MXNet and XGBoost distributed training operators. It extends isolation and security through the delivery of multi-user pipelines, CVE scanning, and support for Google’s Private GKE and Anthos. 1.1 also improves Katib’s hyperparameter tuning features by offering new frameworks &amp; algorithms, and enables flexible configuration &amp; tuning options. 1.1 provides a foundation for consistent and repeatable installation and operations using GitOps methodologies powered by blueprints and kpt primitives. . The ML productivity enhancements in 1.1 include end-to-end workflows using Fairing and Kale. The Fairing workflow enables users to build, train and deploy models from a notebook and Fairing improvements include the support of configuring environment variables and mounting secrets. Fairing also added a config map for a deployer and bug fixes for TensorRTSpec. The workflows enabled by Kale include the ability to write model code in a notebook and then automatically build a Kubeflow pipeline that deploys, trains and tunes that model efficiently, using Katib and cached pipeline steps. Kubeflow 1.1 also delivers stable release deliveries of MXNet and XGBoost operators, which simplify distributed training on multiple nodes and speeds model creation. . The isolation and security feature deliveries include Private GKE and Anthos support, a stable version of Kubeflow Pipelines with Multi-User Kubeflow Pipelines support, and a process for Kubeflow container image scanning, CVE reporting, and an optional process for distroless image creation. 1.1 also includes options for authentication and authorization. This includes the option for administrators to turn off self-service namespace creation mode, as admins may have other processes for namespace creation. The Community also developed a best practice to build user authorization in Kubeflow web apps using subject access review. . The 1.1 Katib improvements deliver new frameworks &amp; algorithms, and flexible configuration &amp; tuning options. The new frameworks &amp; algorithms include: . Integrate goptuna framework with (CMA-ES) Covariance Matrix Adaptation Evolution Strategy algorithm. | DARTS (Differentiable Architecture Search) algorithm implementation. | Better support for HP frameworks (chocolate, hyperopt, skopt). | . Katib also adds these flexible configuration &amp; tuning options: . Implements a python SDK to run Katib experiments from Kubeflow notebooks. | Enables users to run experiments without the goal defined. | Provides a new trial template UI editor. | Enables users to view experiment and suggestion status in the UI, during the experiment run. | Adds a resume policy for experiments to clean-up suggestion resources. | . The installation and operations of Kubeflow have been enhanced to support GitOps methodologies. Several Kubeflow platform providers and software support vendors are developing time-saving GitOps processes to simplify and codify the installation, configuration and operations of the various layers in the Kubeflow 1.1 hardware and software stack. Some examples are provided in the next section. . More details and 1.1 tutorials . Kubeflow 1.1 includes many technical enhancements, which are being delivered via the Community’s release process. Details on the application feature development can be found in the 1.1 KanBan Board and in the Kubeflow Roadmap. As the Kubeflow application improvements are merged, the platform teams (GCP, AWS, IBM, Red Hat, Azure, and Arrikto MiniKF) are working to validate the feature improvements on their respective environments. . Kubeflow 1.1 includes KFServing v0.3, where the focus has been on providing more stability by doing a major move to KNative v1 APIs. Additionally, we added GPU support for PyTorch model servers, and pickled model format support for SKLearn. There were other enhancements vis a vis routing, payload logging, bug fixes etc., details of which can be found here. . Kubeflow 1.1 demo scripts and workflow tutorials are available as validated by the individual platforms. Please find those below: . Kubeflow 1.1 Tutorial for Automated Hyperparameter Tuning, Multi-user Pipelines, Pipeline Caching | GitOps for Kubeflow 1.1 on AWS EKS | . 1.1 users can also leverage several other Kubeflow ecosystem tools including: . Seldon Core 1.1, which handles scaling to thousands of production ML models and provides advanced ML capabilities including Advanced Metrics, Request Logging, Explainers, Outlier Detectors, A/B Tests, Canaries and more. | Feast: A feature store that allows teams ML teams to define, manage, discover, and serve ML features to their models. | . What’s Coming and Getting involved . The Kubeflow Community has started planning for its next release. Although we have a nice backlog of issues, our process includes discussions and surveys with users and contributors to validate use cases and their value. . The Community continues to refine its governance and refine this proposal, Proposal for Kubeflow WG Guidelines/Governance. We are actively developing Working Group team charters, tech leads, chairs and members. We look forward to this growth. . The following provides some helpful links to those looking to get involved with the Kubeflow Community: . Join the Kubeflow Slack channel | Join the kubeflow-discuss mailing list | Attend a weekly community meeting | . If you have questions, run into issues, please leverage the Slack channel and/or submit bugs via Kubeflow on GitHub. Thanks from all of us in the Community, and we look forward to your success with Kubeflow 1.1. . Special thanks to Yuan Tang (Ant Group), Josh Bottum (Arrikto), Constantinos Venetsanopoulos (Arrikto), Yannis Zarkadas (Arrikto), Jiaxin Shan (AWS), Dan Sun (Bloomberg), Andrey Velichkevich (Cisco), Krishna Durai (Cisco), Hamel Husain (GitHub), Willem Pienaar (GoJek), Yuan Gong (Google), Jeremy Lewi (Google), Animesh Singh (IBM) and Clive Cox (Seldon) for their help on this post. .",
            "url": "https://blog.kubeflow.org/release/official/2020/07/31/kubeflow-1.1-blog-post.html",
            "relUrl": "/release/official/2020/07/31/kubeflow-1.1-blog-post.html",
            "date": " • Jul 31, 2020"
        }
        
    
  
    
        ,"post32": {
            "title": "Kubeflow & Kale simplify building better ML Pipelines with automatic hyperparameter tuning",
            "content": "Running pipelines at scale has never been easier. . Kubeflow’s Kale is maturing and fast becoming the superfood that glues together the main Kubeflow components to provide a cohesive and seamless data science experience. . TL;DR: Convert Notebook to Kubeflow Pipelines, run them as hyperparameter tuning experiments, track executions and artifacts with MLMD, cache and maintain an immutable history of executions: Kale brings all of this on the table in a unified workflow tool, simple to use. . Running pipelines at scale has never been easier . Kubeflow’s Kale is maturing and fast becoming the superfood that glues together the main Kubeflow components to provide a cohesive and seamless data science experience. With its newest release, Kale provides an end-to-end workflow that encompasses Jupyter Notebooks, Kubeflow Pipelines, hyperparameter tuning with Katib, metadata tracking with ML Metadata (MLMD), and faster pipeline executions with caching. . If you are new to Kale, head over to this short introduction to get started! . In this blog post, you will learn about the features that Kale is bringing to the Machine Learning community with version 0.5, and learn how to get started with a curated example. . New Face . First off, we are excited to reveal the new Kale logo. Kudos to Konstantinos Palaiologos (Arrikto) for designing the brand new, modern Kale leaf. This will be the new face of the project from now on. . . Hyperparameter Tuning . The major new addition in v0.5 is the support for running pipelines with Katib. Katib is Kubeflow’s component to run general purpose hyperparameter tuning jobs. Just as you would press a single button to convert a notebook to a pipeline, you can now press a button and let Kale start a hyperparameter Job on that pipeline. All you need to do is tell Kale what the HP tuning job should search for. . Running hyperparameter tuning jobs gives you a dramatic boost in delivering good results for your project. The *manual *tuning process of running your model countless times, using different parameters combinations, aggregating them and comparing them, is error-prone and inefficient. Delegating this work to an automated process allows you to become faster, more efficient and accurate. . Parametrize the HP tuning Job directly from the notebook . Katib does not know anything about the jobs that it is actually running (called Trials in the Katib jargon). Katib supports running Trials as simple Jobs (that is, Pods), BatchJobs, TFJobs, and PyTorchJobs. Kale 0.5 integrates Katib with Kubeflow Pipelines. This enables Katib trails to run as pipelines in KFP. The metrics from the pipeline runs are provided to help in model performance analysis and debugging. All Kale needs to know from the user is the search space, the optimization algorithm, and the search goal. . Kale will also make sure that all the runs of a Katib experiment, end up unified and grouped, under a single KFP experiment, to make it easy to search and isolate a particular job. . Kale will also show a live view of the running experiments, directly in the notebook, so you will know how many pipelines are still running and, upon completion, which one performed best. . New features . Pipeline parameters and metrics . In order to run pipelines with hyperparameter tuning, the pipeline needs to be able to accept arguments and produce metrics. Enabling the pipeline to do this, is now tremendously easy. Kale provides two new cell tags: pipeline-parameters and pipeline-metrics. . . Assigning the pipeline-parameters tag on any cell that contains some variables will instruct Kale to transform them to pipeline parameters. These values will then be passed to the pipeline steps that actually make use of them. . . If you want the pipeline to produce some metrics, just print them at the end of the notebook and assign the pipeline-metrics tag to the cell. Kale will take care of understanding which steps produce the metrics and you will see them appear in the KFP dashboard. . Rich notebook outputs . Having your pipelines produce rich outputs (like plots, tables, metrics, …) that can be captured and displayed by the Kubeflow Pipelines dashboard has always been somewhat cumbersome. You would need to write some KFP-specific code to produce json artifacts that would then be interpreted by KFP. . What if you could just write plain Python in your Notebook using your favourite plotting library, and have the plots auto-magically appear as KFP outputs, when the Notebook gets compiled into a pipeline? . Now, when running your notebook code inside a pipeline step, Kale will wrap it and feed it to an ipython kernel, so that all the nice artifacts produced in the notebook, will be produced in the pipeline as well. Kale will capture all these rich outputs automatically and instruct KFP to display them in the dashboard. Effectively, whatever happens in the notebook, now happens in the pipeline as well. The execution context is exactly the same. . Any rich output that is visible in the notebook gets captured by Kale and exposed in the KFP dashboard. . MLMD Integration . An important part of running reproducible Machine Learning collaboratively and at scale, is being able to track pipeline executions, their inputs, their outputs and how these are connected together. Kubeflow provides an ML Metadata service which serves this exact purpose. This service also includes a lineage view to enable the user to have a deep insight into the whole history of events. . Kale is now fully integrated with this service, logging each new execution automatically alongside all the artifacts produced by the pipeline. . Run on GPU . If you need to run a particular step on a GPU node, Kale has you covered too. You can now annotate steps with a dedicated dialog, directly from the notebook and each step can have its own annotations. This is just the first iteration, Kale will support adding any kind of K8s limits or annotations to pipeline steps in the near future. . . Overall UI and performance improvements . The new version of Kale’s JupyterLab brings tons of performance improvements and UI enhancements. Updating the notebook cells’ annotations will now be easier and faster. We covered many corner cases and solved tons of bugs. The UI of the cell’s annotation editor is more consistent with the overall Jupyter style and much more intuitive and easy to use. A big shoutout to Tasos Alexiou (Arrikto) for having spent countless hours in understanding the Jupyter internals and improving our application lifecycle. . Hands-On . To start playing-around with Kale v0.5, head over to the GitHub repository and follow the installation instructions. If you are already running Kubeflow (either in your own cluster or on MiniKF), spin up a new Notebook Server using the image gcr.io/arrikto/jupyter-kale:v0.5.0. . Note: Kale v0.5 needs to run on Kubeflow ≥ 1.0. Also, make sure that the following Kubeflow components are updated as follows: . Katib controller: gcr.io/arrikto/katib-controller:40b5b51a . | Katib Chocolate service: gcr.io/arrikto/suggestion-chocolate:40b5b51a . | . We will release a new version of MiniKF very soon, containing a lot of improvements that will make the Kale experience even better. You will also be able to go through a new Codelab to try out the Kale-Katib integration yourself. Stay tuned for updates on the Arrikto channels. . Road Ahead . We are always looking to improve Kale and help data scientists have a seamless ML workflow from writing code to training, optimizing, and serving their models. . We are excited to have the ML community try out this new version of Kale and the coming MiniKF update. . A special mention must go to the various members of the Arrikto team (Ilias Katsakioris, Chris Pavlou, Kostis Lolos, Tasos Alexiou) who contributed to delivering all these new features. .",
            "url": "https://blog.kubeflow.org/integrations/2020/07/10/kubeflow-kale.html",
            "relUrl": "/integrations/2020/07/10/kubeflow-kale.html",
            "date": " • Jul 10, 2020"
        }
        
    
  
    
        ,"post33": {
            "title": "Introduction to Kubeflow MPI Operator and Industry Adoption",
            "content": "Kubeflow just announced its first major 1.0 release recently, which makes it easy for machine learning engineers and data scientists to leverage cloud assets (public or on-premise) for machine learning workloads. In this post, we’d like to introduce MPI Operator (docs), one of the core components of Kubeflow, currently in alpha, which makes it easy to run synchronized, allreduce-style distributed training on Kubernetes. . There are two major distributed training strategies nowadays: one based on parameter servers and the other based on collective communication primitives such as allreduce. . Parameter server based distribution strategy relies on centralized parameter servers for coordination between workers, responsible for collecting gradients from workers and sending updated parameters to workers. The diagram below shows the interaction between parameter servers and worker nodes under this distributed training strategy. . . While distributed training based on parameter servers can support training very large models and datasets by adding more workers and parameter servers, there are additional challenges involved in order to optimize the performance: . It is not easy to identify the right ratio of the number of workers to the number of parameter servers. For example, if only a small number of parameter servers are used, network communication will likely become the bottleneck for training. . | If many parameter servers are used, the communication may saturate network interconnects. . | The memory quota of workers and parameter servers requires fine tuning to avoid out-of-memory errors or memory waste. . | If the model could fit within the computational resources of each worker, additional maintenance and communication overheads are introduced when the model is partitioned to multiple parameter servers. . | We need to replicate the model on each parameter server in order to support fault-tolerance, which requires additional computational and storage resources. . | . In contrast, distributed training based on collective communication primitives such as allreduce could be more efficient and easier to use in certain use cases. Under allreduce-based distributed training strategy, each worker stores a complete set of model parameters. In other words, no parameter server is needed. Allreduce-based distributed training could address many of the challenges mentioned above: . Each worker stores a complete set of model parameters, no parameter server is needed, so it’s straightforward to add more workers when necessary. . | Failures among the workers can be recovered easily by restarting the failed workers and then load the current model from any of the existing workers. Model does not need to be replicated to support fault-tolerance. . | The model can be updated more efficiently by fully leveraging the network structure and collective communication algorithms. For example, in ring-allreduce algorithm, each of the N workers only needs to communicate with two of its peer workers 2 * (N − 1) times to update all the model parameters completely. . | Scaling up and down the number of workers is as easy as reconstructing the underlying allreduce communicator and re-assigning the ranks among the workers. . | . There are many existing technologies available that provide implementations for these collective communication primitives such as NCCL, Gloo, and many different implementations of MPI. . MPI Operator provides a common Custom Resource Definition (CRD) for defining a training job on a single CPU/GPU, multiple CPU/GPUs, and multiple nodes. It also implements a custom controller to manage the CRD, create dependent resources, and reconcile the desired states. . . Unlike other operators in Kubeflow such as TF Operator and PyTorch Operator that only supports for one machine learning framework, MPI operator is decoupled from underlying framework so it can work well with many frameworks such as Horovod, TensorFlow, PyTorch, Apache MXNet, and various collective communication implementations such as OpenMPI. . For more details on comparisons between different distributed training strategies, various Kubeflow operators, please check out our presentation at KubeCon Europe 2019. . Example API Spec . We’ve been working closely with the community and industry adopters to improve the API spec for MPI Operator so it’s suitable for many different use cases. Below is an example: . apiVersion: kubeflow.org/v1alpha2 kind: MPIJob metadata: name: tensorflow-benchmarks spec: slotsPerWorker: 1 cleanPodPolicy: Running mpiReplicaSpecs: Launcher: replicas: 1 template: spec: containers: - image: mpioperator/tensorflow-benchmarks:latest name: tensorflow-benchmarks command: - mpirun - python - scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py - --model=resnet101 - --batch_size=64 - --variable_update=horovod Worker: replicas: 2 template: spec: containers: - image: mpioperator/tensorflow-benchmarks:latest name: tensorflow-benchmarks resources: limits: nvidia.com/gpu: 1 . Note that MPI Operator provides a flexible but user-friendly API that’s consistent across other Kubeflow operators. . Users can easily customize their launcher and worker pod specs by modifying the relevant sections in the template. For example, customizing to use various types of computational resources such as CPUs, GPUs, memory, etc. . In addition, below is an example spec that performs distributed TensorFlow training job using ImageNet data in TFRecords format stored in a Kubernetes volume: . apiVersion: kubeflow.org/v1alpha2 kind: MPIJob metadata: name: tensorflow-benchmarks spec: slotsPerWorker: 1 cleanPodPolicy: Running mpiReplicaSpecs: Launcher: replicas: 1 template: spec: containers: - image: mpioperator/tensorflow-benchmarks:latest name: tensorflow-benchmarks command: - mpirun - python - scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py - --model=resnet101 - --batch_size=64 - --variable_update=horovod Worker: replicas: 2 template: spec: containers: - image: mpioperator/tensorflow-benchmarks:latest name: tensorflow-benchmarks resources: limits: nvidia.com/gpu: 1 . Architecture . MPI Operator contains a custom controller that listens for changes in MPIJob resources. When a new MPIJob is created, the controller goes through the following logical steps: . Create a ConfigMap that contains: | A helper shell script that can be used by mpirun in place of ssh. It invokes kubectl exec for remote execution. . | A hostfile that lists the pods in the worker StatefulSet (in the form of ${job-id}-worker-0, ${job-id}-worker-1, …), and the available slots (CPUs/GPUs) in each pod. . | . Create the RBAC resources (Role, ServiceAccount, RoleBinding) to allow remote execution (pods/exec). . | Wait for the worker pods to be ready. . | Create the launcher job. It runs under the ServiceAccount created in step 2, and sets up the necessary environment variables for executing mpirun commands remotely. The kubectl binary is delivered to an emptyDir volume through an init container. . | After the launcher job finishes, set the replicas to 0 in the worker StatefulSet. . | . For more details, please check out the design doc for MPI Operator. . Industry Adoption . At the time of writing, there are 13 disclosed industry adopters and many others who’ve been working closely with the community to reach where we are today. We’d like to showcase some of the use cases of MPI Operator in several companies. If your company would like to be included in the list of adopters, please send us a pull request on GitHub! . Ant Group . At Ant Group, we manage Kubernetes clusters with tens of thousands of nodes and have deployed the MPI Operator along with other Kubeflow operators. The MPI Operator leverages the network structure and collective communication algorithms so that users don’t have to worry about the right ratio between the number of workers and parameter servers to obtain the best performance. Users can focus on building out their model architectures without spending time on tuning the downstream infrastructure for distributed training. . The models produced have been widely deployed in production and battle-tested in many different real life scenarios. One notable use case is Saofu — a mobile app for users to scan any “福” (Chinese character that represents fortune) through augmented reality to enter a lucky draw where each user would receive a virtual red envelope with a portion of a significant amount of money. . Bloomberg . Bloomberg, the global business and financial information and news leader, possesses an enormous amount of data — from historical news to real-time market data and everything in between. Bloomberg’s Data Science Platform was built to allow the company’s internal machine learning engineers and data scientists to more easily leverage data and algorithmic models in their daily work, including when training jobs and automatic machine learning models used in the state-of-the-art solutions they’re building. . “The Data Science Platform at Bloomberg offers a TensorFlowJob CRD similar to Kubeflow’s own TFJob, enabling the company’s data scientists to easily train neural network models. Recently, the Data Science Platform team enabled Horovod-based distributed training in its TensorFlowJob via the MPI Operator as an implementation detail. Using MPIJob in the back-end has allowed the Bloomberg Data Science Platform team to quickly offer its machine learning engineers a robust way to train a BERT model within hours using the company’s large corpus of text data’’, says Chengjian Zheng, software engineer from Bloomberg. . Caicloud . Caicloud Clever is an artificial intelligence cloud platform based on Caicloud container cloud platform with powerful hardware resource management and efficient model development capabilities. Caicloud products have been deployed in many 500 China Fortune companies. . “Caicloud Clever supports multiple frameworks of AI model training including TensorFlow, Apache MXNet, Caffe, PyTorch with the help of Kubeflow tf-operator, pytorch-operator and others”, says Ce Gao, AI infrastructure engineer from Caicloud Clever team. “While RingAllReduce distributed training support is requested for improved customer maturity.” . Kubeflow MPI operator is a Kubernetes Operator for allreduce-style distributed training. Caicloud Clever team adopts MPI Operator’s v1alpha2 API. The Kubernetes native API makes it easy to work with the existing systems in the platform. . Iguazio . Iguazio provides a cloud-native data science platform with emphasis on automation, performance, scalability, and use of open-source tools. . According to Yaron Haviv, the Founder and CTO of Iguazio, “We evaluated various mechanisms which will allow us to scale deep learning frameworks with minimal developer effort and found that using the combination of Horovod with the MPI Operator over Kubernetes is the best tool for the job since it enable horizontal scalability, supports multiple frameworks such as TensorFlow and PyTorch and doesn’t require too much extra coding or the complex use of parameter servers.” . Iguazio have integrated the MPI Operator into its managed service offering and its fast data layer for maximum scalability, and work to simplify the usage through open source projects like MLRun (for ML automation and tracking). Check out this blog post with an example application that demonstrates Iguazio’s usage of the MPI Operator. . Polyaxon . Polyaxon is a platform for reproducible and scalable machine learning on Kubernetes, it allows users to iterate faster on their research and model creation. Polyaxon provides a simple abstraction for data scientists and machine learning engineers to streamline their experimentation workflow, and provides a very cohesive abstraction for training and tracking models using popular frameworks such as Scikit-learn, TensorFlow, PyTorch, Apache MXNet, Caffe, etc. . “Several Polyaxon users and customers were requesting an easy way to perform an allreduce-style distributed training, the MPI Operator was the perfect solution to provide such abstraction. Polyaxon is deployed at several companies and research institutions, and the public docker hub has over 9 million downloads.”, says Mourad Mourafiq, the Co-founder of Polyxagon. . Community and Call for Contributions . We are grateful for over 28 individual contributors from over 11 organizations, namely Alibaba Cloud, Amazon Web Services, Ant Group, Bloomberg, Caicloud, Google Cloud, Huawei, Iguazio, NVIDIA, Polyaxon, and Tencent, that have contributed directly to MPI Operator’s codebase and many others who’ve filed issues or helped resolve them, asked and answered questions, and were part of inspiring discussions. We’ve put together a roadmap that provides a high-level overview of where the MPI Operator will grow in future releases and we welcome any contributions from the community! . We could not have achieved our milestones without an incredibly active community. Check out our community page to learn more about how to join the Kubeflow community! . Originally published at https://terrytangyuan.github.io on March 17, 2020. .",
            "url": "https://blog.kubeflow.org/integrations/operators/2020/03/16/mpi-operator.html",
            "relUrl": "/integrations/operators/2020/03/16/mpi-operator.html",
            "date": " • Mar 16, 2020"
        }
        
    
  
    
        ,"post34": {
            "title": "Kubeflow 1.0 - Cloud Native ML for Everyone",
            "content": "Kubeflow 1.0: Cloud Native ML for Everyone . On behalf of the entire community, we are proud to announce Kubeflow 1.0, our first major release. Kubeflow was open sourced at Kubecon USA in December 2017, and during the last two years the Kubeflow Project has grown beyond our wildest expectations. There are now hundreds of contributors from over 30 participating organizations. . Kubeflow’s goal is to make it easy for machine learning (ML) engineers and data scientists to leverage cloud assets (public or on-premise) for ML workloads. You can use Kubeflow on any Kubernetes-conformant cluster. . With 1.0, we are graduating a core set of stable applications needed to develop, build, train, and deploy models on Kubernetes efficiently. (Read more in Kubeflow’s versioning policies and application requirements for graduation.) . Graduating applications include: . Kubeflow’s UI, the central dashboard . | Jupyter notebook controller and web app . | Tensorflow Operator (TFJob) and PyTorch Operator for distributed training . | kfctl for deployment and upgrades . | Profile controller and UI for multiuser management . | . Hear more about Kubeflow’s mission and 1.0 release in this interview with Kubeflow founder and core contributor Jeremy Lewi on the Kubernetes Podcast. . Develop, Build, Train, and Deploy with Kubeflow . Kubeflow’s 1.0 applications that make up our develop, build, train, deploy critical user journey. . With Kubeflow 1.0, users can use Jupyter to develop models. They can then use Kubeflow tools like fairing (Kubeflow’s python SDK) to build containers and create Kubernetes resources to train their models. Once they have a model, they can use KFServing to create and deploy a server for inference. . Getting Started with ML on Kubernetes . Kubernetes is an amazing platform for leveraging infrastructure (whether on public cloud or on-premises), but deploying Kubernetes optimized for ML and integrated with your cloud is no easy task. With 1.0 we are providing a CLI and configuration files so you can deploy Kubeflow with one command: . kfctl apply -f [kfctl_gcp_iap.v1.0.0.yaml](https://github.com/kubeflow/manifests/blob/master/kfdef/kfctl_gcp_iap.yaml) kfctl apply -f [kfctl_k8s_istio.v1.0.0.yaml](https://github.com/kubeflow/manifests/blob/master/kfdef/kfctl_gcp_iap.yaml) kfctl apply -f [kfctl_aws_cognito.v1.0.0.yaml](https://github.com/kubeflow/manifests/blob/master/kfdef/kfctl_gcp_iap.yaml) kfctl apply -f [kfctl_ibm.v1.0.0.yaml](https://github.com/kubeflow/manifests/blob/master/kfdef/kfctl_gcp_iap.yaml) . Jupyter on Kubernetes . In Kubeflow’s user surveys, data scientists have consistently expressed the importance of Jupyter notebooks. Further, they need the ability to integrate isolated Jupyter notebooks with the efficiencies of Kubernetes on Cloud to train larger models using GPUs and run multiple experiments in parallel. Kubeflow makes it easy to leverage Kubernetes for resource management and put the full power of your datacenter at the fingertips of your data scientist. . With Kubeflow, each data scientist or team can be given their own namespace in which to run their workloads. Namespaces provide security and resource isolation. Using Kubernetes resource quotas, platform administrators can easily limit how much resources an individual or team can consume to ensure fair scheduling. . After deploying Kubeflow, users can leverage Kubeflow’s central dashboard for launching notebooks: . Kubeflow’s UI for managing notebooks: view and connect to existing notebooks or launch a new one. . In the Kubeflow UI users can easily launch new notebooks by choosing one of the pre-built docker images for Jupyter or entering the URL of a custom image. Next, users can set how many CPUs and GPUs to attach to their notebook. Notebooks can also include configuration and secrets parameters which simplify access to external repositories and databases. . . Training faster with distributed training . Distributed training is the norm at Google (blog), and one of the most exciting and requested features for deep learning frameworks like TensorFlow and PyTorch. . When we started Kubeflow, one of our key motivations was to leverage Kubernetes to simplify distributed training. Kubeflow provides Kubernetes custom resources that make distributed training with TensorFlow and PyTorch simple. All a user needs to do is define a TFJob or PyTorch resource like the one illustrated below. The custom controller takes care of spinning up and managing all of the individual processes and configuring them to talk to one another: . apiVersion: kubeflow.org/v1 kind: TFJob metadata: name: mnist-train spec: tfReplicaSpecs: Chief: replicas: 1 spec: containers: image: gcr.io/alice-dev/fairing-job/mnist name: tensorflow Ps: replicas: 1 template: spec: containers: image: gcr.io/alice-dev/fairing-job/mnist name: tensorflow Worker: replicas: 10 spec: containers: image: gcr.io/alice-dev/fairing-job/mnist name: tensorflow . Monitoring Model Training With TensorBoard . To train high quality models, data scientists need to debug and monitor the training process with tools like Tensorboard. With Kubernetes and Kubeflow, userscan easily deploy TensorBoard on their Kubernetes cluster by creating YAML files like the ones below. When deploying TensorBoard on Kubeflow, users can take advantage of Kubeflow’s AuthN and AuthZ integration to securely access TensorBoard behind Kubeflow’s ingress on public clouds: . // On GCP: [https://${KFNAME}.endpoints.${PROJECT}.cloud.goog/mnist/kubeflow-mnist/tensorboard/](https://${KFNAME}.endpoints.${PROJECT}.cloud.goog/mnist/kubeflow-mnist/tensorboard/) // On AWS: [http://8fb34ebe-istiosystem-istio-2af2-925939634.us-west-2.elb.amazonaws.com/mnist/anonymous/tensorboard/](http://8fb34ebe-istiosystem-istio-2af2-925939634.us-west-2.elb.amazonaws.com/mnist/anonymous/tensorboard/) . No need to kubectl port-forward to individual pods. . Deploying Models . KFServing is a custom resource built on top of Knative for deploying and managing ML models. KFServing offers the following capabilities not provided by lower level primitives (e.g. Deployment): . Deploy your model using out-of-the-box model servers (no need to write your own flask app) . | Auto-scaling based on load, even for models served on GPUs . | Safe, controlled model rollout . | Explainability (alpha) . | Payload logging (alpha) . | Below is an example of a KFServing spec showing how a model can be deployed. All a user has to do is provide the URI of their model file using storageUri: . apiVersion: &quot;serving.kubeflow.org/v1alpha2&quot; kind: &quot;InferenceService&quot; metadata: name: &quot;sklearn-iris&quot; spec: default: predictor: sklearn: storageUri: &quot;gs://kfserving-samples/models/sklearn/iris&quot; . Check out the samples to learn how to use the above capabilities. . Solutions are More Than Models . A model gathering dust in object storage isn’t doing your organization any good. To put ML to work, you typically need to incorporate that model into an application – whether it’s a web application, mobile app, or part of some backend reporting pipeline. . Frameworks like flask and bootstrap make it easy for data scientists to create rich, visually appealing web applications that put their models to work. Below is a screenshot of the UI we built for Kubeflow’s mnist example. . With Kubeflow, there is no need for data scientists to learn new concepts or platforms to deploy their applications, or to deal with ingress, networking certificates, etc. They can deploy their application just like TensorBoard; the only thing that changes is the Docker image and flags. . . If this sounds like just what you are looking for we recommend: . Visiting our docs to learn how to deploy Kubeflow on your public or private cloud. . | Walking through the mnist tutorial to try our core applications yourself. . | What’s coming in Kubeflow . There’s much more to Kubeflow than what we’ve covered in this blog post. In addition to the applications listed here, we have a number of applications under development: . Pipelines (beta) for defining complex ML workflows . | Metadata (beta) for tracking datasets, jobs, and models, . | Katib (beta) for hyper-parameter tuning . | Distributed operators for other frameworks like xgboost . | . In future releases we will be graduating these applications to 1.0. . User testimonials . All this would be nothing without feedback from and collaboration with our users. Some feedback from people using Kubeflow in production include: . *“The Kubeflow 1.0 release is a significant milestone as it positions Kubeflow to be a viable ML Enterprise platform. Kubeflow 1.0 delivers material productivity enhancements for ML researchers.” — *Jeff Fogarty, AVP ML / Cloud Engineer, US Bank . *“Kubeflow’s data and model storage allows for smooth integration into CI/CD processes, allowing for a much faster and more agile delivery of machine learning models into applications.” — *Laura Schornack, **Shared Services Architect, Chase Commercial Bank** . *“With the launch of Kubeflow 1.0 we now have a feature complete end-to-end open source machine learning platform, allowing everyone from small teams to large unicorns like Gojek to run ML at scale.” — *Willem Pienaar, Engineering Lead, Data Science Platform, GoJek . *“Kubeflow provides a seamless interface to a great set of tools that together manages the complexity of ML workflows and encourages best practices. The Data Science and Machine Learning teams at Volvo Cars are able to iterate and deliver reproducible, production grade services with ease.”— *Leonard Aukea, Volvo Cars . “With Kubeflow at the heart of our ML platform, our small company has been able to stack models in production to improve CR, find new customers, and present the right product to the right customer at the right time.” *— *Senior Director, One Technologies . “Kubeflow is helping GroupBy in standardizing ML workflows and simplifying very complicated deployments!” *— *Mohamed Elsaied, Machine Learning Team Lead, GroupBy . Thank You! . None of this would have been possible without the tens of organizations and hundreds of individuals that have been developing, testing, and evangelizing Kubeflow. . . An Open Community . We could not have achieved our milestone without an incredibly active community. Please come aboard! . Join the Kubeflow Slack channel . | Join the kubeflow-discuss mailing list . | Attend a weekly community meeting . | If you have questions, run into issues, please leverage the Slack channel and/or submit bugs via Kubeflow on GitHub. . | . Thank you all so much — onward! .",
            "url": "https://blog.kubeflow.org/releases/2020/03/02/kubeflow-1-0-cloud-native-ml-for-everyone.html",
            "relUrl": "/releases/2020/03/02/kubeflow-1-0-cloud-native-ml-for-everyone.html",
            "date": " • Mar 2, 2020"
        }
        
    
  
    
        ,"post35": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://blog.kubeflow.org/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post36": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://blog.kubeflow.org/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "This blog is hosted on GitHub Pages, via the kubeflow/blog repo. Instructions on contributing to this blog can be found there. . This website is powered by fastpages. .",
          "url": "https://blog.kubeflow.org/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://blog.kubeflow.org/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}