<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Elastic Training with MPI Operator and Practice | Kubeflow</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Elastic Training with MPI Operator and Practice" />
<meta name="author" content="<a href='https://www.linkedin.com/in/gaocegege/'>Ce Gao</a>, <a href='https://www.linkedin.com/in/wang-zhang'>Wang Zhang</a>" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="With increase in the size of dataset and deep learning models, distributed training emerges as the mainstream approach for training neural network models in industry. While it is feasible now to launch a massive distributed training job on Kubernetes with Kubeflow, advanced features like elastic workload and other cost mitigation approaches remain leashed when we talk about deep learning jobs on Kubernetes." />
<meta property="og:description" content="With increase in the size of dataset and deep learning models, distributed training emerges as the mainstream approach for training neural network models in industry. While it is feasible now to launch a massive distributed training job on Kubernetes with Kubeflow, advanced features like elastic workload and other cost mitigation approaches remain leashed when we talk about deep learning jobs on Kubernetes." />
<link rel="canonical" href="https://blog.kubeflow.org/elastic%20training/operators/2021/03/15/elastic-training.html" />
<meta property="og:url" content="https://blog.kubeflow.org/elastic%20training/operators/2021/03/15/elastic-training.html" />
<meta property="og:site_name" content="Kubeflow" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-03-15T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://blog.kubeflow.org/elastic%20training/operators/2021/03/15/elastic-training.html","@type":"BlogPosting","headline":"Elastic Training with MPI Operator and Practice","dateModified":"2021-03-15T00:00:00-05:00","datePublished":"2021-03-15T00:00:00-05:00","author":{"@type":"Person","name":"<a href='https://www.linkedin.com/in/gaocegege/'>Ce Gao</a>, <a href='https://www.linkedin.com/in/wang-zhang'>Wang Zhang</a>"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.kubeflow.org/elastic%20training/operators/2021/03/15/elastic-training.html"},"description":"With increase in the size of dataset and deep learning models, distributed training emerges as the mainstream approach for training neural network models in industry. While it is feasible now to launch a massive distributed training job on Kubernetes with Kubeflow, advanced features like elastic workload and other cost mitigation approaches remain leashed when we talk about deep learning jobs on Kubernetes.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://blog.kubeflow.org/feed.xml" title="Kubeflow" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-135379910-2','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="apple-touch-icon" sizes="180x180" href="/images/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/images/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/images/favicons/favicon-16x16.png">
<link rel="shortcut icon" type="image/x-icon" href="/images/favicons/favicon.ico">
<link rel="manifest" href="/images/favicons/site.webmanifest">
<link rel="mask-icon" href="/images/favicons/safari-pinned-tab.svg" color="#ffffff">
<meta name="msapplication-config" content="/images/favicons/browserconfig.xml" />
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="theme-color" content="#ffffff"><link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css">

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Kubeflow</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Elastic Training with MPI Operator and Practice</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-03-15T00:00:00-05:00" itemprop="datePublished">
        Mar 15, 2021
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name"><a href='https://www.linkedin.com/in/gaocegege/'>Ce Gao</a>, <a href='https://www.linkedin.com/in/wang-zhang'>Wang Zhang</a></span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      9 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#elastic training">elastic training</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#operators">operators</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#background">Background</a></li>
<li class="toc-entry toc-h2"><a href="#elastic-training">Elastic Training</a></li>
<li class="toc-entry toc-h2"><a href="#elastic-horovod">Elastic Horovod</a></li>
<li class="toc-entry toc-h2"><a href="#elastic-horovod-on-kubernetes">Elastic Horovod on Kubernetes</a></li>
<li class="toc-entry toc-h2"><a href="#demo">Demo</a></li>
<li class="toc-entry toc-h2"><a href="#conclusion">Conclusion</a></li>
</ul><p>With increase in the size of dataset and deep learning models, distributed training emerges as the mainstream approach for training neural network models in industry. While it is feasible now to launch a massive distributed training job on Kubernetes with Kubeflow, advanced features like elastic workload and other cost mitigation approaches remain leashed when we talk about deep learning jobs on Kubernetes.</p>

<p>To address issues on cost and resource utilization, the <a href="https://intl.cloud.tencent.com/product/tke">TKE (Tencent Kubernetes Engine)</a> AI team designs and implements <strong>Elastic Training</strong> in Kubeflow community.</p>

<p>Here we present how the elastic training is performed on Kubernetes. Validated with experiments under circumstances, elastic training lowers cost for distributed training on cloud.</p>

<h2 id="background">
<a class="anchor" href="#background" aria-hidden="true"><span class="octicon octicon-link"></span></a>Background</h2>

<p>Let’s first recap training deep learning models. When we talk about ‘training’, it refers generally to iteratively optimizing parameters in a neural network model with its gradient descent. Accelerated with GPUs, the training can speed up for 10-100 times.</p>

<p>When manufacturers try to integrate more computational resources like GPUs into a single machine, to hold training experiments with more and more data or model parameters, the cost grows exponentially. Therefore, after initially proposed by <a href="https://www.usenix.org/conference/osdi14/technical-sessions/presentation/li_mu">Mu Li on OSDI’14</a>, distributed training takes over training on a single machine when researchers play with massive dataset or large model.</p>

<p>For distributed training in data-parallelism, <a href="https://github.com/horovod/horovod">Horovod</a> is widely adopted given its excellent support on deep learning frameworks like <a href="https://www.tensorflow.org">TensorFlow</a> and <a href="https://pytorch.org">PyTorch</a>, communication optimization and easier programming pattern. In Horovod, all training processes are equal participants, each of which process the gradient calculation and communication.</p>

<p><img src="/images/2021-03-15-elastic-training/horovod-allreduce.png" width="" alt="alt_text" title=""></p>

<p>Because of significant acceleration of training speed as well as the programming pattern that are easier to understand, data-parallelism distributed training, represented by Horovod, is getting more and more attention. However, there still remain some issues:</p>

<ul>
  <li>The cost of training on the cloud is still the hurdle. While researchers no longer face the complexity when training on cloud thanks to Kubernetes and Kubeflow, the cost of training on cloud quells some users.</li>
  <li>Compared with training on a single machine, multi-node distributed training accumulates the probability of training failure. The entire training experiment fails when any of its training process issues an error. This problem becomes even severer when some experiments take days or even weeks.</li>
  <li>When collocating training tasks with other workloads (with higher priority), the resources demand fluctuates as the request for these other workloads may change periodically. This unbalance of resources availability throws cold water on the idea of using hybrid-deployment to maximize resource utilization.</li>
</ul>

<h2 id="elastic-training">
<a class="anchor" href="#elastic-training" aria-hidden="true"><span class="octicon octicon-link"></span></a>Elastic Training</h2>

<p>Researchers and engineers proposed <strong>Elastic Training</strong> as the key to solve the puzzle.</p>

<p>Traditionally, the resource configuration for a distributed training job is fixed. Elastic training breaks this rule and enables users to change the number of instances participating in a distributed training job, bringing the following benefits to clusters with distributed training jobs:</p>

<ul>
  <li>Fault Tolerance: any worker instance can fail as long as at least one is surviving.</li>
  <li>Resources Utilization: when the resources stress piles, the cluster is able to reduce the replicas of workloads with lower priority (distributed training workloads), releasing resource to other workloads (such as prediction service), ensuring SLA for business; after resources released from workloads, elastic training job is able to absorb these resource by scaling up workload replicas.</li>
  <li>Training on Cloud: there is a type of resource on the cloud that is called “spot” or “preemptible” instances; it comes with unexpected low price tags but may be retrieved after guaranteed hour expires.</li>
</ul>

<p>Elastic training appears a perfect match to public cloud. Combined with spot instances, we cut the cost for GPUs from ¥16.21/hour to ¥1.62/hour, reducing the overall cost for the training job by nearly 70%. Under the same budget, elastic training employs more GPUs and accelerates the training speed by 5 to 10 times.</p>

<h2 id="elastic-horovod">
<a class="anchor" href="#elastic-horovod" aria-hidden="true"><span class="octicon octicon-link"></span></a>Elastic Horovod</h2>

<p>As the major player in distributed training framework, Horovod v0.20.0 offers its solution to elastic training, <a href="https://horovod.readthedocs.io/en/stable/elastic_include.html">Elastic Horovod</a>. Here we quotes the architecture differences between Elastic Horovod and existing Horovod from <a href="https://docs.google.com/document/d/15ZoHA5AeSI_boeyIBapg9WPXKrYXMRvPytPzQWTCTn4/edit#">RFC Elastic Horovod</a>:</p>

<p><img src="/images/2021-03-15-elastic-training/horovod-elastic.png" width="" alt="alt_text" title=""></p>

<ul>
  <li>All collective operations are coordinated within a hvd.elastic.run function.</li>
  <li>State is synchronized between workers before the user’s training function is executed.</li>
  <li>Worker failure or worker added events will result in a reset event on other workers.</li>
  <li>Reset events act as barriers to:
    <ul>
      <li>Determine whether the job should continue based on worker exit codes.</li>
      <li>Blacklist failing hosts.</li>
      <li>Launch workers on new hosts.</li>
      <li>Update the rank information on existing workers.</li>
    </ul>
  </li>
  <li>State is synchronized following a reset event.</li>
</ul>

<p>When launching an elastic training job, <code class="language-plaintext highlighter-rouge">horovodrun</code> requires a <code class="language-plaintext highlighter-rouge">discover_hosts.sh</code> script to detect available hosts and slots in real time. In the following section, we refer this script as <code class="language-plaintext highlighter-rouge">discover_hosts.sh</code>. Nevertheless the script needs not to be named as <code class="language-plaintext highlighter-rouge">discover_hosts.sh</code>. An example of <code class="language-plaintext highlighter-rouge">discover_hosts.sh</code> can be found <a href="https://horovod.readthedocs.io/en/stable/elastic_include.html#running-with-horovodrun">here</a>.</p>

<h2 id="elastic-horovod-on-kubernetes">
<a class="anchor" href="#elastic-horovod-on-kubernetes" aria-hidden="true"><span class="octicon octicon-link"></span></a>Elastic Horovod on Kubernetes</h2>

<p><a href="https://github.com/kubeflow/mpi-operator">MPI-Operator</a> is designed to deploy Horovod jobs on Kubernetes. While the operator releases multiple versions, the general idea stays unchanged. It includes:</p>

<p><img src="/images/2021-03-15-elastic-training/mpi-operator.png" width="" alt="alt_text" title=""></p>

<ul>
  <li>MPIJob Controller creates a launcher pod and worker pods according to the replicas configuration in MPIJobs</li>
  <li>For each MPIJob, the controller creates a <a href="https://kubernetes.io/docs/concepts/configuration/configmap/">ConfigMap</a>, which delivers two files: <code class="language-plaintext highlighter-rouge">hostfile</code> and <code class="language-plaintext highlighter-rouge">kubexec.sh</code>
</li>
  <li>With all worker pods ready, <code class="language-plaintext highlighter-rouge">mpirun</code> on launcher pod (granted with <code class="language-plaintext highlighter-rouge">pod/exec</code> permission) uses <code class="language-plaintext highlighter-rouge">kubexec.sh</code> to launch processes on worker pods</li>
</ul>

<p>Launching an Elastic Horovod job is not feasible as there exist several incompatibilities between Elastic Horovod and MPIJob Controller. We take controller-v1 as the example:</p>

<ul>
  <li>No built-in <code class="language-plaintext highlighter-rouge">discover_hosts.sh</code> available on launcher pod</li>
  <li>After worker replica number is turned down, worker pods that are no longer wanted will not be deleted by the controller, leaving the size of the distributed training unchanged</li>
  <li>After worker replica number is turned up, the controller does not update rule in the <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/">Role</a> binded to the launcher pod, preventing the launcher pod from creating processes on newly created pods</li>
</ul>

<p>To address these compatibility issues, we pushed multiple pull requests regarding Horovod and MPI-Operator, including <a href="https://github.com/kubeflow/mpi-operator/pull/335">mpi-operator-pull-335</a> and <a href="https://github.com/horovod/horovod/pull/2199">horovod-pull-2199</a>. As providing an MPI-Operator-specific <code class="language-plaintext highlighter-rouge">discover_hosts.sh</code> is most critical to the launcher pod for Elastic Horovod, we consider two scenarios for converting worker pods with a <code class="language-plaintext highlighter-rouge">Running</code> phase into a <code class="language-plaintext highlighter-rouge">discover_hosts.sh</code> script.</p>

<ul>
  <li>A <strong>dynamic</strong> <code class="language-plaintext highlighter-rouge">discover_hosts.sh</code> composed by MPIJob controller and synchronized to the launcher pod via ConfigMap
    <ul>
      <li>MPIJob controller has a podLister, which can be used to list worker pods readily</li>
      <li>the controller filters worker pods with <code class="language-plaintext highlighter-rouge">status.phase == Running</code> and encode the result into the <code class="language-plaintext highlighter-rouge">discover_hosts.sh</code>
</li>
      <li>the ConfigMap is updated when <code class="language-plaintext highlighter-rouge">discover_hosts.sh</code> is modified and the change will be propagated to the launcher pod by Kubernetes</li>
    </ul>
  </li>
  <li>A <strong>static</strong> <code class="language-plaintext highlighter-rouge">discover_hosts.sh</code> using <code class="language-plaintext highlighter-rouge">kubectl</code> in the launcher pod to list all running worker pods from <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/">APIServer</a>
</li>
</ul>

<p>Scenario 2 changes the delivery image instead of the controller. However, as we cannot limit how frequently users will execute the <code class="language-plaintext highlighter-rouge">discover_hosts.sh</code> script, it poses a potential threat to the APIServer, especially when the count of worker pods is massive.</p>

<p>An fixture to scenario 2 is to replace the <code class="language-plaintext highlighter-rouge">kubectl</code> with a podLister process, removing extra stress from the APIServer. In this way, we install two processes in launcher pod but lack a proper mechanism to keep the podLister alive. Once the podLister dies, there leaves no elasticity for the training job.</p>

<p>Therefore we choose the first scenario and map the <code class="language-plaintext highlighter-rouge">disocver_hosts.sh</code> under <code class="language-plaintext highlighter-rouge">/etc/mpi/</code>. We also fixed the other compatibility issues after the worker replica configuration changes. For users choose non-elastic mode, just simply ignore <code class="language-plaintext highlighter-rouge">/etc/mpi/discover_hosts.sh</code>.</p>

<p>Concerns comes to scenario 1 as well. There is a delay between the ConfigMap and what <code class="language-plaintext highlighter-rouge">horovodrun</code> sees from the <code class="language-plaintext highlighter-rouge">discover_hosts.sh</code> in the launcher pod. This delay, on one hand, can be tweaked by cluster admin and on the other hand, can be considered as tiny compared to the training elapsed time or the time for Elastic Horovod to handle worker changes.</p>

<h2 id="demo">
<a class="anchor" href="#demo" aria-hidden="true"><span class="octicon octicon-link"></span></a>Demo</h2>

<p>We present a demo to show how to operate an Elastic Horovod job with MPI Operator.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bash-5.0<span class="nv">$ </span>kubectl create <span class="nt">-f</span> ./tensorflow-mnist-elastic.yaml
mpijob.kubeflow.org/tensorflow-mnist-elastic 
createdbash-5.0<span class="nv">$ </span>kubectl get po
NAME    READY   STATUS    RESTARTS  AGE
tensorflow-mnist-elastic-launcher   1/1     Running   0          14s
tensorflow-mnist-elastic-worker-0   1/1     Running   0          14s
tensorflow-mnist-elastic-worker-1   1/1     Running   0          14s
</code></pre></div></div>

<p>The job is created with two workers. After the training begins, we change <code class="language-plaintext highlighter-rouge">MPIJob.Spec.MPIReplicaSpecs["Worker"].Replicas</code> to <strong>3</strong>, adding another worker. Let’s check how the <code class="language-plaintext highlighter-rouge">discover_hosts.sh</code> changes:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bash-5.0<span class="nv">$ </span>kubectl <span class="nb">exec </span>tensorflow-mnist-elastic-launcher <span class="nt">--</span> /etc/mpi/discover_hosts.sh
tensorflow-mnist-elastic-worker-0:1
tensorflow-mnist-elastic-worker-1:1
bash-5.0<span class="nv">$ </span><span class="nb">cat</span> ./patch_r3.yaml
spec:
  mpiReplicaSpecs:
    <span class="s2">"Worker"</span>:
      replicas: 3
bash-5.0<span class="nv">$ </span>kubectl patch mpijob tensorflow-mnist-elastic <span class="nt">--patch</span> <span class="s2">"</span><span class="si">$(</span><span class="nb">cat </span>patch_r3.yaml<span class="si">)</span><span class="s2">"</span> <span class="nt">--type</span><span class="o">=</span>merge
mpijob.kubeflow.org/tensorflow-mnist-elastic patched
bash-5.0<span class="nv">$ </span>kubectl <span class="nb">exec </span>tensorflow-mnist-elastic-launcher <span class="nt">--</span> /etc/mpi/discover_hosts.sh
tensorflow-mnist-elastic-worker-0:1
tensorflow-mnist-elastic-worker-1:1
tensorflow-mnist-elastic-worker-2:1
</code></pre></div></div>

<p>We reduce the replica count to 1, retrieving 2 worker instances.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bash-5.0<span class="nv">$ </span><span class="nb">cat</span> ./patch_r1.yaml
spec:
  mpiReplicaSpecs:
    <span class="s2">"Worker"</span>:
      replicas: 1
bash-5.0<span class="nv">$ </span>kubectl patch mpijob tensorflow-mnist-elastic <span class="nt">--patch</span> <span class="s2">"</span><span class="si">$(</span><span class="nb">cat </span>patch_r1.yaml<span class="si">)</span><span class="s2">"</span> <span class="nt">--type</span><span class="o">=</span>merge
mpijob.kubeflow.org/tensorflow-mnist-elastic patched
bash-5.0<span class="nv">$ </span>kubectl get po
NAME               READY   STATUS        RESTARTS   AGE
tensorflow-mnist-elastic-launcher   1/1     Running       0          4m48s
tensorflow-mnist-elastic-worker-0   1/1     Running       0          4m48s
tensorflow-mnist-elastic-worker-1   1/1     Terminating   0          4m48s
tensorflow-mnist-elastic-worker-2   1/1     Terminating   0          2m21s
</code></pre></div></div>

<p>The elastic training persists.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Thu Mar 11 01:53:18 2021[1]&lt;stdout&gt;:Step <span class="c">#40    Loss: 0.284265</span>
Thu Mar 11 01:53:18 2021[0]&lt;stdout&gt;:Step <span class="c">#40    Loss: 0.259497</span>
Thu Mar 11 01:53:18 2021[2]&lt;stdout&gt;:Step <span class="c">#40    Loss: 0.229993</span>
Thu Mar 11 01:54:27 2021[2]&lt;stderr&gt;:command terminated with <span class="nb">exit </span>code 137
Process 2 <span class="nb">exit </span>with status code 137.
Thu Mar 11 01:54:27 2021[0]&lt;stderr&gt;:command terminated with <span class="nb">exit </span>code 137
Process 0 <span class="nb">exit </span>with status code 137.
Thu Mar 11 01:54:57 2021[1]&lt;stderr&gt;:[2021-03-11 01:54:57.532928: E /tmp/pip-install-2jy0u7mn/horovod/horovod/common/operations.cc:525] Horovod background loop uncaught exception: <span class="o">[</span>/tmp/pip-install-2jy0u7mn/horovod/third_party/compatible_gloo/gloo/transport/tcp/pair.cc:575] Connection closed by peer <span class="o">[</span>10.244.2.27]:54432
WARNING:root:blacklist failing host: tensorflow-mnist-elastic-worker-2
WARNING:root:blacklist failing host: tensorflow-mnist-elastic-worker-1
Thu Mar 11 01:54:58 2021[1]&lt;stdout&gt;:Step <span class="c">#50    Loss: 0.207741</span>
Thu Mar 11 01:55:00 2021[1]&lt;stdout&gt;:Step <span class="c">#60    Loss: 0.119361</span>
Thu Mar 11 01:55:02 2021[1]&lt;stdout&gt;:Step <span class="c">#70    Loss: 0.131966</span>
</code></pre></div></div>

<p>As we can see, Elastic Horovod on MPI-Operator now supports tweaking worker replicas dynamically. As a future work, we aim to support  <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"><code class="language-plaintext highlighter-rouge">Horizontal Pod Autoscaler</code></a> to MPIJob as well as other features like designated worker deletion.</p>

<h2 id="conclusion">
<a class="anchor" href="#conclusion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion</h2>

<p>When the concept of cloud native and distributed training fuse to elastic training on Kubernetes, it lowers the cost and gives robustness and flexibility. As a team, we are working with PyTorch, Horovod and other communities to propel elastic training. We wish to further introduce our work on elasticity with PS/Worker training mode, optimization for resource and job priority and other topics on cloud native AI.</p>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="kubeflow/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/elastic%20training/operators/2021/03/15/elastic-training.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>The Machine Learning Toolkit for Kubernetes.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/kubeflow" title="kubeflow"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/kubeflow" title="kubeflow"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
