<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://blog.kubeflow.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://blog.kubeflow.org/" rel="alternate" type="text/html" /><updated>2025-08-08T15:41:05-05:00</updated><id>https://blog.kubeflow.org/feed.xml</id><title type="html">Kubeflow</title><subtitle>The Machine Learning Toolkit for Kubernetes.</subtitle><entry><title type="html">Democratizing AI Model Training on Kubernetes: Introducing Kubeflow Trainer V2</title><link href="https://blog.kubeflow.org/trainer/intro/" rel="alternate" type="text/html" title="Democratizing AI Model Training on Kubernetes: Introducing Kubeflow Trainer V2" /><published>2025-07-21T00:00:00-05:00</published><updated>2025-07-21T00:00:00-05:00</updated><id>https://blog.kubeflow.org/trainer/introducing-trainer-v2</id><content type="html" xml:base="https://blog.kubeflow.org/trainer/intro/">&lt;p&gt;Running machine learning workloads on Kubernetes can be challenging.
Distributed training and LLMs fine-tuning, in particular, involves managing multiple nodes, GPUs, large datasets, and fault tolerance, which often requires deep Kubernetes knowledge.
The &lt;strong&gt;Kubeflow Trainer v2 (KF Trainer)&lt;/strong&gt; was created to hide this complexity, by abstracting Kubernetes from AI Practitioners and providing the easiest, most scalable way to run distributed PyTorch jobs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The main goals of Kubeflow Trainer v2 include:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Make AI/ML workloads easier to manage at scale&lt;/li&gt;
  &lt;li&gt;Provide a Pythonic interface to train models&lt;/li&gt;
  &lt;li&gt;Deliver the easiest and most scalable PyTorch distributed training on Kubernetes&lt;/li&gt;
  &lt;li&gt;Add built-in support for fine-tuning large language models&lt;/li&gt;
  &lt;li&gt;Abstract Kubernetes complexity from AI Practitioners&lt;/li&gt;
  &lt;li&gt;Consolidate efforts between Kubernetes Batch WG and Kubeflow community&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We’re deeply grateful to all contributors and community members who made the &lt;strong&gt;Trainer v2&lt;/strong&gt; possible with their hard work and valuable feedback.
We’d like to give special recognition to &lt;a href=&quot;https://github.com/andreyvelich&quot;&gt;andreyvelich&lt;/a&gt;, &lt;a href=&quot;https://github.com/tenzen-y&quot;&gt;tenzen-y&lt;/a&gt;, &lt;a href=&quot;https://github.com/electronic-waste&quot;&gt;electronic-waste&lt;/a&gt;, &lt;a href=&quot;https://github.com/astefanutti&quot;&gt;astefanutti&lt;/a&gt;, &lt;a href=&quot;https://github.com/ironicbo&quot;&gt;ironicbo&lt;/a&gt;, &lt;a href=&quot;https://github.com/mahdikhashan&quot;&gt;mahdikhashan&lt;/a&gt;, &lt;a href=&quot;https://github.com/kramaranya&quot;&gt;kramaranya&lt;/a&gt;, &lt;a href=&quot;https://github.com/harshal292004&quot;&gt;harshal292004&lt;/a&gt;, &lt;a href=&quot;https://github.com/akshaychitneni&quot;&gt;akshaychitneni&lt;/a&gt;, &lt;a href=&quot;https://github.com/chenyi015&quot;&gt;chenyi015&lt;/a&gt; and the rest of the contributors.
We would also like to highlight &lt;a href=&quot;https://github.com/ahg-g&quot;&gt;ahg-g&lt;/a&gt;, &lt;a href=&quot;https://github.com/kannon92&quot;&gt;kannon92&lt;/a&gt;, and &lt;a href=&quot;https://github.com/vsoch&quot;&gt;vsoch&lt;/a&gt; whose feedback was essential while we designed the Kubeflow Trainer architecture together with the Batch WG.
See the full &lt;a href=&quot;https://kubeflow.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;amp;var-period_name=Last%206%20months&amp;amp;var-metric=commits&amp;amp;var-repogroup_name=kubeflow%2Ftrainer&amp;amp;var-country_name=All&amp;amp;var-companies=All&quot;&gt;contributor list&lt;/a&gt; for everyone who helped make this release possible.&lt;/p&gt;

&lt;h1 id=&quot;background-and-evolution&quot;&gt;Background and Evolution&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Kubeflow Trainer v2&lt;/strong&gt; represents the next evolution of the &lt;strong&gt;Kubeflow Training Operator&lt;/strong&gt;, building on over seven years of experience running ML workloads on Kubernetes.
The journey began in 2017 when the &lt;strong&gt;Kubeflow&lt;/strong&gt; project introduced &lt;strong&gt;TFJob&lt;/strong&gt; to orchestrate TensorFlow training on Kubernetes.
At that time, Kubernetes lacked many of the advanced batch processing features needed for distributed ML training, so the community had to implement these capabilities from scratch.&lt;/p&gt;

&lt;p&gt;Over the years, the project expanded to support multiple ML frameworks including &lt;strong&gt;PyTorch&lt;/strong&gt;, &lt;strong&gt;MXNet&lt;/strong&gt;, &lt;strong&gt;MPI&lt;/strong&gt;, and &lt;strong&gt;XGBoost&lt;/strong&gt; through various specialized operators.
In 2021, these were consolidated into the unified &lt;strong&gt;&lt;a href=&quot;https://docs.google.com/document/d/1x1JPDQfDMIbnoQRftDH1IzGU0qvHGSU4W6Jl4rJLPhI/edit?tab=t.0#heading=h.e33ufidnl8z6&quot;&gt;Training Operator v1&lt;/a&gt;&lt;/strong&gt;.
Meanwhile, the Kubernetes community introduced the &lt;strong&gt;Batch Working Group&lt;/strong&gt;, developing important APIs like JobSet, Kueue, Indexed Jobs, and PodFailurePolicy that improved HPC and AI workload management.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Trainer v2&lt;/strong&gt; leverages these Kubernetes-native improvements to make use of existing functionality and not reinvent the wheel.
This collaboration between the Kubernetes and Kubeflow communities delivers a more standardized approach to ML training on Kubernetes.&lt;/p&gt;

&lt;h1 id=&quot;user-personas&quot;&gt;User Personas&lt;/h1&gt;

&lt;p&gt;One of the main challenges with ML training on Kubernetes is that it often requires &lt;strong&gt;AI Practitioners&lt;/strong&gt; to have an understanding of &lt;strong&gt;Kubernetes concepts&lt;/strong&gt; and the &lt;strong&gt;infrastructure&lt;/strong&gt; being used for training. This distracts AI Practitioners from their primary focus.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The KF Trainer v2&lt;/strong&gt; addresses this by &lt;strong&gt;separating the infrastructure configuration from the training job definition&lt;/strong&gt;.
This separation is built around three new custom resources definitions (CRDs):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TrainingRuntime&lt;/code&gt; - a namespace-scoped resource that contains the infrastructure details that are required for a training job, such as the training image to use, failure policy, and gang-scheduling configuration.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ClusterTrainingRuntime&lt;/code&gt; - similar to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TrainingRuntime&lt;/code&gt;, but cluster scoped.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TrainJob&lt;/code&gt; - specifies the training job configuration, including the training code to run, config for pulling the training dataset &amp;amp; model, and a reference to the training runtime.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The diagram below shows how different personas interact with these custom resources:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2025-07-21-introducing-trainer-v2/user-personas.drawio.svg&quot; alt=&quot;user_personas&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Platform Administrators&lt;/strong&gt; define and manage &lt;strong&gt;the infrastructure configurations&lt;/strong&gt; required for training jobs using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TrainingRuntimes&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ClusterTrainingRuntimes&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;AI Practitioners&lt;/strong&gt; focus on model development using the simplified &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TrainJob&lt;/code&gt; resource or &lt;strong&gt;Python SDK&lt;/strong&gt; wrapper, providing a reference to &lt;strong&gt;the training runtime&lt;/strong&gt; created by &lt;strong&gt;Platform Administrators&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;python-sdk&quot;&gt;Python SDK&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;The KF Trainer v2&lt;/strong&gt; introduces a &lt;strong&gt;redesigned Python SDK&lt;/strong&gt;, which is intended to be the &lt;strong&gt;primary interface for AI Practitioners&lt;/strong&gt;.
The SDK provides a unified interface across multiple ML frameworks and cloud environments, abstracting away the underlying Kubernetes complexity.&lt;/p&gt;

&lt;p&gt;The diagram below illustrates how Kubeflow Trainer provides a consistent experience for running ML jobs across different ML frameworks, Kubernetes infrastructures, and cloud providers:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2025-07-21-introducing-trainer-v2/trainerv2.png&quot; alt=&quot;trainerv2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kubeflow Trainer v2&lt;/strong&gt; supports multiple ML frameworks through &lt;strong&gt;pre-configured runtimes&lt;/strong&gt;. The table below shows the current framework support:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2025-07-21-introducing-trainer-v2/runtimes.png&quot; alt=&quot;runtimes&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The SDK makes it easier for users familiar with Python to &lt;strong&gt;create, manage, and monitor training jobs&lt;/strong&gt;, without requiring them to deal with any YAML definitions:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from kubeflow.trainer import TrainerClient

client = TrainerClient()

def my_train_func():
    &quot;&quot;&quot;User defined function that runs on each distributed node process&quot;&quot;&quot;
    import os
    import torch
    import torch.distributed as dist
    from torch.utils.data import DataLoader, DistributedSampler
    
    # Setup PyTorch distributed
    backend = &quot;nccl&quot; if torch.cuda.is_available() else &quot;gloo&quot;
    local_rank = int(os.getenv(&quot;LOCAL_RANK&quot;, 0))
    dist.init_process_group(backend=backend)
    
    # Define your model, dataset, and training loop
    model = YourModel()
    dataset = YourDataset()
    train_loader = DataLoader(dataset, sampler=DistributedSampler(dataset))
    
    # Your training logic here
    for epoch in range(num_epochs):
        for batch in train_loader:
            # Forward pass, backward pass, optimizer step
            ...
            
    # Wait for the distributed training to complete
    dist.barrier()
    if dist.get_rank() == 0:
        print(&quot;Training is finished&quot;)

    # Clean up PyTorch distributed
    dist.destroy_process_group()

job_name = client.train(
  runtime=client.get_runtime(&quot;torch-distributed&quot;),
  trainer=CustomTrainer(
    func=my_train_func,
    num_nodes=5,
    resources_per_node={
      &quot;gpu&quot;: 2,
     },
  ),
)

job = client.get_job(name=job_name)

for step in job.steps:
   print(f&quot;Step: {step.name}, Status: {step.status}&quot;)

client.get_job_logs(job_name, follow=True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The SDK handles all Kubernetes API interactions. This eliminates the need for AI Practitioners to directly interact with the Kubernetes API.&lt;/p&gt;

&lt;h1 id=&quot;simplified-api&quot;&gt;Simplified API&lt;/h1&gt;

&lt;p&gt;Previously, in the &lt;strong&gt;Kubeflow Training Operator&lt;/strong&gt; users worked with different custom resources for each ML framework, each with their own framework-specific configurations.
The &lt;strong&gt;KF Trainer v2&lt;/strong&gt; replaces these multiple CRDs with a &lt;strong&gt;unified TrainJob API&lt;/strong&gt; that works with &lt;strong&gt;multiple ML frameworks&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;For example, here’s how a &lt;strong&gt;PyTorch training job&lt;/strong&gt; looks like using &lt;strong&gt;KF Trainer v1&lt;/strong&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: kubeflow.org/v1
kind: PyTorchJob
metadata:
  name: pytorch-simple
  namespace: kubeflow
spec:
  pytorchReplicaSpecs:
    Master:
      replicas: 1
      restartPolicy: OnFailure
      template:
        spec:
          containers:
            - name: pytorch
              image: docker.io/kubeflowkatib/pytorch-mnist:v1beta1-45c5727
              imagePullPolicy: Always
              command:
                - &quot;python3&quot;
                - &quot;/opt/pytorch-mnist/mnist.py&quot;
                - &quot;--epochs=1&quot;
    Worker:
      replicas: 1
      restartPolicy: OnFailure
      template:
        spec:
          containers:
            - name: pytorch
              image: docker.io/kubeflowkatib/pytorch-mnist:v1beta1-45c5727
              imagePullPolicy: Always
              command:
                - &quot;python3&quot;
                - &quot;/opt/pytorch-mnist/mnist.py&quot;
                - &quot;--epochs=1&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the &lt;strong&gt;KF Trainer v2&lt;/strong&gt;, creating an equivalent job becomes much simpler:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: trainer.kubeflow.org/v1alpha1
kind: TrainJob
metadata:
  name: pytorch-simple
  namespace: kubeflow
spec:
  trainer:
    numNodes: 2
    image: docker.io/kubeflowkatib/pytorch-mnist:v1beta1-45c5727
    command:
      - &quot;python3&quot;
      - &quot;/opt/pytorch-mnist/mnist.py&quot;
      - &quot;--epochs=1&quot;
  runtimeRef:
    name: torch-distributed
    apiGroup: trainer.kubeflow.org
    kind: ClusterTrainingRuntime
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Additional &lt;strong&gt;infrastructure&lt;/strong&gt; and &lt;strong&gt;Kubernetes-specific&lt;/strong&gt; details are provided in the referenced &lt;strong&gt;runtime&lt;/strong&gt; definition, and managed separately by &lt;strong&gt;Platform Administrators&lt;/strong&gt;.
In the future, we might support other runtimes in addition to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TrainingRuntime&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ClusterTrainingRuntime&lt;/code&gt;, for example &lt;a href=&quot;https://github.com/kubeflow/trainer/issues/2249&quot;&gt;SlurmRuntime&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;extensibility-and-pipeline-framework&quot;&gt;Extensibility and Pipeline Framework&lt;/h1&gt;

&lt;p&gt;One of the challenges in &lt;strong&gt;KF Trainer v1&lt;/strong&gt; was supporting additional ML frameworks, especially for closed-sourced frameworks.
The v2 architecture addresses this by introducing a &lt;strong&gt;Pipeline Framework&lt;/strong&gt; that allows Platform Administrators  to &lt;strong&gt;extend the Plugins&lt;/strong&gt; and &lt;strong&gt;support orchestration&lt;/strong&gt; for their custom in-house ML frameworks.&lt;/p&gt;

&lt;p&gt;The diagram below shows Kubeflow Trainer Pipeline Framework overview:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2025-07-21-introducing-trainer-v2/trainer-pipeline-framework.drawio.svg&quot; alt=&quot;trainer_pipeline_framework&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The framework works through a series of phases - &lt;strong&gt;Startup&lt;/strong&gt;, &lt;strong&gt;PreExecution&lt;/strong&gt;, &lt;strong&gt;Build&lt;/strong&gt;, and &lt;strong&gt;PostExecution&lt;/strong&gt; - each with &lt;strong&gt;extension points&lt;/strong&gt; where custom Plugins can hook in.
This approach allows adding support for new frameworks, custom validation logic, or specialized training orchestration without changing the underlying system.&lt;/p&gt;

&lt;h1 id=&quot;llms-fine-tuning-support&quot;&gt;LLMs Fine-Tuning Support&lt;/h1&gt;

&lt;p&gt;Another improvement of &lt;strong&gt;Trainer v2&lt;/strong&gt; is its &lt;strong&gt;built-in support for fine-tuning large language models&lt;/strong&gt;, where we provide two types of trainers:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BuiltinTrainer&lt;/code&gt; - already includes the fine-tuning logic and allows AI Practitioners to quickly start fine-tuning requiring only parameter adjustments.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CustomTrainer&lt;/code&gt; - allows users to provide their own training function that encapsulates the entire LLMs fine-tuning.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the first release, we support &lt;strong&gt;TorchTune LLM Trainer&lt;/strong&gt; as the initial option for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BuiltinTrainer&lt;/code&gt;.
For TorchTune, we provide pre-configured runtimes (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ClusterTrainingRuntime&lt;/code&gt;) that currently support &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Llama-3.2-1B-Instruct&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Llama-3.2-3B-Instruct&lt;/code&gt; in the &lt;a href=&quot;https://github.com/kubeflow/trainer/tree/master/manifests/base/runtimes/torchtune/llama3_2&quot;&gt;manifest&lt;/a&gt;.
This approach means that in the future, we can add more frameworks, such as &lt;a href=&quot;https://github.com/unslothai/unsloth&quot;&gt;unsloth&lt;/a&gt;, as additional &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BuiltinTrainer&lt;/code&gt; options.
Here’s an example using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BuiltinTrainer&lt;/code&gt; with &lt;strong&gt;TorchTune&lt;/strong&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;job_name = client.train(
    runtime=Runtime(
        name=&quot;torchtune-llama3.2-1b&quot;
    ),
    initializer=Initializer(
        dataset=HuggingFaceDatasetInitializer(
            storage_uri=&quot;hf://tatsu-lab/alpaca/data&quot;
        ),
        model=HuggingFaceModelInitializer(
            storage_uri=&quot;hf://meta-llama/Llama-3.2-1B-Instruct&quot;,
            access_token=&quot;&amp;lt;YOUR_HF_TOKEN&amp;gt;&quot;  # Replace with your Hugging Face token,
        )
    ),
    trainer=BuiltinTrainer(
        config=TorchTuneConfig(
            dataset_preprocess_config=TorchTuneInstructDataset(
                source=DataFormat.PARQUET,
            ),
            resources_per_node={
                &quot;gpu&quot;: 1,
            }
        )
    )
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This example uses a &lt;strong&gt;builtin runtime image&lt;/strong&gt; that uses a foundation Llama model, and fine-tunes it using a dataset pulled from Hugging Face, with the TorchTune configuration provided by the AI Practitioner.
For more details, please refer to &lt;a href=&quot;https://github.com/kubeflow/trainer/blob/master/examples/torchtune/llama3_2/alpaca-trainjob-yaml.ipynb&quot;&gt;this example&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;dataset-and-model-initializers&quot;&gt;Dataset and Model Initializers&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Trainer v2&lt;/strong&gt; provides &lt;strong&gt;dedicated initializers&lt;/strong&gt; for datasets and models, which significantly simplify the setup process.
Instead of each training pod independently downloading large models and datasets, &lt;strong&gt;initializers handle this once&lt;/strong&gt; and &lt;strong&gt;share the data&lt;/strong&gt; across all training nodes through a &lt;strong&gt;shared volume&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;This approach saves both &lt;strong&gt;time and resources&lt;/strong&gt; by preventing network slowdowns, and &lt;strong&gt;reducing GPU waiting time&lt;/strong&gt; during setup by offloading data loading tasks to CPU-based initializers, which preserves expensive GPU resources for the actual training.&lt;/p&gt;

&lt;h1 id=&quot;use-of-jobset-api&quot;&gt;Use of JobSet API&lt;/h1&gt;

&lt;p&gt;Under the hood, the &lt;strong&gt;KF Trainer v2&lt;/strong&gt; uses &lt;strong&gt;&lt;a href=&quot;https://jobset.sigs.k8s.io/docs/overview/&quot;&gt;JobSet&lt;/a&gt;&lt;/strong&gt;, a &lt;strong&gt;Kubernetes-native API&lt;/strong&gt; for managing groups of jobs.
This integration allows the KF Trainer v2 to better utilize standard Kubernetes features instead of trying to recreate them.&lt;/p&gt;

&lt;h1 id=&quot;kueue-integration&quot;&gt;Kueue Integration&lt;/h1&gt;

&lt;p&gt;Resource management is improved through integration with &lt;strong&gt;&lt;a href=&quot;https://kueue.sigs.k8s.io/&quot;&gt;Kueue&lt;/a&gt;&lt;/strong&gt;, a &lt;strong&gt;Kubernetes-native queueing system&lt;/strong&gt;.
The KF Trainer v2 includes initial support for Kueue through Pod Integration, which allows individual training pods to be queued when resources are busy.
We are working on &lt;strong&gt;&lt;a href=&quot;https://github.com/kubernetes-sigs/kueue/issues/3884&quot;&gt;native Kueue support&lt;/a&gt;&lt;/strong&gt; for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TrainJob&lt;/code&gt; to provide richer queueing features in future releases.&lt;/p&gt;

&lt;h1 id=&quot;mpi-support&quot;&gt;MPI Support&lt;/h1&gt;

&lt;p&gt;The &lt;strong&gt;KF Trainer v2&lt;/strong&gt; also provides &lt;strong&gt;MPI v2 support&lt;/strong&gt;, which includes &lt;strong&gt;automatic generation of SSH keys&lt;/strong&gt; for secure inter-node communication and boosting performance MPI on Kubernetes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2025-07-21-introducing-trainer-v2/MPI-support.drawio.svg&quot; alt=&quot;MPI_support&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The diagram above shows how this works in practice - the &lt;strong&gt;KF Trainer&lt;/strong&gt; automatically &lt;strong&gt;handles the SSH key generation&lt;/strong&gt; and &lt;strong&gt;MPI communication&lt;/strong&gt; between training pods, which allows frameworks like &lt;a href=&quot;https://www.deepspeed.ai/&quot;&gt;DeepSpeed&lt;/a&gt; to coordinate training across multiple GPU nodes without requiring manual configuration of inter-node communication.&lt;/p&gt;

&lt;h1 id=&quot;gang-scheduling&quot;&gt;Gang-Scheduling&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Gang-scheduling&lt;/strong&gt; is an important feature for distributed training that ensures &lt;strong&gt;all pods in a training job are scheduled together&lt;/strong&gt; or not at all.
This prevents scenarios where only some pods are scheduled while others remain pending due to resource constraints, which would waste GPU resources and prevent training from starting.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The KF Trainer v2&lt;/strong&gt; provides &lt;strong&gt;built-in gang-scheduling support&lt;/strong&gt; through &lt;strong&gt;PodGroupPolicy API&lt;/strong&gt;.
This creates &lt;strong&gt;PodGroup resources&lt;/strong&gt; that ensure all required pods can be scheduled simultaneously before the training job starts.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Platform Administrators&lt;/strong&gt; can configure gang-scheduling in their &lt;strong&gt;TrainingRuntime&lt;/strong&gt; or &lt;strong&gt;ClusterTrainingRuntime&lt;/strong&gt; definitions. Here’s an example:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;trainer.kubeflow.org/v1alpha1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ClusterTrainingRuntime&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;torch-distributed-gang-scheduling&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;mlPolicy&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;numNodes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;numProcPerNode&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;podGroupPolicy&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;coscheduling&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;scheduleTimeoutSeconds&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;120&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# ... rest of runtime configuration&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Currently, &lt;strong&gt;KF Trainer v2&lt;/strong&gt; supports the &lt;strong&gt;Co-Scheduling plugin&lt;/strong&gt; from &lt;a href=&quot;https://github.com/kubernetes-sigs/scheduler-plugins&quot;&gt;Kubernetes scheduler-plugins&lt;/a&gt; project.
&lt;strong&gt;&lt;a href=&quot;https://github.com/kubeflow/trainer/pull/2672&quot;&gt;Volcano&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&quot;https://github.com/kubeflow/trainer/pull/2663&quot;&gt;KAI&lt;/a&gt;&lt;/strong&gt; scheduler support is coming in future releases to provide more advanced scheduling capabilities.&lt;/p&gt;

&lt;h1 id=&quot;fault-tolerance-improvements&quot;&gt;Fault Tolerance Improvements&lt;/h1&gt;

&lt;p&gt;Training jobs can sometimes fail due to node issues or other problems. The &lt;strong&gt;KF Trainer v2&lt;/strong&gt; improves handling these faults by supporting &lt;strong&gt;Kubernetes PodFailurePolicy&lt;/strong&gt;, which allows users to &lt;strong&gt;define specific rules&lt;/strong&gt; for handling different types of failures, such as restarting the job after temporary node issues or terminating the job after critical errors.&lt;/p&gt;

&lt;h1 id=&quot;whats-next&quot;&gt;What’s Next?&lt;/h1&gt;

&lt;p&gt;Future enhancements will continue to improve the user experience, integrate deeper with other Kubeflow components, and support more training frameworks.
&lt;strong&gt;Upcoming features&lt;/strong&gt; include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/kubeflow/sdk/issues/22&quot;&gt;Local Execution&lt;/a&gt;&lt;/strong&gt; - run training jobs locally without Kubernetes&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://docs.google.com/document/d/1rX7ELAHRb_lvh0Y7BK1HBYAbA0zi9enB0F_358ZC58w/edit?tab=t.0#heading=h.e0573r7wwkgl&quot;&gt;Unified Kubeflow SDK&lt;/a&gt;&lt;/strong&gt; - a single SDK for all Kubeflow projects&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/kubeflow/trainer/issues/2648&quot;&gt;Trainer UI&lt;/a&gt;&lt;/strong&gt; - a user interface to expose high level metrics for training jobs and monitor training logs&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/kubernetes-sigs/kueue/issues/3884&quot;&gt;Native Kueue integration&lt;/a&gt;&lt;/strong&gt; - improve resource management and scheduling capabilities for TrainJob resources&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/kubeflow/trainer/issues/2245&quot;&gt;Model Registry integrations&lt;/a&gt;&lt;/strong&gt; - export trained models directly to Model Registry&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/kubeflow/community/pull/864&quot;&gt;Distributed Data Cache&lt;/a&gt;&lt;/strong&gt; - in-memory Apache Arrow caching for tabular datasets&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/kubeflow/trainer/pull/2672&quot;&gt;Volcano support&lt;/a&gt;&lt;/strong&gt; - advanced AI-specific scheduling with gang scheduling, priority queues, and resource management capabilities&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/kubeflow/trainer/pull/2643&quot;&gt;JAX runtime support&lt;/a&gt;&lt;/strong&gt; - ClusterTrainingRuntime for JAX distributed training&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/kubeflow/trainer/pull/2663&quot;&gt;KAI Scheduler support&lt;/a&gt;&lt;/strong&gt; - NVIDIA’s GPU-optimized scheduler for AI workloads&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;migration-from-training-operator-v1&quot;&gt;Migration from Training Operator v1&lt;/h1&gt;

&lt;p&gt;For users migrating from &lt;strong&gt;Kubeflow Training Operator v1&lt;/strong&gt;, check out a &lt;a href=&quot;https://www.kubeflow.org/docs/components/trainer/operator-guides/migration/&quot;&gt;&lt;strong&gt;Migration Guide&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;resources-and-community&quot;&gt;Resources and Community&lt;/h1&gt;

&lt;p&gt;For more information about &lt;strong&gt;Trainer V2&lt;/strong&gt;, check out the &lt;a href=&quot;https://www.kubeflow.org/docs/components/trainer/&quot;&gt;Kubeflow Trainer documentation&lt;/a&gt; and the &lt;a href=&quot;https://github.com/kubeflow/trainer/tree/master/docs/proposals/2170-kubeflow-trainer-v2&quot;&gt;design proposal&lt;/a&gt; for technical implementation details.&lt;/p&gt;

&lt;p&gt;For more details about Kubeflow Trainer, you can also watch our KubeCon presentations:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://youtu.be/Lgy4ir1AhYw&quot;&gt;Democratizing AI Model Training on Kubernetes with Kubeflow TrainJob and JobSet&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://youtu.be/Fnb1a5Kaxgo&quot;&gt;From High Performance Computing To AI Workloads on Kubernetes: MPI Runtime in Kubeflow TrainJob&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Join the community via the &lt;a href=&quot;https://www.kubeflow.org/docs/about/community/#kubeflow-slack-channels&quot;&gt;#kubeflow-trainer&lt;/a&gt; channel on CNCF Slack, or attend the &lt;a href=&quot;https://docs.google.com/document/d/1MChKfzrKAeFRtYqypFbMXL6ZIc_OgijjkvbqmwRV-64/edit?tab=t.0#heading=h.o8oe6e5kry87&quot;&gt;AutoML and Training Working Group&lt;/a&gt; meetings to contribute or ask questions.
Your feedback, contributions, and questions are always welcome!&lt;/p&gt;</content><author><name>Kubeflow Trainer Team</name></author><category term="trainer" /><summary type="html">Running machine learning workloads on Kubernetes can be challenging. Distributed training and LLMs fine-tuning, in particular, involves managing multiple nodes, GPUs, large datasets, and fault tolerance, which often requires deep Kubernetes knowledge. The Kubeflow Trainer v2 (KF Trainer) was created to hide this complexity, by abstracting Kubernetes from AI Practitioners and providing the easiest, most scalable way to run distributed PyTorch jobs.</summary></entry><entry><title type="html">From Raw Data to Model Serving: A Blueprint for the AI/ML Lifecycle with Kubeflow</title><link href="https://blog.kubeflow.org/fraud-detection-e2e/" rel="alternate" type="text/html" title="From Raw Data to Model Serving: A Blueprint for the AI/ML Lifecycle with Kubeflow" /><published>2025-07-15T00:00:00-05:00</published><updated>2025-07-15T00:00:00-05:00</updated><id>https://blog.kubeflow.org/fraud-detection-e2e</id><content type="html" xml:base="https://blog.kubeflow.org/fraud-detection-e2e/">&lt;p&gt;Are you looking for a practical, reproducible way to take a machine learning project from raw data all the way to a deployed, production-ready model? This post is your blueprint for the AI/ML lifecycle: you’ll learn how to use &lt;a href=&quot;https://www.kubeflow.org&quot;&gt;Kubeflow&lt;/a&gt; and open source tools such as &lt;a href=&quot;https://github.com/feast-dev/feast&quot;&gt;Feast&lt;/a&gt; to build a workflow you can run on your laptop and adapt to your own projects.&lt;/p&gt;

&lt;p&gt;We’ll walk through the entire ML lifecycle—from data preparation to live inference—leveraging the Kubeflow platform to create a cohesive, production-grade MLOps workflow.&lt;/p&gt;

&lt;h2 id=&quot;project-overview&quot;&gt;Project Overview&lt;/h2&gt;

&lt;p&gt;The project implements a complete MLOps workflow for a fraud detection use case. Fraud detection is a critical application in financial services, where organizations need to identify potentially fraudulent transactions in real-time while minimizing false positives that could disrupt legitimate customer activity.&lt;/p&gt;

&lt;p&gt;Our fraud detection system leverages machine learning to analyze large volumes of transaction data, learn patterns from historical behavior, and flag suspicious transactions that deviate from normal patterns. The model considers various features such as transaction amounts, location data, merchant information, and user behavior patterns to make predictions. This makes fraud detection an ideal use case for demonstrating MLOps concepts because it requires:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Real-time inference&lt;/strong&gt;: Fraud detection decisions must be made instantly as transactions occur&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Feature consistency&lt;/strong&gt;: The same features used in training must be available during inference to ensure model accuracy&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: The system must handle high transaction volumes&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Continuous learning&lt;/strong&gt;: Models need regular retraining as fraud patterns evolve&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Compliance and auditability&lt;/strong&gt;: Financial services require comprehensive model tracking and governance&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The workflow ingests raw transaction data, proceeds through data preparation and feature engineering, then model training and registration, and finally deploys the model as a production-ready inference service that can evaluate transactions in real-time.&lt;/p&gt;

&lt;p&gt;The entire workflow is orchestrated as a Kubeflow Pipeline, which provides a powerful framework for defining, deploying, and managing complex machine learning pipelines on Kubernetes.&lt;/p&gt;

&lt;p&gt;Here is a high-level overview of the pipeline:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/2025-07-15-fraud-detection-e2e/pipeline.png&quot; alt=&quot;pipeline.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;a-note-on-the-data&quot;&gt;A Note on the Data&lt;/h2&gt;

&lt;p&gt;The pipeline assumes that the initial datasets (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train.csv&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;test.csv&lt;/code&gt;, etc.) are already available. For readers who wish to follow along or generate their own sample data, a script is provided in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;synthetic_data_generation&lt;/code&gt; directory. This script was used to create the initial data for this project but is not part of the automated Kubeflow pipeline itself.&lt;/p&gt;

&lt;h2 id=&quot;why-kubeflow&quot;&gt;Why Kubeflow?&lt;/h2&gt;

&lt;p&gt;This project demonstrates the power of using Kubeflow to abstract away the complexity of Kubernetes infrastructure, allowing AI Engineers, Data Scientists, and ML engineers to focus on what matters most: the data and model performance.&lt;/p&gt;

&lt;h3 id=&quot;key-benefits&quot;&gt;Key Benefits&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Infrastructure Abstraction&lt;/strong&gt;: Instead of manually managing Kubernetes deployments, service accounts, networking, and storage configurations, the pipeline handles all the infrastructure complexity behind the scenes. You define your ML workflow as code, and Kubeflow takes care of orchestrating the execution across your Kubernetes cluster.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Focus on AI, Not DevOps&lt;/strong&gt;: With the infrastructure automated, you can spend your time on the activities that directly impact model performance:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Experimenting with different feature engineering approaches&lt;/li&gt;
  &lt;li&gt;Tuning hyperparameters and model architectures&lt;/li&gt;
  &lt;li&gt;Analyzing prediction results and model behavior&lt;/li&gt;
  &lt;li&gt;Iterating on data preparation and validation strategies&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Reproducible and Scalable&lt;/strong&gt;: The pipeline ensures that every run follows the same steps with the same environment configurations, making your experiments reproducible. When you’re ready to scale up, the same pipeline can run on larger Kubernetes clusters without code changes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Production-Ready from Day One&lt;/strong&gt;: By using production-grade tools like KServe for model serving, Feast for feature management, and the Model Registry for governance, your development pipeline is already structured for production deployment.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Portable and Cloud-Agnostic&lt;/strong&gt;: The entire workflow runs on standard Kubernetes, making it portable across different cloud providers or on-premises environments. What works on your laptop will work in production.&lt;/p&gt;

&lt;p&gt;This approach shifts the cognitive load from infrastructure management to data science innovation, enabling faster experimentation and more reliable production deployments.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;getting-started-prerequisites-and-cluster-setup&quot;&gt;Getting Started: Prerequisites and Cluster Setup&lt;/h2&gt;

&lt;p&gt;Before diving into the pipeline, you need to set up your local environment. This project is designed to run on a local Kubernetes cluster using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kind&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;prerequisites&quot;&gt;Prerequisites&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;A container engine, like &lt;a href=&quot;https://podman.io/&quot;&gt;Podman&lt;/a&gt; or &lt;a href=&quot;https://www.docker.com/get-started&quot;&gt;Docker&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.python.org/downloads/&quot;&gt;Python&lt;/a&gt; (3.11 or newer).&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/astral-sh/uv&quot;&gt;uv&lt;/a&gt;: A fast Python package installer.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kubernetes.io/docs/tasks/tools/install-kubectl/&quot;&gt;kubectl&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kind.sigs.k8s.io/docs/user/quick-start/#installation&quot;&gt;kind&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://min.io/docs/minio/linux/reference/minio-mc.html&quot;&gt;mc (MinIO Client)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This setup was tested on a VM with 12GB RAM, 8 CPUs, and 150GB of disk space.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;1-create-a-local-kubernetes-cluster&quot;&gt;1. Create a Local Kubernetes Cluster&lt;/h3&gt;

&lt;p&gt;First, create a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kind&lt;/code&gt; cluster. The following command will set up a new cluster with a specific node image compatible with the required components:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kind create cluster &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; fraud-detection-e2e-demo &lt;span class=&quot;nt&quot;&gt;--image&lt;/span&gt; kindest/node:v1.31.6
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;2-deploy-kubeflow-pipelines&quot;&gt;2. Deploy Kubeflow Pipelines&lt;/h3&gt;

&lt;p&gt;With your cluster running, the next step is to deploy Kubeflow Pipelines. For this project, the &lt;a href=&quot;https://www.kubeflow.org/docs/components/pipelines/operator-guides/installation/#deploying-kubeflow-pipelines&quot;&gt;standalone installation&lt;/a&gt; is recommended, as it’s lighter and faster to set up than a full Kubeflow deployment.&lt;/p&gt;

&lt;p&gt;Follow the official &lt;a href=&quot;https://www.kubeflow.org/docs/components/pipelines/operator-guides/installation/#deploying-kubeflow-pipelines&quot;&gt;Kubeflow Pipelines standalone installation guide&lt;/a&gt; for the latest instructions.&lt;/p&gt;

&lt;h3 id=&quot;3-upload-the-raw-data-to-minio&quot;&gt;3. Upload the Raw Data to MinIO&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://min.io/&quot;&gt;MinIO&lt;/a&gt; is an open source, S3-compatible object storage system. In this project, MinIO is used to store raw datasets, intermediate artifacts, and model files, making them accessible to all pipeline components running in Kubernetes.&lt;/p&gt;

&lt;p&gt;Before uploading, you need to port-forward the MinIO service so it’s accessible locally. &lt;strong&gt;Run the following command in a separate terminal window:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl port-forward &lt;span class=&quot;nt&quot;&gt;--namespace&lt;/span&gt; kubeflow svc/minio-service 9000:9000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, generate the synthetic data and copy it to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;feature_engineering/feature_repo/data/input/&lt;/code&gt; if you haven’t done yet. The synthetic data generation script creates the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;raw_transaction_datasource.csv&lt;/code&gt; file that serves as the primary input for the pipeline.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;synthetic_data_generation
uv &lt;span class=&quot;nb&quot;&gt;sync
source&lt;/span&gt; .venv/bin/activate
python synthetic_data_generation.py
&lt;span class=&quot;nb&quot;&gt;cp &lt;/span&gt;raw_transaction_datasource.csv ../feature_engineering/feature_repo/data/input
&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ..
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You should see an output similar to the following. The generation may take a few minutes depending on your hardware.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Using CPython 3.11.11
Creating virtual environment at: .venv
Resolved 7 packages &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;14ms
Installed 6 packages &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;84ms
 + &lt;span class=&quot;nv&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;2.3.0
 + &lt;span class=&quot;nv&quot;&gt;pandas&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;2.3.0
 + python-dateutil&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;2.9.0.post0
 + &lt;span class=&quot;nv&quot;&gt;pytz&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;2025.2
 + &lt;span class=&quot;nv&quot;&gt;six&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;1.17.0
 + &lt;span class=&quot;nv&quot;&gt;tzdata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;2025.2
loading data...
generating transaction level data...
        0 of 1,000,000 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;0%&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;complete
  &lt;/span&gt;100,000 of 1,000,000 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;10%&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;complete
  &lt;/span&gt;200,000 of 1,000,000 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;20%&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;complete
  &lt;/span&gt;300,000 of 1,000,000 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;30%&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;complete
  &lt;/span&gt;400,000 of 1,000,000 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;40%&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;complete
  &lt;/span&gt;500,000 of 1,000,000 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;50%&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;complete
  &lt;/span&gt;600,000 of 1,000,000 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;60%&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;complete
  &lt;/span&gt;700,000 of 1,000,000 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;70%&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;complete
  &lt;/span&gt;800,000 of 1,000,000 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;80%&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;complete
  &lt;/span&gt;900,000 of 1,000,000 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;90%&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;complete&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, install and configure the &lt;a href=&quot;https://min.io/docs/minio/linux/reference/minio-mc.html&quot;&gt;MinIO Client (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mc&lt;/code&gt;)&lt;/a&gt; if you haven’t already. Then, set up the alias and upload the datasets:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mc &lt;span class=&quot;nb&quot;&gt;alias set &lt;/span&gt;minio-local http://localhost:9000 minio minio123
mc mb minio-local/mlpipeline
mc &lt;span class=&quot;nb&quot;&gt;cp&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; feature_engineering/feature_repo/data/input/ minio-local/mlpipeline/artifacts/feature_repo/data/
mc &lt;span class=&quot;nb&quot;&gt;cp &lt;/span&gt;feature_engineering/feature_repo/feature_store.yaml minio-local/mlpipeline/artifacts/feature_repo/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will create the required bucket and directory structure in MinIO and upload your raw datasets, making them available for the pipeline.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Once the upload is complete, you can stop the port-forward process.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;4-install-model-registry-kserve-spark-operator-and-set-policies&quot;&gt;4. Install Model Registry, KServe, Spark Operator, and Set Policies&lt;/h3&gt;

&lt;p&gt;While the datasets are uploading to MinIO, you can proceed to install the remaining Kubeflow components and set up the required Kubernetes policies. The following steps summarize what’s in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;setup.sh&lt;/code&gt;:&lt;/p&gt;

&lt;h4 id=&quot;install-model-registry&quot;&gt;Install Model Registry&lt;/h4&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl apply &lt;span class=&quot;nt&quot;&gt;-k&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;https://github.com/kubeflow/model-registry/manifests/kustomize/overlays/db?ref=v0.2.16&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;install-kserve&quot;&gt;Install KServe&lt;/h4&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create namespace kserve
kubectl config set-context &lt;span class=&quot;nt&quot;&gt;--current&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--namespace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kserve
curl &lt;span class=&quot;nt&quot;&gt;-s&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;https://raw.githubusercontent.com/kserve/kserve/release-0.15/hack/quick_install.sh&quot;&lt;/span&gt; | bash
kubectl config set-context &lt;span class=&quot;nt&quot;&gt;--current&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--namespace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kubeflow
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;install-kubeflow-spark-operator&quot;&gt;Install Kubeflow Spark Operator&lt;/h4&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;helm repo add &lt;span class=&quot;nt&quot;&gt;--force-update&lt;/span&gt; spark-operator https://kubeflow.github.io/spark-operator
helm &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;spark-operator spark-operator/spark-operator &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--namespace&lt;/span&gt; spark-operator &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--create-namespace&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Make sure the Spark Operator is watching all namespaces:&lt;/span&gt;
helm upgrade spark-operator spark-operator/spark-operator &lt;span class=&quot;nt&quot;&gt;--set&lt;/span&gt; spark.jobNamespaces&lt;span class=&quot;o&quot;&gt;={}&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--namespace&lt;/span&gt; spark-operator
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;apply-service-accounts-roles-secrets-and-serving-runtime&quot;&gt;Apply Service Accounts, Roles, Secrets, and Serving Runtime&lt;/h4&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;manifests/&lt;/code&gt; directory contains several YAML files that set up the necessary service accounts, permissions, secrets, and runtime configuration for both KServe and Spark jobs. Here’s what each file does:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kserve-sa.yaml&lt;/code&gt;: Creates a service account for KServe, referencing the MinIO secret.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kserve-minio-secret.yaml&lt;/code&gt;: Creates a secret with MinIO credentials and endpoint info, so KServe can access models and artifacts in MinIO.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kserve-role.yaml&lt;/code&gt;: Defines a ClusterRole allowing management of KServe InferenceService resources.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kserve-role-binding.yaml&lt;/code&gt;: Binds the above ClusterRole to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pipeline-runner&lt;/code&gt; service account in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubeflow&lt;/code&gt; namespace, so pipeline steps can create/manage inference services.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;serving-runtime.yaml&lt;/code&gt;: Registers a custom ServingRuntime for ONNX models, specifying the container image and runtime configuration for model serving.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;spark-sa.yaml&lt;/code&gt;: Creates a service account for Spark jobs in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubeflow&lt;/code&gt; namespace.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;spark-role.yaml&lt;/code&gt;: Defines a Role granting Spark jobs permissions to manage pods, configmaps, services, secrets, PVCs, and SparkApplication resources in the namespace.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;spark-role-binding.yaml&lt;/code&gt;: Binds the above Role to both the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;spark&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pipeline-runner&lt;/code&gt; service accounts in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubeflow&lt;/code&gt; namespace.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kustomization.yaml&lt;/code&gt;: A Kustomize manifest that groups all the above resources for easy application.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Apply all of these with:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl apply &lt;span class=&quot;nt&quot;&gt;-k&lt;/span&gt; ./manifests &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kubeflow
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;These resources ensure that KServe and Spark jobs have the right permissions and configuration to run in your Kubeflow environment.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;building-and-understanding-the-pipeline-images&quot;&gt;Building and Understanding the Pipeline Images&lt;/h2&gt;

&lt;p&gt;In Kubeflow Pipelines, each step of a pipeline runs inside a container. This containerized approach provides several key benefits: isolation between steps, reproducible environments, and the ability to use different runtime requirements for different stages of your pipeline.&lt;/p&gt;

&lt;p&gt;While Kubeflow Pipelines provides default images for common tasks, most real-world ML projects require custom images tailored to their specific needs. Each pipeline component in this project uses a specialized container image that includes the necessary dependencies, libraries, and code to execute that particular step of the ML workflow.&lt;/p&gt;

&lt;p&gt;This section covers how to build these custom images. For detailed information about what each image does and how the code inside each container works, refer to the individual pipeline step sections that follow.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; You only need to build and push these images if you want to modify the code for any of the pipeline components. If you’re using the project as-is, you can use the prebuilt images referenced in the pipeline.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The pipeline uses custom container images for the following components:&lt;/p&gt;

&lt;h3 id=&quot;image-locations&quot;&gt;Image Locations&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;data_preparation/Containerfile&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;feature_engineering/Containerfile&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pipeline/Containerfile&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rest_predictor/Containerfile&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train/Containerfile&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;how-to-build&quot;&gt;How to Build&lt;/h3&gt;

&lt;p&gt;You can build each image using Podman or Docker. For example, to build the data preparation image:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;data_preparation
podman build &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; fraud-detection-e2e-demo-data-preparation:latest &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# or&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# docker build -t fraud-detection-e2e-demo-data-preparation:latest .&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can also refer to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;build_images.sh&lt;/code&gt; script in the project root to see how to build all images in sequence.&lt;/p&gt;

&lt;p&gt;Repeat this process for each component, adjusting the tag and directory as needed.&lt;/p&gt;

&lt;h3 id=&quot;entry-points&quot;&gt;Entry points&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;data_preparation:&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python main.py&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;feature_engineering:&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python feast_feature_engineering.py&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;pipeline:&lt;/strong&gt; Used for orchestrating the pipeline steps (see &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fraud-detection-e2e.py&lt;/code&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;rest_predictor:&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python predictor.py&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;train:&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python train.py&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pushing-images&quot;&gt;Pushing Images&lt;/h3&gt;

&lt;p&gt;After building, push the images to a container registry accessible by your Kubernetes cluster. Update the image references in your pipeline as needed.&lt;/p&gt;

&lt;h2 id=&quot;the-kubeflow-pipeline&quot;&gt;The Kubeflow Pipeline&lt;/h2&gt;

&lt;p&gt;The main pipeline definition is in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pipeline/fraud-detection-e2e.py&lt;/code&gt;. This file is the entrypoint for the Kubeflow pipeline and orchestrates all the steps described below.&lt;/p&gt;

&lt;p&gt;With your environment and permissions set up, you’re ready to run the end-to-end pipeline. Let’s walk through each stage of the workflow and see how Kubeflow orchestrates the entire machine learning lifecycle—from data preparation to real-time inference.&lt;/p&gt;

&lt;h3 id=&quot;1-data-preparation-with-spark&quot;&gt;1. Data Preparation with Spark&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://spark.apache.org/&quot;&gt;Apache Spark&lt;/a&gt; is a powerful open source engine for large-scale data processing and analytics. In this project, we use Spark to efficiently process and transform raw transaction data before it enters the ML pipeline.&lt;/p&gt;

&lt;p&gt;To run Spark jobs on Kubernetes, we use the &lt;a href=&quot;https://www.kubeflow.org/docs/components/spark-operator/&quot;&gt;Kubeflow Spark Operator&lt;/a&gt;. The Spark Operator makes it easy to submit and manage Spark applications as native Kubernetes resources, enabling scalable, distributed data processing as part of your MLOps workflow.&lt;/p&gt;

&lt;h4 id=&quot;container-image-for-data-preparation&quot;&gt;Container Image for Data Preparation&lt;/h4&gt;

&lt;p&gt;This pipeline step uses a custom container image built from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;data_preparation/Containerfile&lt;/code&gt;. The image includes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;PySpark and dependencies:&lt;/strong&gt; Required libraries for distributed data processing&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;MinIO client libraries:&lt;/strong&gt; For reading from and writing to object storage&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Custom data processing code:&lt;/strong&gt; The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;main.py&lt;/code&gt; script that implements the data transformation logic&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The container runs with the entry point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python main.py&lt;/code&gt;, which orchestrates all the data preparation tasks within the Spark job.&lt;/p&gt;

&lt;p&gt;The pipeline begins by launching a Spark job that performs several key data preparation steps, implemented in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;data_preparation/main.py&lt;/code&gt;:&lt;/p&gt;

&lt;h4 id=&quot;combining-datasets&quot;&gt;Combining Datasets&lt;/h4&gt;

&lt;p&gt;The job reads the raw &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train.csv&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;test.csv&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;validate.csv&lt;/code&gt; datasets, adds a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;set&lt;/code&gt; column to each, and combines them:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;train_set&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;INPUT_DIR&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;train.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inferSchema&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_set&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;INPUT_DIR&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;test.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inferSchema&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;validate_set&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;INPUT_DIR&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;validate.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inferSchema&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_set&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;withColumn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;set&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;train&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_set&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;withColumn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;set&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;test&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;validate_set&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;validate_set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;withColumn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;set&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;valid&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;all_sets&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unionByName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unionByName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;validate_set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;type-conversion-and-feature-engineering&quot;&gt;Type Conversion and Feature Engineering&lt;/h4&gt;

&lt;p&gt;It converts certain columns to boolean types and generates unique IDs:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;all_sets&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;all_sets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;withColumn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;fraud&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;fraud&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;all_sets&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;all_sets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;withColumn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;repeat_retailer&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;repeat_retailer&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;all_sets&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;all_sets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;withColumn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;used_chip&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;used_chip&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;all_sets&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;all_sets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;withColumn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;used_pin_number&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;used_pin_number&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;all_sets&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;all_sets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;withColumn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;online_order&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;online_order&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;orderBy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;all_sets&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;all_sets&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;withColumn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;idx&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row_number&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;over&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;withColumn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;user_id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;concat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;user_&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;idx&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;withColumn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;transaction_id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;concat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;txn_&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;idx&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;idx&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;timestamping&quot;&gt;Timestamping&lt;/h4&gt;

&lt;p&gt;The job adds &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;created&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;updated&lt;/code&gt; timestamp columns:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;date_col&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;created&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;updated&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;all_sets&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;all_sets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;withColumn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;date_col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_timestamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;point-in-time-feature-calculation&quot;&gt;Point-in-Time Feature Calculation&lt;/h4&gt;

&lt;p&gt;Using the raw transaction history, the Spark job calculates features such as the number of previous transactions, average/max/stddev of previous transaction amounts, and days since the last/first transaction:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;calculate_point_in_time_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transactions_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# ... (see full code in data_preparation/main.py)
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# Aggregates and joins features for each user at each point in time
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;output&quot;&gt;Output&lt;/h4&gt;

&lt;p&gt;The final processed data is saved as both a CSV (for entity definitions) and a Parquet file (for feature storage) in MinIO:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;entity_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;option&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;header&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;overwrite&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;entity_file_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;overwrite&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parquet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parquet_file_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;All of this logic is orchestrated by the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prepare_data&lt;/code&gt; component in the pipeline, which launches the Spark job on Kubernetes.&lt;/p&gt;

&lt;h3 id=&quot;2-feature-engineering-with-feast&quot;&gt;2. Feature Engineering with Feast&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://feast.dev/&quot;&gt;Feast&lt;/a&gt; is an open source feature store that lets you manage and serve features for both training and inference, ensuring consistency and reducing the risk of training/serving skew. In machine learning, a “feature” is an individual measurable property or characteristic of the data being analyzed—in our fraud detection case, features include transaction amounts, distances from previous transactions, merchant types, and user behavior patterns that help the model distinguish between legitimate and fraudulent activity.&lt;/p&gt;

&lt;h4 id=&quot;container-image-for-feature-engineering&quot;&gt;Container Image for Feature Engineering&lt;/h4&gt;

&lt;p&gt;This pipeline step uses a custom container image built from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;feature_engineering/Containerfile&lt;/code&gt;. The image includes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Feast feature store:&lt;/strong&gt; The complete Feast installation for feature management&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Python dependencies:&lt;/strong&gt; Required libraries for feature processing and materialization&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Feature repository definition:&lt;/strong&gt; The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;repo_definition.py&lt;/code&gt; file that defines the feature views and entities&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;MinIO client libraries:&lt;/strong&gt; For uploading the materialized features and online store to object storage&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The container runs with the entry point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python feast_feature_engineering.py&lt;/code&gt;, which handles the Feast operations including applying feature definitions, materializing features, and uploading the results to MinIO.&lt;/p&gt;

&lt;p&gt;After data preparation, the pipeline uses Feast to register, materialize, and store features for downstream steps. This process starts with defining the features you want to use. For example, in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;feature_repo/repo_definition.py&lt;/code&gt;, you’ll find a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FeatureView&lt;/code&gt; that lists features like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;distance_from_home&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ratio_to_median_purchase_price&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;transactions_fv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FeatureView&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;transactions&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;entities&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transaction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;schema&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Field&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;user_id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feast&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;types&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Field&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;distance_from_home&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feast&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;types&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Field&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ratio_to_median_purchase_price&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feast&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;types&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# ... other features
&lt;/span&gt;    &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;online&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;source&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transaction_source&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once the features are defined, the pipeline runs two key Feast commands. First, it applies the feature definitions to the store:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;subprocess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;feast&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;apply&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cwd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_repo_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;check&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then, it materializes the computed features from the Parquet file into Feast’s online store, making them available for real-time inference:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;subprocess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;feast&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;materialize&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start_date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end_date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cwd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_repo_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;check&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, the resulting feature data and the online store database are uploaded to MinIO, so they’re accessible to the rest of the pipeline:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fput_object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MINIO_BUCKET&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;object_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;local_file_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;By using Feast in this way, you ensure that the same features are available for both model training and real-time predictions, making your ML workflow robust and reproducible.&lt;/p&gt;

&lt;h3 id=&quot;3-model-training&quot;&gt;3. Model Training&lt;/h3&gt;

&lt;p&gt;With the features materialized in Feast, the next step is to train the fraud detection model. The pipeline’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train_model&lt;/code&gt; component retrieves the processed features and prepares them for training. The features used include behavioral and transaction-based signals such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;distance_from_last_transaction&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ratio_to_median_purchase_price&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;used_chip&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;used_pin_number&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;online_order&lt;/code&gt;.&lt;/p&gt;

&lt;h4 id=&quot;container-image-for-model-training&quot;&gt;Container Image for Model Training&lt;/h4&gt;

&lt;p&gt;This pipeline step uses a custom container image built from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train/Containerfile&lt;/code&gt;. The image includes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Machine learning libraries:&lt;/strong&gt; TensorFlow/Keras for neural network training, scikit-learn for data preprocessing&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ONNX Runtime:&lt;/strong&gt; For converting and exporting the trained model to ONNX format&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;PySpark:&lt;/strong&gt; For loading and processing the feature data from Parquet files&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;MinIO client libraries:&lt;/strong&gt; For downloading features and uploading the trained model artifacts&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The container runs with the entry point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python train.py&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The training script loads the features, splits the data into train, validation, and test sets, and scales the input features for better model performance:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;train_features&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;set&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;train&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;validate_features&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;set&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;valid&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_features&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;set&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;test&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# ... select and scale features ...
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It then builds and trains a neural network model using Keras, handling class imbalance and exporting the trained model in ONNX format for portable, high-performance inference:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;build_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_indexes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;validation_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class_weight&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;save_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Exports to ONNX
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;By structuring the training step this way, the pipeline ensures that the model is trained on the same features that will be available at inference time, supporting a robust and reproducible MLOps workflow.&lt;/p&gt;

&lt;h3 id=&quot;4-model-registration&quot;&gt;4. Model Registration&lt;/h3&gt;

&lt;p&gt;Once the model is trained, it’s important to track, version, and manage it before deploying to production. This is where the &lt;a href=&quot;https://www.kubeflow.org/docs/components/model-registry/&quot;&gt;Kubeflow Model Registry&lt;/a&gt; comes in. The Model Registry acts as a centralized service for managing machine learning models and their metadata, making it easier to manage deployments, rollbacks, and audits.&lt;/p&gt;

&lt;h4 id=&quot;container-image-for-model-registration&quot;&gt;Container Image for Model Registration&lt;/h4&gt;

&lt;p&gt;This pipeline step uses a custom container image built from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pipeline/Containerfile&lt;/code&gt;. The image includes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Kubeflow Pipelines SDK:&lt;/strong&gt; For pipeline orchestration and component definitions&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Model Registry client:&lt;/strong&gt; Python libraries for interacting with the Kubeflow Model Registry&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Pipeline orchestration code:&lt;/strong&gt; The core pipeline definition and component functions&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The container is used as the base image for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;register_model&lt;/code&gt; component, which executes the model registration logic inline within the pipeline definition. This approach allows the registration step to run lightweight operations without requiring a separate, specialized container image.&lt;/p&gt;

&lt;p&gt;In the pipeline, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;register_model&lt;/code&gt; component takes the trained model artifact and registers it in the Model Registry. This process includes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Assigning a unique name and version:&lt;/strong&gt; The model is registered with a name (e.g., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;fraud-detection&quot;&lt;/code&gt;) and a version, which is typically tied to the pipeline run ID for traceability.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Storing metadata:&lt;/strong&gt; Along with the model artifact, metadata such as the model format, storage location, and additional tags or descriptions can be stored for governance and reproducibility.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Making the model discoverable:&lt;/strong&gt; Registered models can be easily found and referenced for deployment, monitoring, or rollback.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here’s how the registration step is implemented in the pipeline:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dsl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;component&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base_image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PIPELINE_IMAGE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;register_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NamedTuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'outputs'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_version&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;model_registry&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ModelRegistry&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;registry&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ModelRegistry&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;server_address&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;http://model-registry-service.kubeflow.svc.cluster.local&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8080&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;author&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;fraud-detection-e2e-pipeline&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;user_token&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;non-used&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;is_secure&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;model_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;fraud-detection&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model_version&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;registry&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;register_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;uri&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uri&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_version&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;model_format_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;onnx&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;model_source_class&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;pipelinerun&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;model_source_group&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;fraud-detection&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;model_source_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;model_source_kind&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;kfp&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;model_source_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;fraud-detection-e2e-pipeline&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_version&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;By registering the model in this way, you ensure that every model deployed for inference is discoverable, reproducible, and governed—an essential part of any production-grade MLOps workflow.&lt;/p&gt;

&lt;h3 id=&quot;5-real-time-inference-with-kserve&quot;&gt;5. Real-Time Inference with KServe&lt;/h3&gt;

&lt;p&gt;The final stage of the pipeline is deploying the registered model as a real-time inference service using KServe. &lt;a href=&quot;https://kserve.github.io/website/&quot;&gt;KServe&lt;/a&gt; is an open source model serving platform for Kubernetes that standardizes how you deploy, scale, and manage machine learning models in production.&lt;/p&gt;

&lt;h4 id=&quot;container-image-for-real-time-inference&quot;&gt;Container Image for Real-Time Inference&lt;/h4&gt;

&lt;p&gt;This pipeline step uses a custom container image built from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rest_predictor/Containerfile&lt;/code&gt;. The image includes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;KServe Python SDK:&lt;/strong&gt; For building custom model serving endpoints&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ONNX Runtime:&lt;/strong&gt; For running the trained model in ONNX format&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Feast feature store client:&lt;/strong&gt; For retrieving real-time features during inference&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Model Registry client:&lt;/strong&gt; For downloading the registered model artifacts&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Custom predictor code:&lt;/strong&gt; The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;predictor.py&lt;/code&gt; script that implements the inference logic&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The container runs with the entry point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python predictor.py&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The pipeline’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;serve&lt;/code&gt; component creates a KServe InferenceService using this custom Python predictor.&lt;/p&gt;

&lt;p&gt;This is done by creating a Kubernetes custom resource (CR) of kind &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;InferenceService&lt;/code&gt;, which tells KServe how to deploy and manage the model server. The resource specifies the container image, command, arguments, and service account to use for serving the model.&lt;/p&gt;

&lt;p&gt;Here’s how the InferenceService is defined and created in the pipeline:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;inference_service&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kserve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;V1beta1InferenceService&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;api_version&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kserve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constants&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;KSERVE_GROUP&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;/v1beta1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;InferenceService&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;V1ObjectMeta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;-&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;job_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kserve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_default_target_namespace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;&quot;modelregistry/registered-model-id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;&quot;modelregistry/model-version-id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_version&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kserve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;V1beta1InferenceServiceSpec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;predictor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kserve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;V1beta1PredictorSpec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;service_account_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;kserve-sa&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;V1Container&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;inference-container&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rest_predictor_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;python&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;predictor.py&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;--model-name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;--model-version&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_version_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ks_client&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kserve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;KServeClient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ks_client&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inference_service&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The custom predictor does more than just run the model: it also integrates directly with the Feast online feature store. When a prediction request arrives with a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;user_id&lt;/code&gt;, the predictor first fetches the user’s latest features from Feast and then feeds them to the ONNX model for inference. Here’s a simplified view of the predictor’s logic:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ONNXModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kserve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# ... download model and initialize Feast feature store ...
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_store&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FeatureStore&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;repo_path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_repo_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;InferenceSession&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/app/model&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ready&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;async&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;payload&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;user_id&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;payload&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;user_id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;feature_dict&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_store&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_online_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;entity_rows&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;user_id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;user_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}],&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;features_to_request&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;input_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;feature_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;distance_from_last_transaction&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;feature_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ratio_to_median_purchase_price&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;feature_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;used_chip&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;feature_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;used_pin_number&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;feature_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;online_order&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;user_id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;user_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;prediction&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tolist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;br /&gt;
By default, KServe supports several model serving runtimes, including &lt;a href=&quot;https://github.com/triton-inference-server/server&quot;&gt;Triton Inference Server&lt;/a&gt; (often used via the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kserve-tritonserver&lt;/code&gt; runtime). However, the official Triton server does not support macOS/arm64, which is why this project uses a custom Python predictor for local development and demonstration.&lt;br /&gt;
If you are running on a supported platform (such as x86_64 Linux), you may want to use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kserve-tritonserver&lt;/code&gt; runtime for production workloads, as it offers high performance and native ONNX support.&lt;br /&gt;
If you want to use Feast for online feature retrieval at inference time, a custom Python predictor (like the one in this repo) is the most straightforward approach. If you use the standard &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kserve-tritonserver&lt;/code&gt; runtime, you would need to implement feature fetching as a &lt;a href=&quot;https://github.com/triton-inference-server/python_backend&quot;&gt;Triton Python backend&lt;/a&gt; or as a pre-processing step outside of Triton, since Triton itself does not natively integrate with Feast.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;By structuring the inference step this way, the pipeline ensures that the deployed model always uses the freshest features for each prediction, supporting robust, real-time fraud detection.&lt;/p&gt;

&lt;h2 id=&quot;importing-and-running-the-pipeline&quot;&gt;Importing and Running the Pipeline&lt;/h2&gt;

&lt;p&gt;Once your environment is set up and the data is uploaded, you’re ready to run the pipeline.&lt;/p&gt;

&lt;h3 id=&quot;import-the-pipeline&quot;&gt;Import the Pipeline&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Open the Kubeflow Pipelines UI (usually at &lt;a href=&quot;http://localhost:8080&quot;&gt;http://localhost:8080&lt;/a&gt; if you used the default port-forward).&lt;/li&gt;
  &lt;li&gt;Click &lt;strong&gt;Pipelines&lt;/strong&gt; in the sidebar, then click &lt;strong&gt;Upload pipeline&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Upload the compiled pipeline YAML file (e.g., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pipeline/fraud-detection-e2e.yaml&lt;/code&gt;).&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;run-the-pipeline&quot;&gt;Run the Pipeline&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;After uploading, click on your pipeline in the list.&lt;/li&gt;
  &lt;li&gt;Click &lt;strong&gt;Create run&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Optionally customize the run name and description (the defaults work fine), then click &lt;strong&gt;Start&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You can monitor the progress and view logs for each step directly in the UI.&lt;/p&gt;

&lt;h2 id=&quot;testing-the-live-endpoint&quot;&gt;Testing the Live Endpoint&lt;/h2&gt;

&lt;p&gt;With the inference service running, you can now interact with your deployed model in real time. Let’s see how to send prediction requests and interpret the results.&lt;/p&gt;

&lt;p&gt;Before sending requests, port-forward the inference pod so the service is accessible locally. &lt;strong&gt;Run this command in a separate terminal window:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kubeflow get pods &lt;span class=&quot;nt&quot;&gt;-l&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;component&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;predictor &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;jsonpath&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;{.items[*].metadata.name}&quot;&lt;/span&gt; | &lt;span class=&quot;nb&quot;&gt;tr&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;' '&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'\n'&lt;/span&gt; | &lt;span class=&quot;nb&quot;&gt;grep&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'^fraud-detection'&lt;/span&gt; | &lt;span class=&quot;nb&quot;&gt;head&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-n1&lt;/span&gt; | xargs &lt;span class=&quot;nt&quot;&gt;-I&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt; kubectl port-forward &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kubeflow pod/&lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt; 8081:8080
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With the port-forward active, you can now send a request to the model:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl &lt;span class=&quot;nt&quot;&gt;-X&lt;/span&gt; POST http://localhost:8081/v1/models/onnx-model:predict &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;-H&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Content-Type: application/json&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'{&quot;user_id&quot;: &quot;user_0&quot;}'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The service retrieves features for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;user_0&lt;/code&gt;, runs a prediction, and returns the fraud probability.&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;user_id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;user_0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;prediction&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:[[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.8173668384552002&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Note: The result of the prediction may vary depending on the initial raw data you uploaded.&lt;br /&gt;
Try sending requests with a few different &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;user_id&lt;/code&gt; values (e.g., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;user_1&quot;&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;user_2&quot;&lt;/code&gt;, etc.) to see how the predictions change.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This post has walked you through a complete, reproducible AI/ML workflow—from raw data to a live model serving endpoint—using Kubeflow and open source tools. Along the way, you’ve seen how to prepare data with Spark, manage features with Feast, train and register models, and deploy real-time inference services with KServe, all orchestrated in a portable pipeline you can run on your own laptop.&lt;/p&gt;

&lt;p&gt;By following this blueprint, you can adapt and extend the process for your own machine learning projects, whether you’re working locally or scaling up to production. Kubeflow’s modular platform and ecosystem make it possible to manage the entire ML lifecycle in a consistent, automated, and open way.&lt;/p&gt;

&lt;p&gt;Ready to try it yourself? The complete source code for this project is available on &lt;a href=&quot;https://github.com/hbelmiro/fraud_detection_e2e_demo/tree/kubeflow&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;</content><author><name>Helber Belmiro</name></author><category term="mlops" /><category term="pipelines" /><category term="spark" /><category term="feast" /><category term="model-registry" /><category term="kserve" /><summary type="html">Are you looking for a practical, reproducible way to take a machine learning project from raw data all the way to a deployed, production-ready model? This post is your blueprint for the AI/ML lifecycle: you’ll learn how to use Kubeflow and open source tools such as Feast to build a workflow you can run on your laptop and adapt to your own projects.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.kubeflow.org/images/logo.png" /><media:content medium="image" url="https://blog.kubeflow.org/images/logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Kubeflow 1.10 Release Announcement</title><link href="https://blog.kubeflow.org/kubeflow-1.10-release/" rel="alternate" type="text/html" title="Kubeflow 1.10 Release Announcement" /><published>2025-03-26T00:00:00-05:00</published><updated>2025-03-26T00:00:00-05:00</updated><id>https://blog.kubeflow.org/kubeflow-1.10-release</id><content type="html" xml:base="https://blog.kubeflow.org/kubeflow-1.10-release/">&lt;p&gt;Kubeflow 1.10.0 delivers essential updates that enhance the flexibility, efficiency, and scalability of machine learning
workflows. The new features span across several components, improving both user experience and system performance.&lt;/p&gt;

&lt;h2 id=&quot;highlight-features&quot;&gt;Highlight features&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Trainer 2.0&lt;/li&gt;
  &lt;li&gt;New UI for Model Registry&lt;/li&gt;
  &lt;li&gt;Spark Operator as a core Kubeflow component&lt;/li&gt;
  &lt;li&gt;Kubernetes and container security (CISO compatibility)&lt;/li&gt;
  &lt;li&gt;Hyperparameter Optimization for LLMs Fine-Tuning&lt;/li&gt;
  &lt;li&gt;Loop parallelism in Pipelines&lt;/li&gt;
  &lt;li&gt;New parameter distributions for Katib&lt;/li&gt;
  &lt;li&gt;Deeper Model Registry integrations with KServe&lt;/li&gt;
  &lt;li&gt;New Python SDK, OCI storage, and model caching for KServe&lt;/li&gt;
  &lt;li&gt;New security contexts and rootless Istio-CNI integrations for Spark Operator&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;kubeflow-platform-manifests--security&quot;&gt;Kubeflow Platform (Manifests &amp;amp; Security)&lt;/h2&gt;

&lt;p&gt;The Kubeflow Platform Working Group focuses on simplifying Kubeflow installation, operations, and security. See details below.&lt;/p&gt;

&lt;h3 id=&quot;manifests&quot;&gt;Manifests:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Spark Operator 2.1.0 included in Kubeflow platform, although not installed yet by default&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/kubeflow/manifests/blob/master/README.md&quot;&gt;Documentation updates&lt;/a&gt; that make it easier to install,
extend and upgrade Kubeflow&lt;/li&gt;
  &lt;li&gt;For more details and future plans please consult the &lt;a href=&quot;https://github.com/kubeflow/manifests/issues/2763&quot;&gt;1.10.0&lt;/a&gt; and
&lt;a href=&quot;https://github.com/kubeflow/manifests/issues/3038&quot;&gt;1.10.1/1.11.0&lt;/a&gt; milestones&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Notebooks&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Dashboard&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Pipelines&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Katib&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Trainer&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;KServe&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Model Registry&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Spark&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;https://github.com/kubeflow/kubeflow/issues/7459&quot;&gt;1.10&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;https://github.com/kubeflow/kubeflow/tags&quot;&gt;1.10&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;https://github.com/kubeflow/pipelines/releases&quot;&gt;2.4.1&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;https://github.com/kubeflow/katib/releases&quot;&gt;0.18&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;https://github.com/kubeflow/trainer/releases&quot;&gt;1.9&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;https://github.com/kserve/kserve/releases&quot;&gt;0.14&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;https://github.com/kubeflow/model-registry/releases&quot;&gt;0.2.15&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;https://github.com/kubeflow/spark-operator/releases/tag/v2.1.0&quot;&gt;2.1.0&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Kubernetes&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Kind&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Kustomize&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Cert Manager&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Knative&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Istio&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Dex&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Oauth2-proxy&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.31-1.33&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.26&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5.4.3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.16.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.16&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.24&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.41&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7.7&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;security&quot;&gt;Security:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;CVE reductions - regular scanning with trivy&lt;/li&gt;
  &lt;li&gt;Kubernetes and container security best practices:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/kubeflow/manifests/issues/2528&quot;&gt;Rootless containers&lt;/a&gt; / PodSecurityStandards restricted for:
Istio-CNI, Knative, Dex, Oauth2-proxy, Spark&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/kubeflow/manifests/pull/3050&quot;&gt;50 % done&lt;/a&gt;: KFP, Notebooks / Workspaces, Katib, Trainer, Kserve, …&lt;/li&gt;
      &lt;li&gt;Istio-CNI as default for rootless Kubeflow postponed to &lt;a href=&quot;https://github.com/kubeflow/manifests/milestone/2&quot;&gt;1.10.1&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;OIDC-authservice has been replaced by oauth2-proxy&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/kubeflow/manifests#oauth2-proxy&quot;&gt;Oauth2-proxy&lt;/a&gt; and &lt;a href=&quot;https://github.com/kubeflow/manifests#dex&quot;&gt;Dex&lt;/a&gt;
documentation for external OIDC authentication (Keycloak, and OIDC providers such as Azure, Google etc.)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Trivy CVE scans March 25 2025:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Working Group&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Images&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Critical CVE&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;High CVE&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Medium CVE&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Low CVE&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Katib&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;101&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;417&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;734&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Pipelines&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;57&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;490&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4030&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1922&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Workbenches(Notebooks)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;12&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;12&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;59&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;179&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;224&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Kserve&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;16&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;21&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;305&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6803&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1588&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Manifests&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;14&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;94&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;52&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Trainer&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Model Registry&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;13&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;153&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;188&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Spark&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;37&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1640&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;141&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;All Images&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;81&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;115&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1009&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;13275&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4804&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;pipelines&quot;&gt;Pipelines&lt;/h2&gt;

&lt;h3 id=&quot;support-for-placeholders-in-resource-limits&quot;&gt;Support for Placeholders in Resource Limits&lt;/h3&gt;

&lt;p&gt;Kubeflow Pipelines 2.4.1 introduces support for &lt;a href=&quot;https://github.com/kubeflow/pipelines/pull/11501&quot;&gt;placeholders in resource limits&lt;/a&gt;,
enhancing flexibility in pipeline execution.This update allows users to define dynamic resource limits using
parameterized values, enabling more adaptable and reusable pipeline definitions.&lt;/p&gt;

&lt;h3 id=&quot;support-for-loop-parallelism&quot;&gt;Support for Loop Parallelism&lt;/h3&gt;

&lt;p&gt;Kubeflow Pipelines 2.4.1 introduces a new &lt;a href=&quot;https://github.com/kubeflow/pipelines/issues/8718&quot;&gt;Parallelism Limit for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ParallelFor&lt;/code&gt; tasks&lt;/a&gt;,
giving users the ability to run massively parallel inference pipelines, with more control over parallel execution in
their workflows. This feature allows users to specify the maximum number of parallel iterations, preventing resource
overutilization and improving system stability. When running large pipelines with GPUs, proper use of this feature could
save your team thousands of dollars in compute expenses.&lt;/p&gt;

&lt;h3 id=&quot;implement-subdag-output-resolution&quot;&gt;Implement SubDAG Output Resolution&lt;/h3&gt;

&lt;p&gt;Kubeflow 1.10 ensures that &lt;a href=&quot;https://github.com/kubeflow/pipelines/pull/11196&quot;&gt;pipelines using nested DAGs&lt;/a&gt; work
correctly and reliably when treated as components. Outputs from deeply nested DAGs will now resolve properly, avoiding
broken dependencies.&lt;/p&gt;

&lt;h2 id=&quot;model-registry&quot;&gt;Model Registry&lt;/h2&gt;

&lt;p&gt;Model Registry introduces a new user interface and enhanced model management capabilities.&lt;/p&gt;

&lt;h3 id=&quot;model-registry-ui&quot;&gt;Model Registry UI&lt;/h3&gt;

&lt;p&gt;The new Kubeflow &lt;a href=&quot;https://www.kubeflow.org/docs/components/model-registry/getting-started/#using-the-model-registry-ui&quot;&gt;Model Registry UI&lt;/a&gt;
provides a user-friendly web interface for managing machine learning models within the Kubeflow platform. It centralizes
model metadata, version tracking, and artifact management, streamlining MLOps workflows.&lt;/p&gt;

&lt;p&gt;Key features include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Easy model registration with custom metadata&lt;/li&gt;
  &lt;li&gt;Comprehensive model management with filtering and sorting&lt;/li&gt;
  &lt;li&gt;Archiving capabilities&lt;/li&gt;
  &lt;li&gt;Version control&lt;/li&gt;
  &lt;li&gt;Metadata editing&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../images/2025-03-26-kubeflow-1.10-release/model-registry-ui.png&quot; alt=&quot;Model Registry UI&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The UI interacts with the Model Registry’s REST API, making it accessible to users of all technical backgrounds and
enhancing collaboration across data science, ML engineering, and MLOps teams.&lt;/p&gt;

&lt;p&gt;To get started with the Model Registry UI, which is currently in Alpha, you can follow the instructions
&lt;a href=&quot;https://www.kubeflow.org/docs/components/model-registry/installation/#installing-on-kubeflow-platform&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The Kubeflow Model Registry UI Team would like to conduct user research to identify possible enhancements we can contribute in future iterations of the Kubeflow Model Registry UI. If you are interested in participating in this study, please fill out &lt;a href=&quot;https://docs.google.com/forms/d/e/1FAIpQLSeCveL-b0NyUohYa86I3VeTXeynEQLpV5Loj-1HkoUVDwlVCQ/viewform&quot;&gt;this survey&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&quot;custom-storage-initializer&quot;&gt;Custom Storage Initializer&lt;/h3&gt;

&lt;p&gt;The Model Registry Custom Storage Initializer (CSI) is a custom implementation of the KServe ClusterStorageContainer.
This feature allows users to utilize Model Registry metadata to download and deploy models efficiently. With the newest
release of the Model Registry, it is now possible to install and use the Custom Storage Initializer (CSI).&lt;/p&gt;

&lt;p&gt;You can find detailed installation instructions and a small example in the “Getting Started” section of the Model
Registry component on the Kubeflow website.&lt;/p&gt;

&lt;p&gt;For additional information and future developments towards better integration with KServe, you can refer to the slides
&lt;a href=&quot;https://docs.google.com/presentation/d/1wprxN0n23EMkPRX_PaZZcIzZbn_i8Sh_&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;training-operator-trainer--katib&quot;&gt;Training Operator (Trainer) &amp;amp; Katib&lt;/h2&gt;

&lt;p&gt;Kubeflow 1.10 enhances the Training Operator and Katib, providing new tools and APIs for hyperparameter optimization,
particularly for large language models.&lt;/p&gt;

&lt;p&gt;Moreover, the Kubeflow Training Operator now supports &lt;a href=&quot;https://github.com/kubeflow/trainer/issues/1619&quot;&gt;JAX for distributed training&lt;/a&gt;,
enabling users to leverage JAX’s capabilities for efficient and scalable model training.&lt;/p&gt;

&lt;p&gt;Finally, if you want to get involved with Trainer V2, take a look at this &lt;a href=&quot;https://github.com/kubeflow/trainer/tree/master/docs/proposals/2170-kubeflow-trainer-v2&quot;&gt;KEP&lt;/a&gt;
and &lt;a href=&quot;https://github.com/kubeflow/trainer/issues/2170&quot;&gt;issue&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;hyperparameter-optimization-api-for-llms&quot;&gt;Hyperparameter Optimization API for LLMs&lt;/h3&gt;

&lt;p&gt;Katib introduces a new high-level &lt;a href=&quot;https://github.com/kubeflow/katib/issues/2339&quot;&gt;API for hyperparameter tuning&lt;/a&gt;,
streamlining LLMOps workflows in Kubernetes. This API integrates Katib and the Training Operator to automate
hyperparameter optimization, reducing manual effort for data scientists fine-tuning large language models. For more
information, refer to the &lt;a href=&quot;https://blog.kubeflow.org/gsoc-2024-project-4/&quot;&gt;feature release blog post&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;support-for-various-parameter-distributions&quot;&gt;Support for Various Parameter Distributions&lt;/h3&gt;

&lt;p&gt;Katib now adds &lt;a href=&quot;https://github.com/kubeflow/katib/issues/2374&quot;&gt;support for multiple probability distributions&lt;/a&gt;.
Previously limited to uniform distributions, Katib now supports log-uniform, normal, and log-normal distributions,
providing data scientists with greater &lt;a href=&quot;https://youtu.be/4myE0DPp6Ko&quot;&gt;flexibility in tuning hyperparameters&lt;/a&gt;. This is
particularly useful for parameters like learning rates, which benefit from log-uniform sampling, or values expected to
vary around a mean, suited for normal distributions.&lt;/p&gt;

&lt;h3 id=&quot;push-based-metrics-collection&quot;&gt;Push-Based Metrics Collection&lt;/h3&gt;

&lt;p&gt;Katib now allows users to push metrics to Katib DB directly. The new push-based design provides administrative and
performanace improvements to the existing pull based design. For further details, please refer to the
&lt;a href=&quot;https://blog.kubeflow.org/gsoc-2024-project-6/&quot;&gt;Push-Based Metrics Collection blog post&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;dashboard--notebooks&quot;&gt;Dashboard &amp;amp; Notebooks&lt;/h2&gt;

&lt;p&gt;Kubeflow 1.10 improves the observability and usability of Notebooks, while providing updated
&lt;a href=&quot;https://github.com/kubeflow/kubeflow/pull/7687&quot;&gt;default images&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;prometheus-metrics-for-notebooks&quot;&gt;Prometheus Metrics for Notebooks&lt;/h3&gt;

&lt;p&gt;Both the Notebooks component and CRUD backends now feature Prometheus metrics. Notebooks expose custom metrics using the
prom-client library, and CRUD backends utilize the prometheus_flask_exporter library. This ensures consistent metrics
integration across all backend services.&lt;/p&gt;

&lt;h3 id=&quot;more-descriptive-error-messages&quot;&gt;More Descriptive Error Messages&lt;/h3&gt;

&lt;p&gt;Error messages for notebook creation failures due to resource constraints are now more descriptive. Users can quickly
identify issues such as insufficient resources.&lt;/p&gt;

&lt;h2 id=&quot;spark-operator&quot;&gt;Spark Operator&lt;/h2&gt;

&lt;p&gt;The Spark Operator, now integrated as a core Kubeflow component, includes several key enhancements focusing on
architecture, security, and &lt;a href=&quot;https://blog.kubeflow.org/operators/benchmarking/performance/2025/03/15/kubeflow-spark-operator-benchmarks.html&quot;&gt;performance&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Rebuilt with Controller Runtime (v2.0.0): Modernized core architecture using controller-runtime, aligning with
Kubernetes controller patterns for improved structure, extensibility, and testability.&lt;/li&gt;
  &lt;li&gt;YuniKorn Gang Scheduling Support (v2.0.0): Enables efficient scheduling of Spark driver &amp;amp; executor pods as a group,
ideal for large-scale data pipelines with resource guarantees.&lt;/li&gt;
  &lt;li&gt;Enhanced Security Contexts &amp;amp; SeccompProfile Support (v2.1.1): Adds support for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seccompProfile: RuntimeDefault&lt;/code&gt; &amp;amp; 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;readOnlyRootFilesystem&lt;/code&gt;, aligning with Kubernetes Pod Security Standards and minimizing security risk.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;kserve&quot;&gt;KServe&lt;/h2&gt;

&lt;p&gt;KServe v0.14.1 introduces several essential features that enhance its capabilities for deploying and managing machine
learning models.&lt;/p&gt;

&lt;h3 id=&quot;new-python-sdk&quot;&gt;New Python SDK&lt;/h3&gt;

&lt;p&gt;The release includes a new Python SDK with both REST and GRPC inference clients, offering asynchronous support and the
ability to handle tensor data in binary format.&lt;/p&gt;

&lt;h3 id=&quot;oci-storage-for-models&quot;&gt;OCI Storage for Models&lt;/h3&gt;

&lt;p&gt;OCI storage for models has also been promoted to a stable feature, with improvements to stability by configuring OCI
models as init containers.&lt;/p&gt;

&lt;h3 id=&quot;model-cache-feature&quot;&gt;Model Cache Feature&lt;/h3&gt;

&lt;p&gt;Additionally, the introduction of the Model Cache feature leverages local node storage to reduce model load times,
especially for large models, enhancing scalability.&lt;/p&gt;

&lt;h3 id=&quot;hugging-face-integration&quot;&gt;Hugging Face Integration&lt;/h3&gt;

&lt;p&gt;KServe v0.14.1 further expands integration with Hugging Face, enabling direct model deployment from the Hugbing Face hub
via a new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hf://&lt;/code&gt; URI schema.&lt;/p&gt;

&lt;h2 id=&quot;what-comes-next&quot;&gt;What comes next?&lt;/h2&gt;

&lt;p&gt;If you want to take a peek into the Kubeflow 1.11 roadmap planning and contribute with your ideas, see
&lt;a href=&quot;https://github.com/kubeflow/kubeflow/issues/7459&quot;&gt;Notebooks&lt;/a&gt;,
&lt;a href=&quot;https://github.com/kubeflow/manifests/milestone/2&quot;&gt;Manifests &amp;amp; Security&lt;/a&gt;, Pipelines, Model Registry, Katib,
Training Operator.&lt;/p&gt;

&lt;h2 id=&quot;how-to-get-started-with-110&quot;&gt;How to get started with 1.10&lt;/h2&gt;

&lt;p&gt;Visit the Kubeflow 1.10 &lt;a href=&quot;https://github.com/kubeflow/manifests/releases&quot;&gt;release page&lt;/a&gt; or head over to the Getting
Started and Support pages.&lt;/p&gt;

&lt;h2 id=&quot;join-the-community&quot;&gt;Join the Community&lt;/h2&gt;

&lt;p&gt;We would like to thank everyone for the contribution to Kubeflow 1.10, especially Ricardo Martinelli De Oliveira for his
work as the v1.10 Release Manager, all the release team and the working group leads, who relentlessly dedicate their
time to this great project.&lt;/p&gt;

&lt;p&gt;Release team members : Ricardo Martinelli De Oliveira, Dimitris Poulopoulos, Matteo Mortari, Julius von Kohout
Valentina Rodriguez Sosa, Helber Belmiro, Vraj Bhatt, Diego Lovison, Dagvanorov Lkhagvajav, Sailesh Duddupudi,
Manos Vlassis, Tarek Abouzeid, Milos Grubjesic&lt;/p&gt;

&lt;p&gt;Working Group leads : Andrey Velichkevich, Julius von Kohout,  Mathew Wicks, …&lt;/p&gt;

&lt;p&gt;Kubeflow Steering Committee : Andrey Velichkevich, Julius von Kohout, Yuan Tang, Johnu George, Francisco Javier Araceo&lt;/p&gt;

&lt;p&gt;Participating Distributions : Charmed Kubeflow (Canonical), Nutanix, OpenShift AI (RedHat), QBO&lt;/p&gt;

&lt;p&gt;You can find more details about Kubeflow distributions
&lt;a href=&quot;https://www.kubeflow.org/docs/started/installing-kubeflow/#packaged-distributions&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;want-to-help&quot;&gt;Want to help?&lt;/h2&gt;

&lt;p&gt;The Kubeflow community Working Groups hold open meetings and are always looking for more volunteers and users to unlock
the potential of machine learning. If you’re interested in becoming a Kubeflow contributor, please feel free to check
out the resources below. We look forward to working with you!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Visit our &lt;a href=&quot;https://www.kubeflow.org/docs/about/community/&quot;&gt;Kubeflow website&lt;/a&gt; or Kubeflow GitHub Page.&lt;/li&gt;
  &lt;li&gt;Join the &lt;a href=&quot;https://www.kubeflow.org/docs/about/community/&quot;&gt;Kubeflow Slack channel&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Join the &lt;a href=&quot;https://groups.google.com/g/kubeflow-discuss&quot;&gt;kubeflow-discuss&lt;/a&gt; mailing list.&lt;/li&gt;
  &lt;li&gt;Attend our weekly &lt;a href=&quot;https://www.kubeflow.org/docs/about/community/#kubeflow-community-call&quot;&gt;community meeting&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Kubeflow 1.10 Release Team, Dimitris Poulopoulos</name></author><category term="release" /><summary type="html">Kubeflow 1.10.0 delivers essential updates that enhance the flexibility, efficiency, and scalability of machine learning workflows. The new features span across several components, improving both user experience and system performance.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.kubeflow.org/images/logo.png" /><media:content medium="image" url="https://blog.kubeflow.org/images/logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">🚀 Announcing the Kubeflow Spark Operator Benchmarking Results</title><link href="https://blog.kubeflow.org/operators/benchmarking/performance/2025/03/15/kubeflow-spark-operator-benchmarks.html" rel="alternate" type="text/html" title="🚀 Announcing the Kubeflow Spark Operator Benchmarking Results" /><published>2025-03-15T00:00:00-05:00</published><updated>2025-03-15T00:00:00-05:00</updated><id>https://blog.kubeflow.org/operators/benchmarking/performance/2025/03/15/kubeflow-spark-operator-benchmarks</id><content type="html" xml:base="https://blog.kubeflow.org/operators/benchmarking/performance/2025/03/15/kubeflow-spark-operator-benchmarks.html">&lt;p&gt;Kubernetes has become the go-to platform for running large-scale &lt;a href=&quot;https://spark.apache.org/&quot;&gt;Apache Spark&lt;/a&gt; workloads. But as workloads scale, &lt;strong&gt;how do you ensure your Spark jobs run efficiently without hitting bottlenecks?&lt;/strong&gt; Managing thousands of concurrent Spark jobs can introduce &lt;strong&gt;severe performance challenges&lt;/strong&gt;—from &lt;strong&gt;CPU saturation&lt;/strong&gt; in the Spark Operator to &lt;strong&gt;Kubernetes API slowdowns&lt;/strong&gt; and &lt;strong&gt;job scheduling inefficiencies&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;To address these challenges, we are excited to introduce the &lt;strong&gt;Kubeflow Spark Operator Benchmarking Results and Toolkit&lt;/strong&gt;—a comprehensive framework to analyze performance, pinpoint bottlenecks, and optimize your Spark on Kubernetes deployments.&lt;/p&gt;

&lt;h2 id=&quot;-whats-included&quot;&gt;🔍 What’s Included?&lt;/h2&gt;
&lt;p&gt;This benchmarking effort provides &lt;strong&gt;three key outcomes&lt;/strong&gt; to help you take full control of your Spark on Kubernetes deployment:&lt;/p&gt;

&lt;p&gt;✅ &lt;strong&gt;&lt;a href=&quot;https://www.kubeflow.org/docs/components/spark-operator/performance/benchmarking/&quot;&gt;Benchmarking Results&lt;/a&gt;&lt;/strong&gt; – A detailed evaluation of performance insights and tuning recommendations for large-scale Spark workloads.&lt;br /&gt;
🛠 &lt;strong&gt;&lt;a href=&quot;https://github.com/awslabs/data-on-eks/tree/main/analytics/terraform/spark-k8s-operator/examples/benchmark/spark-operator-benchmark-kit&quot;&gt;Benchmarking Test Toolkit&lt;/a&gt;&lt;/strong&gt; – A fully reproducible test suite to help users evaluate their own Spark Operator performance and validate improvements.&lt;br /&gt;
📊 &lt;strong&gt;&lt;a href=&quot;https://grafana.com/grafana/dashboards/23032-spark-operator-scale-test-dashboard/&quot;&gt;Open-Sourced Grafana Dashboard&lt;/a&gt;&lt;/strong&gt; – A &lt;strong&gt;battle-tested&lt;/strong&gt; visualization tool designed specifically to track large-scale Spark Operator deployments, providing real-time monitoring of job processing efficiency, API latencies, and system health.&lt;/p&gt;

&lt;h2 id=&quot;-the-challenges-why-benchmarking-matters&quot;&gt;❌ The Challenges: Why Benchmarking Matters&lt;/h2&gt;
&lt;p&gt;Running &lt;strong&gt;thousands of Spark jobs&lt;/strong&gt; on Kubernetes at scale uncovers several &lt;strong&gt;performance roadblocks&lt;/strong&gt; that can &lt;strong&gt;cripple efficiency&lt;/strong&gt; if left unresolved:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;🚦 Spark Operator Becomes CPU-Bound&lt;/strong&gt;: When handling thousands of Spark jobs, the controller pod maxes out CPU resources, limiting job submission rates.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;🐢 High API Server Latency&lt;/strong&gt;: As workloads scale, Kubernetes API responsiveness degrades—job status updates slow down, affecting observability and scheduling efficiency.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;🕒 Webhook Overhead Slows Job Starts&lt;/strong&gt;: Using webhooks adds &lt;strong&gt;~60 seconds&lt;/strong&gt; of extra latency per job, reducing throughput in high-concurrency environments.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;💥 Namespace Overload Causes Failures&lt;/strong&gt;: Running &lt;strong&gt;6,000+ SparkApplications in a single namespace&lt;/strong&gt; resulted in &lt;strong&gt;pod failures&lt;/strong&gt; due to excessive environment variables and service object overload.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 &lt;strong&gt;So, how do you fix these issues and optimize your Spark Operator deployment?&lt;/strong&gt;&lt;br /&gt;
That’s where our &lt;strong&gt;benchmarking results and toolkit&lt;/strong&gt; come in.&lt;/p&gt;

&lt;h2 id=&quot;-tuning-best-practices-for-spark-operator&quot;&gt;🛠 Tuning Best Practices for Spark Operator&lt;/h2&gt;
&lt;p&gt;Based on our benchmarking findings, we provide &lt;strong&gt;clear, actionable recommendations&lt;/strong&gt; for improving Spark Operator performance at scale.&lt;/p&gt;

&lt;p&gt;If you’re running &lt;strong&gt;thousands of concurrent Spark jobs&lt;/strong&gt;, here’s what you need to do:&lt;/p&gt;

&lt;h3 id=&quot;deploy-multiple-spark-operator-instances&quot;&gt;&lt;strong&gt;Deploy Multiple Spark Operator Instances&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;💡 &lt;strong&gt;Why?&lt;/strong&gt; A single Spark Operator instance struggles to keep up with high job submission rates.&lt;br /&gt;
✅ &lt;strong&gt;Solution&lt;/strong&gt;: When a single Spark Operator instance struggles with high job submission rates, leading to CPU saturation and slower job launches, &lt;strong&gt;deploying multiple instances can help&lt;/strong&gt;. Distribute the workload by assigning different namespaces to each instance. For example, one instance can manage `&lt;strong&gt;20 namespaces&lt;/strong&gt; while another handles a separate set of &lt;strong&gt;20 namespaces&lt;/strong&gt;. This prevents bottlenecks and ensures efficient Spark job execution.&lt;/p&gt;

&lt;h3 id=&quot;disable-webhooks-for-faster-job-starts&quot;&gt;&lt;strong&gt;Disable Webhooks for Faster Job Starts&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;💡 &lt;strong&gt;Why?&lt;/strong&gt; Webhooks introduce &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~60 seconds&lt;/code&gt; of delay per job due to validation and mutation overhead, reducing throughput in large workloads.
✅ &lt;strong&gt;Solution&lt;/strong&gt;: Instead of using &lt;strong&gt;webhooks&lt;/strong&gt; for volume mounts, node selectors, or taints, define &lt;strong&gt;Spark Pod Templates&lt;/strong&gt; directly within the Spark job definition—no additional files are needed. Disable webhooks by setting &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;webhook.enable=false&lt;/code&gt; in the Helm chart.&lt;/p&gt;

&lt;h3 id=&quot;increase-controller-workers&quot;&gt;&lt;strong&gt;Increase Controller Workers&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;💡 &lt;strong&gt;Why?&lt;/strong&gt; By default, the operator runs with &lt;strong&gt;10 controller workers&lt;/strong&gt;, but our benchmarks showed increasing this to &lt;strong&gt;20 or 30 workers&lt;/strong&gt; improved job throughput.&lt;br /&gt;
✅ &lt;strong&gt;Solution&lt;/strong&gt;: Set &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;controller.workers=20&lt;/code&gt; if your Operator pod runs on a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;36-core&lt;/code&gt; CPU or higher to enable faster parallel job execution. For larger workloads (e.g., 72+ cores), increase to 40+ workers for better parallel job execution.&lt;/p&gt;

&lt;h3 id=&quot;enable-a-batch-scheduler-volcano--yunikorn&quot;&gt;&lt;strong&gt;Enable a Batch Scheduler (Volcano / YuniKorn)&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;💡 &lt;strong&gt;Why?&lt;/strong&gt; Kubernetes’ default scheduler isn’t optimized for batch workloads, leading to &lt;strong&gt;inefficient job placements&lt;/strong&gt;.&lt;br /&gt;
✅ &lt;strong&gt;Solution&lt;/strong&gt;: Enable &lt;strong&gt;Volcano&lt;/strong&gt; or &lt;strong&gt;YuniKorn&lt;/strong&gt; (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;batchScheduler.enable=true&lt;/code&gt;) to optimize job scheduling. These schedulers provide &lt;strong&gt;gang scheduling, queue management, and multi-tenant resource sharing&lt;/strong&gt;. Benchmarks show that &lt;strong&gt;Apache YuniKorn&lt;/strong&gt; schedules jobs faster than the default Kubernetes scheduler.&lt;/p&gt;

&lt;h3 id=&quot;optimize-api-server-scaling&quot;&gt;&lt;strong&gt;Optimize API Server Scaling&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;💡 &lt;strong&gt;Why?&lt;/strong&gt; API server latency spikes to &lt;strong&gt;600ms+ under heavy load&lt;/strong&gt;, affecting Spark job responsiveness.&lt;br /&gt;
✅ &lt;strong&gt;Solution&lt;/strong&gt;: Scale API server replicas, allocate more CPU and memory, and optimize event handling. Ensure your &lt;strong&gt;Kubernetes API server and etcd&lt;/strong&gt; auto-scale to handle bursty workloads efficiently. Monitor &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kube-apiserver&lt;/code&gt; metrics and scale &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;etcd&lt;/code&gt; accordingly. If running thousands of Spark pods, consider &lt;strong&gt;manually increasing control plane node sizes&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;distribute-spark-jobs-across-multiple-namespaces&quot;&gt;&lt;strong&gt;Distribute Spark Jobs Across Multiple Namespaces&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;💡 &lt;strong&gt;Why?&lt;/strong&gt; Running too many jobs in a single namespace causes &lt;strong&gt;environment variable overflows&lt;/strong&gt;, leading to pod failures.&lt;br /&gt;
✅ &lt;strong&gt;Solution&lt;/strong&gt;: When too many pods are placed in a single namespace, operations like listing or modifying resources can generate large &lt;strong&gt;API server&lt;/strong&gt; responses, increasing latency. For example, retrieving all pods may result in a substantial size in response, consuming significant server resources. Additionally, &lt;strong&gt;etcd&lt;/strong&gt;, Kubernetes’ key-value store, can become a bottleneck when handling frequent updates from a high number of pods in one namespace. Heavy read and write operations can strain etcd, causing increased latencies and potential timeouts. To improve performance and stability, it is recommended to &lt;strong&gt;distribute workloads across multiple namespaces&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;monitor--tune-using-the-open-source-grafana-dashboard&quot;&gt;&lt;strong&gt;Monitor &amp;amp; Tune Using the Open-Source Grafana Dashboard&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;💡 &lt;strong&gt;Why?&lt;/strong&gt; Observability is key to identifying performance bottlenecks.&lt;br /&gt;
✅ &lt;strong&gt;Solution&lt;/strong&gt;: Use our &lt;strong&gt;&lt;a href=&quot;https://grafana.com/grafana/dashboards/23032-spark-operator-scale-test-dashboard/&quot;&gt;Spark Operator Scale Test Dashboard&lt;/a&gt;&lt;/strong&gt; to track job submission rates, API latencies, and CPU utilization in real time.&lt;/p&gt;

&lt;h2 id=&quot;-learn-more--get-started&quot;&gt;📖 Learn More &amp;amp; Get Started&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;Kubeflow Spark Operator Benchmarking Results and Toolkit&lt;/strong&gt; provide an in-depth &lt;strong&gt;performance playbook&lt;/strong&gt; for running Spark at scale on Kubernetes. Whether you’re troubleshooting an existing deployment or planning for future growth, this toolkit arms you with &lt;strong&gt;data-driven insights&lt;/strong&gt; and &lt;strong&gt;best practices&lt;/strong&gt; for success.&lt;/p&gt;

&lt;p&gt;🚀 &lt;strong&gt;Ready to optimize your Spark workloads?&lt;/strong&gt; Dive into the full results and toolkit below:&lt;br /&gt;
📖 &lt;strong&gt;&lt;a href=&quot;https://www.kubeflow.org/docs/components/spark-operator/performance/benchmarking/&quot;&gt;Kubeflow Spark Operator Benchmarks&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;</content><author><name>&lt;a href='https://www.linkedin.com/in/varaprofile/'&gt;Vara Bonthu&lt;/a&gt;, &lt;a href='https://www.linkedin.com/in/manabumccloskey/'&gt;Manabu McCloskey&lt;/a&gt;, &lt;a href='https://www.linkedin.com/in/ratnopamc/'&gt;Ratnopam Chakrabarti &lt;/a&gt;, &lt;a href='https://www.linkedin.com/in/alanhalcyon/'&gt;Alan Halcyon&lt;/a&gt;</name></author><category term="operators" /><category term="benchmarking" /><category term="performance" /><summary type="html">Kubernetes has become the go-to platform for running large-scale Apache Spark workloads. But as workloads scale, how do you ensure your Spark jobs run efficiently without hitting bottlenecks? Managing thousands of concurrent Spark jobs can introduce severe performance challenges—from CPU saturation in the Spark Operator to Kubernetes API slowdowns and job scheduling inefficiencies.</summary></entry><entry><title type="html">Optimizing RAG Pipelines with Katib: Hyperparameter Tuning for Better Retrieval &amp;amp; Generation</title><link href="https://blog.kubeflow.org/katib/rag/" rel="alternate" type="text/html" title="Optimizing RAG Pipelines with Katib: Hyperparameter Tuning for Better Retrieval &amp;amp; Generation" /><published>2025-02-21T00:00:00-06:00</published><updated>2025-02-21T00:00:00-06:00</updated><id>https://blog.kubeflow.org/katib/katib-rag-optimization</id><content type="html" xml:base="https://blog.kubeflow.org/katib/rag/">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;As artificial intelligence and machine learning models become more
sophisticated, optimising their performance remains a critical challenge.
Kubeflow provides a robust component, &lt;a href=&quot;https://www.kubeflow.org/docs/components/katib/&quot;&gt;Katib&lt;/a&gt;, designed for
hyperparameter optimization and neural architecture search. As a part of the
Kubeflow ecosystem, Katib enables scalable, automated tuning of underlying
machine learning models, reducing the manual effort required for parameter
selection while improving model performance across diverse ML workflows.&lt;/p&gt;

&lt;p&gt;With Retrieval-Augmented Generation (&lt;a href=&quot;https://en.wikipedia.org/wiki/Retrieval-augmented_generation&quot;&gt;RAG&lt;/a&gt;) becoming an increasingly
popular approach for improving search and retrieval quality, optimizing its
parameters is essential to achieving high-quality results. RAG pipelines involve
multiple hyperparameters that influence retrieval accuracy, hallucination
reduction, and language generation quality. In this blog, we will explore how
Katib can be leveraged to fine-tune a RAG pipeline, ensuring optimal performance
by systematically adjusting key hyperparameters.&lt;/p&gt;

&lt;h1 id=&quot;lets-get-started&quot;&gt;Let’s Get Started!&lt;/h1&gt;

&lt;h2 id=&quot;step-1-setup&quot;&gt;STEP 1: Setup&lt;/h2&gt;

&lt;p&gt;Since compute resources are scarcer than a perfectly labeled dataset :), we’ll
use a lightweight &lt;a href=&quot;https://kind.sigs.k8s.io/&quot;&gt;Kind cluster (Kubernetes in Docker)&lt;/a&gt;
cluster to run this example locally. Rest assured, this setup can seamlessly
scale to larger clusters by increasing the dataset size and the number of
hyperparameters to tune.&lt;/p&gt;

&lt;p&gt;To get started, we’ll first install the Katib control plane in our cluster by
following the steps outlined &lt;a href=&quot;https://www.kubeflow.org/docs/components/katib/installation/&quot;&gt;in the documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;step-2-implementing-rag-pipeline&quot;&gt;STEP 2: Implementing RAG pipeline&lt;/h2&gt;

&lt;p&gt;In this implementation, we use a &lt;a href=&quot;https://www.sciencedirect.com/topics/computer-science/retrieval-model&quot;&gt;retriever model&lt;/a&gt;, which
encodes queries and documents into vector representations to find the most
relevant matches, to fetch relevant documents based on a query and a generator
model to produce coherent text responses.&lt;/p&gt;

&lt;h3 id=&quot;implementation-details&quot;&gt;Implementation Details:&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Retriever: Sentence Transformer &amp;amp; FAISS (Facebook AI Similarity Search) Index
    &lt;ul&gt;
      &lt;li&gt;A SentenceTransformer model (paraphrase-MiniLM-L6-v2) encodes predefined
documents into vector representations.&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://ai.meta.com/tools/faiss/&quot;&gt;FAISS&lt;/a&gt; is used to index these document embeddings and perform
efficient similarity searches to retrieve the most relevant documents.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Generator: Pre-trained GPT-2 Model
    &lt;ul&gt;
      &lt;li&gt;A Hugging Face GPT-2 text generation pipeline (which can be replaced with
any other model) is used to generate responses based on the retrieved
documents. I chose GPT-2 for this example as it is lightweight enough to
run on my local machine while still generating coherent responses.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Query Processing &amp;amp; Response Generation
    &lt;ul&gt;
      &lt;li&gt;When a query is submitted, the retriever encodes it and searches the FAISS
index for the top-k most similar documents.&lt;/li&gt;
      &lt;li&gt;These retrieved documents are concatenated to form the input context, which
is then passed to the GPT-2 model to generate a response.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Evaluation: &lt;a href=&quot;https://huggingface.co/spaces/evaluate-metric/bleu&quot;&gt;BLEU&lt;/a&gt; (Bilingual Evaluation Understudy) Score Calculation
    &lt;ul&gt;
      &lt;li&gt;To assess the quality of generated responses, we use the BLEU score, a
popular metric for evaluating text generation.&lt;/li&gt;
      &lt;li&gt;The evaluate function takes a query, retrieves documents, generates a
response, and compares it against a ground-truth reference to compute a
BLEU score with smoothing functions from the nltk library.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To run Katib, we will use the &lt;a href=&quot;https://www.kubeflow.org/docs/components/katib/installation/#installing-python-sdk&quot;&gt;Katib SDK&lt;/a&gt;, which provides a programmatic interface for defining and running 
hyperparameter tuning experiments in Kubeflow.&lt;/p&gt;

&lt;p&gt;Katib requires an &lt;a href=&quot;https://www.kubeflow.org/docs/components/katib/user-guides/hp-tuning/configure-experiment/#configuring-the-experiment&quot;&gt;objective&lt;/a&gt; function, which:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Defines what we want to optimize (e.g., BLEU score for text generation quality).&lt;/li&gt;
  &lt;li&gt;Executes the RAG pipeline with different hyperparameter values.&lt;/li&gt;
  &lt;li&gt;Returns an evaluation metric so Katib can compare different hyperparameter configurations.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;objective&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Import dependencies inside the function (required for Katib)
&lt;/span&gt;    &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
    &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;faiss&lt;/span&gt;
    &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sentence_transformers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SentenceTransformer&lt;/span&gt;
    &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;transformers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;
    &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nltk.translate.bleu_score&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sentence_bleu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SmoothingFunction&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# Function to fetch documents (Modify as needed)
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fetch_documents&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Returns a predefined list of documents or loads them from a file.&quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# OR, to load from a file:
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# with open(&quot;/path/to/documents.json&quot;, &quot;r&quot;) as f:
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;#     return json.load(f)
&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Define the RAG pipeline within the function
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;rag_pipeline_execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;top_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;temperature&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Retrieves relevant documents and generates a response using GPT-2.&quot;&quot;&quot;&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Initialize retriever
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;retriever_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SentenceTransformer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;paraphrase-MiniLM-L6-v2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Sample documents
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;documents&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fetch_documents&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Encode documents
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;doc_embeddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;retriever_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;documents&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;faiss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;IndexFlatL2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;doc_embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;doc_embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Encode query and retrieve top-k documents
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;query_embedding&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;retriever_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;distances&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;search&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query_embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;top_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;retrieved_docs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;documents&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Generate response using GPT-2
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;generator&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text-generation&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;gpt2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;gpt2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;context&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;retrieved_docs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;generated&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;generator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_length&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;temperature&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;temperature&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_return_sequences&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;generated&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;generated_text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# TODO: Provide queries and ground truth directly here or load them dynamically from a file/external volume.
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;query&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Example: &quot;Tell me about the Eiffel Tower.&quot;
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;ground_truth&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Example: &quot;The Eiffel Tower is a famous landmark in Paris.&quot;
&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Extract hyperparameters
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;top_k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;top_k&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;temperature&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;temperature&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Generate response
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rag_pipeline_execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;top_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;temperature&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Compute BLEU score
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;reference&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ground_truth&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()]&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Tokenized reference
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;candidate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Tokenized candidate response
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;smoothie&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SmoothingFunction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;method1&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bleu_score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sentence_bleu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reference&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;candidate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;smoothing_function&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;smoothie&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Print BLEU score in Katib-compatible format
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f&quot;BLEU=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bleu_score&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: Make sure to return the result in the format of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;parameter&amp;gt;=&amp;lt;value&amp;gt;&lt;/code&gt;
for Katib’s metrics collector to be able to utilize it. More ways to configure
the output are available in &lt;a href=&quot;https://www.kubeflow.org/docs/components/katib/user-guides/metrics-collector/#pull-based-metrics-collector&quot;&gt;Katib Metrics
Collector&lt;/a&gt; guide.&lt;/p&gt;

&lt;h2 id=&quot;step-3-run-a-katib-experiment&quot;&gt;STEP 3: Run a Katib Experiment&lt;/h2&gt;

&lt;p&gt;Once our pipeline is encapsulated within the objective function, we can configure Katib to optimize the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BLEU&lt;/code&gt; score by 
tuning the hyperparameters:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;top_k&lt;/code&gt;: The number of documents retrieved (eg. between 10 and 20).&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;temperature&lt;/code&gt;: The randomness of text generation (eg. between 0.5 and 1.0).&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;define-hyperparameter-search-space&quot;&gt;Define hyperparameter search space&lt;/h1&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;top_k&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;katib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;search&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;temperature&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;katib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;search&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;double&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Let’s submit the experiment! We’ll use the &lt;a href=&quot;https://github.com/kubeflow/katib/blob/c18035e1041ca1b87ea7eb7c01cb81b5e2b922b3/sdk/python/v1beta1/kubeflow/katib/api/katib_client.py#L178&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tune&lt;/code&gt; API &lt;/a&gt; that will run multiple trials to find the optimal &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;top_k&lt;/code&gt; 
and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;temperature&lt;/code&gt; values for our RAG pipeline.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;katib_client&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;katib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;KatibClient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;kubeflow&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;rag-tuning-experiment&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;katib_client&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;objective&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;objective&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;algorithm_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;grid&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Grid search for hyperparameter tuning
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;objective_metric_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;BLEU&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;objective_type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;maximize&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;objective_goal&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;max_trial_count&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Run up to 10 trials
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;parallel_trial_count&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Run 2 trials in parallel
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;resources_per_trial&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;cpu&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;memory&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;2Gi&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;base_image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;python:3.10-slim&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;packages_to_install&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;transformers==4.36.0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;sentence-transformers==2.2.2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;faiss-cpu==1.7.4&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;numpy==1.23.5&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;huggingface_hub==0.20.0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;nltk==3.9.1&quot;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once the experiment is submitted, we can see output indicating that Katib has started the trials:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-commandline&quot;&gt;Experiment Trials status: 0 Trials, 0 Pending Trials, 0 Running Trials, 0 Succeeded Trials, 0 Failed Trials, 0 EarlyStopped Trials, 0 MetricsUnavailable Trials
Current Optimal Trial:
 {'best_trial_name': None,
 'observation': {'metrics': None},
 'parameter_assignments': None}
Experiment conditions:
 [{'last_transition_time': datetime.datetime(2025, 3, 13, 19, 40, 32, tzinfo=tzutc()),
 'last_update_time': datetime.datetime(2025, 3, 13, 19, 40, 32, tzinfo=tzutc()),
 'message': 'Experiment is created',
 'reason': 'ExperimentCreated',
 'status': 'True',
 'type': 'Created'}]
Waiting for Experiment: kubeflow/rag-tuning-experiment to reach Succeeded condition

.....

Experiment Trials status: 9 Trials, 0 Pending Trials, 2 Running Trials, 7 Succeeded Trials, 0 Failed Trials, 0 EarlyStopped Trials, 0 MetricsUnavailable Trials
Current Optimal Trial:
 {'best_trial_name': 'rag-tuning-experiment-66tmh9g7',
 'observation': {'metrics': [{'latest': '0.047040418725887996',
                              'max': '0.047040418725887996',
                              'min': '0.047040418725887996',
                              'name': 'BLEU'}]},
 'parameter_assignments': [{'name': 'top_k', 'value': '10'},
                           {'name': 'temperature', 'value': '0.6'}]}
Experiment conditions:
 [{'last_transition_time': datetime.datetime(2025, 3, 13, 19, 40, 32, tzinfo=tzutc()),
 'last_update_time': datetime.datetime(2025, 3, 13, 19, 40, 32, tzinfo=tzutc()),
 'message': 'Experiment is created',
 'reason': 'ExperimentCreated',
 'status': 'True',
 'type': 'Created'}, {'last_transition_time': datetime.datetime(2025, 3, 13, 19, 40, 52, tzinfo=tzutc()),
 'last_update_time': datetime.datetime(2025, 3, 13, 19, 40, 52, tzinfo=tzutc()),
 'message': 'Experiment is running',
 'reason': 'ExperimentRunning',
 'status': 'True',
 'type': 'Running'}]
Waiting for Experiment: kubeflow/rag-tuning-experiment to reach Succeeded condition
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can also see the experiments and trials being run to search for the optimized parameter:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-commandline&quot;&gt;kubectl get experiments.kubeflow.org -n kubeflow
NAME                    TYPE      STATUS   AGE
rag-tuning-experiment   Running   True     10m
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-commandline&quot;&gt;kubectl get trials --all-namespaces
NAMESPACE   NAME                             TYPE      STATUS   AGE
kubeflow    rag-tuning-experiment-7wskq9b9   Running   True     10m
kubeflow    rag-tuning-experiment-cll6bt4z   Running   True     10m
kubeflow    rag-tuning-experiment-hzxrzq2t   Running   True     10m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The list of completed trials and their results will be shown in the UI like
below. Steps to access Katib UI are available &lt;a href=&quot;https://www.kubeflow.org/docs/components/katib/user-guides/katib-ui/&quot;&gt;in the documentation&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2025-02-21-katib-rag-optimization/katib_experiment_run.jpeg&quot; alt=&quot;completed_runs&quot; /&gt;
&lt;img src=&quot;/images/2025-02-21-katib-rag-optimization/katib_ui.jpeg&quot; alt=&quot;trial details&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;In this experiment, we leveraged Kubeflow Katib to optimize a
Retrieval-Augmented Generation (RAG) pipeline, systematically tuning key
hyperparameters like top_k and temperature to enhance retrieval precision and
generative response quality.&lt;/p&gt;

&lt;p&gt;For anyone working with RAG systems or hyperparameter optimization, Katib is a
powerful tool—enabling scalable, efficient, and intelligent tuning of machine
learning models! We hope this tutorial helps you streamline hyperparameter
tuning and unlock new efficiencies in your ML workflows!&lt;/p&gt;</content><author><name>Varsha Prasad Narsing (@varshaprasad96)</name></author><category term="katib" /><summary type="html">Introduction</summary></entry><entry><title type="html">Synthetic Data Generation with Kubeflow Pipelines</title><link href="https://blog.kubeflow.org/kfp/2025/02/16/synthetic-data-using-kfp.html" rel="alternate" type="text/html" title="Synthetic Data Generation with Kubeflow Pipelines" /><published>2025-02-16T00:00:00-06:00</published><updated>2025-02-16T00:00:00-06:00</updated><id>https://blog.kubeflow.org/kfp/2025/02/16/synthetic-data-using-kfp</id><content type="html" xml:base="https://blog.kubeflow.org/kfp/2025/02/16/synthetic-data-using-kfp.html">&lt;h3 id=&quot;synthetic-data-generation---why-and-how&quot;&gt;Synthetic Data Generation - Why and How?&lt;/h3&gt;

&lt;p&gt;When creating insights, decisions, and actions from data, the best results come from real data. But accessing real data often requires lengthy security and legal processes. The data may also be incomplete, biased, or too small, and during early exploration, we may not even know if it’s worth pursuing. While real data is essential for proper evaluation, gaps or limited access frequently hinder progress until the formal process is complete.&lt;/p&gt;

&lt;p&gt;To address these challenges, synthetic data provides an alternative. It mimics real data’s statistical properties while preserving privacy and accessibility. Synthetic data generators (synthesizers) are models trained on real data to generate new datasets that follow the same statistical distributions and relationships but do not contain real records. This allows for accelerated development, improved data availability, and enhanced privacy.&lt;/p&gt;

&lt;p&gt;Depending on the technique used, synthetic data not only mirrors statistical base properties of real data but also preserves correlations between features. These synthesizers — such as those based on Gaussian Copulas, Generative Adversarial Networks (GANs), and Variational Autoencoders (VAEs) — enable the creation of high-fidelity synthetic datasets. See more description of these techniques below.&lt;/p&gt;

&lt;h3 id=&quot;key-benefits-of-using-synthetic-data&quot;&gt;Key Benefits of Using Synthetic Data&lt;/h3&gt;

&lt;p&gt;While the above focuses on speed of development in general, and augmentation of data to improve performance of analytical modes, there are more motivations for &lt;em&gt;creating&lt;/em&gt; (synthetic) data:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Enhanced Privacy and Security&lt;/strong&gt;&lt;br /&gt;
Mimics real datasets without containing sensitive or personally identifiable information, mitigating privacy risks and ensuring compliance with regulations like GDPR.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Improved Data Availability&lt;/strong&gt;&lt;br /&gt;
Enables testing and training of models without requiring extensive real-world data collection.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Innovation and Experimentation&lt;/strong&gt;&lt;br /&gt;
Allows safe experimentation with new algorithms and models without exposing sensitive data, fostering rapid prototyping in a secure environment.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Ethical and Responsible AI Development&lt;/strong&gt;&lt;br /&gt;
Ensures training data is free from biases present in real-world datasets, promoting fair and unbiased AI systems.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Accelerated Testing and Deployment&lt;/strong&gt;&lt;br /&gt;
Supports testing of new products, services, and systems in a controlled yet realistic setting, ensuring they are robust, scalable, and ready for real-world use.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Cost Efficiency&lt;/strong&gt;&lt;br /&gt;
Reduces expenses related to data collection, storage, and compliance by eliminating the need for large-scale real-world data acquisition.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Regulatory Compliance Simplification&lt;/strong&gt;&lt;br /&gt;
Helps organizations navigate complex data regulations by offering a compliant alternative to real-world datasets, easing cross-border data transfers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Balanced and Augmented Datasets&lt;/strong&gt;&lt;br /&gt;
Supplements real-world data by balancing underrepresented classes, improving model performance, and reducing biases in AI training.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Resilience Against Data Scarcity&lt;/strong&gt;&lt;br /&gt;
Enables AI development in domains where real-world data is limited, expensive, or difficult to obtain—such as healthcare and cybersecurity—by generating high-quality alternative datasets.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To realize these benefits, we need effective tools for generating synthetic data. Different frameworks exist for this purpose, ranging from cloud-based platforms to open-source solutions. In this post, we focus on &lt;strong&gt;open-source synthetic data generation frameworks&lt;/strong&gt; that provide control, flexibility, and on-premise deployment options.&lt;/p&gt;

&lt;h3 id=&quot;frameworks-for-creating-synthetic-data&quot;&gt;Frameworks for Creating Synthetic Data&lt;/h3&gt;

&lt;p&gt;This post focuses exclusively on open source frameworks.
Some data cannot be sent to the cloud, so some cloud-based synthetic data generation solutions are not always a good fit. 
For data already in cloud, we can use other cloud-based frameworks to generate synthetic data.&lt;/p&gt;

&lt;p&gt;Synthesizers are motivated by multiple factors, but in this context, our focus remains on generating synthetic data for on-premise use.&lt;/p&gt;

&lt;p&gt;So, what framework did we (initially) choose? Currently, we are using the open source version of &lt;a href=&quot;https://sdv.dev/&quot;&gt;SDV&lt;/a&gt;, 
an easy-to-use framework with a strong community and many useful features out-of-the-box (e.g. built-in evaluators, many modeling techniques). 
The field of synthetic data is evolving rapidly. While we do not aim to cover the latest advancements exhaustively, the use of Foundation models is certainly an area of interest.&lt;/p&gt;

&lt;p&gt;One of the most widely used open-source libraries for synthetic data generation is &lt;strong&gt;Synthetic Data Vault (SDV)&lt;/strong&gt;. It provides multiple synthesizers, each tailored for different types of data and statistical properties.&lt;/p&gt;

&lt;h3 id=&quot;the-synthetic-data-vault-sdv&quot;&gt;The Synthetic Data Vault (SDV)&lt;/h3&gt;

&lt;p&gt;When you initialize and fit a synthesizer (like GaussianCopulaSynthesizer, CTGANSynthesizer, etc. - see below), it trains a model based on 
the dataset you provide. This model learns the distribution of the data, capturing the relationships and dependencies between 
different features in the dataset. The synthesizer doesn’t memorize individual records from the dataset. Instead, it tries to learn the underlying statistical patterns, correlations, and distributions present in the data.&lt;/p&gt;

&lt;p&gt;Below are the (free) synthesizers provided by SDV that we evaluated on each use case. Each synthesizer does this differently:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;GaussianCopulaSynthesizer:&lt;/strong&gt; Uses statistical copula functions to model relationships between features, ensuring accurate marginal distributions.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;CTGANSynthesizer:&lt;/strong&gt; Uses Generative Adversarial Networks (GANs) to learn complex data distributions, particularly effective for categorical and mixed-type data.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;TVAESynthesizer:&lt;/strong&gt; Leverages Variational Autoencoders (VAEs) to capture latent representations, useful for continuous and structured data.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;CopulaGANSynthesizer:&lt;/strong&gt; Combines Copula-based statistical modeling with GANs to generate data with complex dependencies.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;PARSynthesizer:&lt;/strong&gt; Uses autoregressive models to generate sequential data while preserving temporal dependencies.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;There are more synthesizers, also from SDV, but not all are open source.&lt;/em&gt; We used the first four, when evaluating optimal synthesizer for our different use cases.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Generators - generating new data - on demand&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Synthesizers are statistical and (more often) AI models trained to mimic the real data. Once developed, the resulting models are used to create as much synthetic data as you find useful for your use case. Once trained, the synthesizer uses the learned model to generate new synthetic data that follows the same statistical properties and distributions as the original dataset, without directly copying any real data points. If you need more data? Just call the generator.&lt;/p&gt;

&lt;h3 id=&quot;evaluation-criteria-for-synthetic-data&quot;&gt;Evaluation Criteria for Synthetic Data&lt;/h3&gt;

&lt;p&gt;But, how good is synthetic data, how do we evaluate it?&lt;/p&gt;

&lt;p&gt;There are many aspects to consider when making use of synthetic data, and it is important to evaluate which synthetic data generation technique (synthesizer) is best for our specific dataset and use case.&lt;/p&gt;

&lt;p&gt;We need to ensure a good balance between:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Usability – How useful is the synthetic data for the intended use case?&lt;/li&gt;
  &lt;li&gt;Fidelity – How well does the synthetic data preserve statistical properties of the real data?&lt;/li&gt;
  &lt;li&gt;Privacy – Does the generated data ensure an acceptable level of privacy for the given use case?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For now, we are focusing only on usability and fidelity, using framework-provided measurements for fidelity and workflows described below to assess usability.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Comments on privacy and privacy preserving techniques&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Ensuring privacy in synthetic data is a non-trivial problem, even if there are techniques to ensure levels of privacy, it remains an active area of research.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Privacy problems, in synthetic data?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;While synthetic data enhances privacy by removing personally identifiable information, it is not inherently risk-free. Some key challenges include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Overfitting and Memorization: If a synthesizer is overfitted, it may generate synthetic records that closely resemble real data, leading to privacy leakage.&lt;/li&gt;
  &lt;li&gt;Anomaly Exposure: Unique individuals or rare events in the dataset (e.g., a very wealthy individual or a rare disease) may be unintentionally replicated in synthetic data, creating a risk of re-identification.&lt;/li&gt;
  &lt;li&gt;Re-identification Attacks: Even if synthetic data is statistically different from real data, attackers may use background knowledge to infer sensitive details about individuals.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One additional problem here is that it might be the anomalies we really are looking for. Currently we are experimenting with various differential privacy strategies, but it is still early days, and we do not focus on them in the examples below.&lt;/p&gt;

&lt;h3 id=&quot;our-on-premise-analytics-platform-arcus&quot;&gt;Our On-Premise Analytics Platform: ARCUS&lt;/h3&gt;

&lt;div style=&quot;display: flex; align-items: center; gap: 20px;&quot;&gt;
  &lt;p&gt;
    ARCUS is Telia’s advanced on-premise analytics platform, designed to support a wide range of use cases.
    The platform provides a Kubeflow-based MLOps environment for descriptive, predictive, generative, and (ongoing) agentic AI. 
    Fully built on open-source, ARCUS integrates a comprehensive stack of components into a unified platform - where Kubernetes is the cornerstone.
  &lt;/p&gt;
&lt;/div&gt;

&lt;h3 id=&quot;needed-environment-to-create-synthetic-data&quot;&gt;Needed environment to create synthetic data&lt;/h3&gt;

&lt;p&gt;For an efficient, automated selection of the best synthesizer, we need a number of things - from the underlying platform with GPUs and MLOps (Kubeflow).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Kubeflow pipelines&lt;/li&gt;
  &lt;li&gt;GPU capabilities (for performance and efficiency)&lt;/li&gt;
  &lt;li&gt;Development (IDE) environment (for framework building and running)&lt;/li&gt;
  &lt;li&gt;Modern data platform (MinIO, Airflow) automating the synthetic data generation datasets&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;parallelism-needed&quot;&gt;Parallelism needed&lt;/h4&gt;

&lt;p&gt;In the (Kube)flows below, we run evaluations in parallel - one for respective synthesizer, followed by a comparison of usability and fidelity scores, selecting the ‘winner’.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; In earlier version of Kubeflow we noticed that the parallelism wasn’t acting as expected, waiting for all threads to complete before moving to next step. We had to create a temporary workaround for this, now solved in Kubeflow Pipelines 2.3.0.&lt;/p&gt;

&lt;p&gt;Below, we briefly describe the base flow for selecting synthesizer, followed by one use case where we use the resulting data generator for ML development in cloud.&lt;/p&gt;

&lt;h2 id=&quot;exploring-the-creation-and-usefulness-of-synthetic-data&quot;&gt;Exploring the Creation and Usefulness of Synthetic Data&lt;/h2&gt;

&lt;p&gt;This is what we want to do: we have a use case, the supporting data, and developed ML model.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;How similar is the synthetic data compared to the real data (interesting for e.g. visualization use cases)?&lt;/li&gt;
  &lt;li&gt;How well do the ML models based on synthetic data keep up with ML models based on real data?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Validation of synthetic data techniques&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Create the synthetic data and save the best synthetic data generator. In this step similarity measures are created by the out of the box SDV framework&lt;/li&gt;
  &lt;li&gt;Create the ML model (in our case classifier model) both on real data and the using the synthetic data. Compare the performance of both models against the same real data testset.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/images/2025-02-16-synthetic-data-using-kfp/image-2.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From above, we have an example where the final synthesizer is collected and saved. This step is used in the example  below, exporting the resulting synthetic data generator to cloud.&lt;/p&gt;

&lt;h2 id=&quot;using-synthetic-data-generators-to-enable-multiple-environments-without-data-transfer&quot;&gt;Using Synthetic Data Generators to Enable Multiple Environments without Data Transfer&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Below is a usecase where we need to make use of both on-premise and cloud, without moving data to cloud.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Problem statement:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Our data cannot be moved from on-premise to cloud.&lt;/li&gt;
  &lt;li&gt;We need extra compute power, in our public cloud environment, to create an ML model for use on-premise.&lt;/li&gt;
  &lt;li&gt;The ML model is to be used on-premise, on new incoming data streams (that cannot be moved to cloud)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Create synthetic data for our on-premise environment use-cases, and - as a side product we save 
away the synthetic data generator (the pickled model used to create synthetic data).&lt;/li&gt;
  &lt;li&gt;Copy the synthetic data generator to cloud&lt;/li&gt;
  &lt;li&gt;Use the synthetic data generator in the cloud, creating synthetic data for training of an ML model&lt;/li&gt;
  &lt;li&gt;Copy the ML model on-premise, and use it for new incoming data&lt;/li&gt;
  &lt;li&gt;Evaluate: Compare the on-premise AI model with the model created in the cloud - against the same test data&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/images/2025-02-16-synthetic-data-using-kfp/image.png&quot; width=&quot;700&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Division of work, what is done on-premise with Kubeflow, and what is done in cloud (AWS SageMaker)?&lt;/strong&gt;&lt;/p&gt;

&lt;h5 id=&quot;on-premise&quot;&gt;On-premise&lt;/h5&gt;

&lt;p&gt;See the above &lt;em&gt;Validation of synthetic data techniques&lt;/em&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Develop the model on real data – for the comparison later with the cloud model.&lt;/li&gt;
  &lt;li&gt;Create synthetic generators, evaluate the generators, and export the best generator to AWS.&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;cloud&quot;&gt;Cloud&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;Use the imported synthetic generator (from on-premise)&lt;/li&gt;
  &lt;li&gt;Create synthetic data using the synthetic data generator&lt;/li&gt;
  &lt;li&gt;Develop the model and determine which synthetic generator is the best&lt;/li&gt;
  &lt;li&gt;Increase the amount of synthetic data, to see if the increase of synthetic data improves model performance (not for sure it will, see below comment)&lt;/li&gt;
  &lt;li&gt;Export model to on-premise&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2025-02-16-synthetic-data-using-kfp/image-3.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In some more detail below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2025-02-16-synthetic-data-using-kfp/image-4.png&quot; width=&quot;700&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;on-premise-1&quot;&gt;On-premise&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;Compare real data model against synthetic data model – using real test data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;In the current examples we see near equivalent performance of the ML models (a few percentage points lower for models created using synthetic data). We experimented with increasing the size of the synthetic dataset, with minor improvements. Augmenting the training data is expected (not tested here) to have more effects when using deep learning algorithms.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;Clearly, the above workflows would be very cumbersome to build and maintain without Kubeflow. 
Our solution is entirely open source, Kubernetes based, and uses Kubeflow and SDV to give us the scalability, robustness, and detailed control that is required.&lt;/p&gt;

&lt;p&gt;The area of synthetic data generation is moving fast with the overall AI field. 
Reports from &lt;a href=&quot;https://www.ibm.com/think/topics/synthetic-data&quot;&gt;IBM&lt;/a&gt; and others, of the increased usage of synthetic data for e.g. LLM training is frequent but the application areas are much greater.
We also expect more capable synthesizers and, hopefully, privacy preserving techniques to keep up with the innovation in this area.
Our original main motivator was speed up in innovation and experimentation, and overall - speed to market. Often a key pain for our teams.&lt;/p&gt;

&lt;p&gt;Looking ahead, we are exploring the development of a synthesizer catalog — ideally integrated into our overall data catalog — to enable users to rapidly experiment with ideas and get started more efficiently.&lt;/p&gt;</content><author><name>&lt;a href='https://www.linkedin.com/in/aaked'&gt;Åke Edlund&lt;/a&gt;, &lt;a href='https://www.linkedin.com/in/tarekabouzeid91'&gt;Tarek Abouzeid&lt;/a&gt;</name></author><category term="kfp" /><summary type="html">Synthetic Data Generation - Why and How?</summary></entry><entry><title type="html">Kubeflow and Me: A Story Started with Push-based Metrics Collection</title><link href="https://blog.kubeflow.org/gsoc-2024-project-6/" rel="alternate" type="text/html" title="Kubeflow and Me: A Story Started with Push-based Metrics Collection" /><published>2024-09-28T00:00:00-05:00</published><updated>2024-09-28T00:00:00-05:00</updated><id>https://blog.kubeflow.org/gsoc-2024-summary-push-basd-metrics-collection</id><content type="html" xml:base="https://blog.kubeflow.org/gsoc-2024-project-6/">&lt;p&gt;This summer, I gained a precious opportunity to participate in the Google Summer of Code(GSoC), in which I would contribute to Katib and fulfill a project named &lt;a href=&quot;https://www.kubeflow.org/events/gsoc-2024/#project-6-push-based-metrics-collection-for-katib&quot;&gt;“Push-based Metrics Collection in Katib”&lt;/a&gt; within 12 weeks. 
Firstly, I got to know about GSoC and Kubeflow with the recommendation from the former active maintainer Ce Gao(gaocegege)’s personal blog. And I was deeply impressed by the idea of cloud native AI toolkits, I decided to dive into this area and learn some skills to enhance my career and future.
In the blog, I’ll provide my personal insight into Katib, for those who are interested in cloud native, AI, and hyperparameters tuning.&lt;/p&gt;

&lt;h2 id=&quot;problem&quot;&gt;Problem&lt;/h2&gt;

&lt;p&gt;The project aims to provide a Python SDK API interface for users to push metrics to Katib DB directly.&lt;/p&gt;

&lt;p&gt;The current implementation of Metrics Collector is pull-based, raising design problems such as determining the frequency at which we scrape the metrics, performance issues like the overhead caused by too many sidecar containers, and restrictions on developing environments that must support sidecar containers and admission webhooks. And also, for data scientists, they need to pay attention to the format of metrics printed in the training scripts, which is error prone and may be hard to recognize.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;We decided to implement a new API for Katib Python SDK to offer users a push-based way to store metrics directly into the Kaitb DB and resolve those issues raised by pull-based metrics collection.&lt;/p&gt;

&lt;p&gt;In the new design, users just need to set &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;metrics_collector_config={&quot;kind&quot;: &quot;Push&quot;}&lt;/code&gt; in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tune()&lt;/code&gt; function and call the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;report_metrics()&lt;/code&gt; API in their objective function to push metrics to Katib DB directly. There are no sidecar containers and restricted metric log formats any more. After that, Trial Controller will continuously collect metrics from Katib DB and update the status of Trial, which is the same as pull-based metrics collection.&lt;/p&gt;

&lt;p&gt;If you are interested in it, please refer to this &lt;a href=&quot;https://www.kubeflow.org/docs/components/katib/user-guides/metrics-collector/#push-based-metrics-collector&quot;&gt;doc&lt;/a&gt; and &lt;a href=&quot;https://github.com/kubeflow/katib/blob/master/examples/v1beta1/sdk/mnist-with-push-metrics-collection.ipynb&quot;&gt;example&lt;/a&gt; for more details.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/2024-09-28-gsoc-2024-summary-push-based-metrics-collection/push-based-metrics-collection.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;my-contributions-during-the-gsoc&quot;&gt;My Contributions during the GSoC&lt;/h2&gt;

&lt;p&gt;I raised numerous PRs for the Katib and Training-Operator project. Some of them are related to my GSoC project, and others may contribute to the completeness of UTs (Unit Tests), simplicity of dependency management, and the compatibility of the UI component.&lt;/p&gt;

&lt;p&gt;For reference, the coding period can be rougly divided into 3 stages:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Convert the proposal to a KEP and discuss the architecture, API design, etc. (~4 weeks) with the mentors&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Develop a push-based metrics collection interface according to the KEP. (~8 weeks)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Write some examples and documentation &amp;amp; Present my work to the Kubeflow Community.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Also, I raised some issues not only to describe the problems and bugs I met during the coding period, but also to suggest the future enhancement direction for Katib and the Training-Operator.&lt;/p&gt;

&lt;p&gt;There is a &lt;a href=&quot;https://github.com/kubeflow/katib/issues/2340&quot;&gt;Github Issue&lt;/a&gt; tracks the progress of developing push-based metrics collection for katib during the GSoC coding phase. If you are interested in my work or Katib, please can check this issue for more details.&lt;/p&gt;

&lt;h2 id=&quot;lessons-learned&quot;&gt;Lessons Learned&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Think Twice, Code Once&lt;/strong&gt;: Andrey taught me that we should think of the API specification and all the related details before coding. This can significantly reduce the workload of the coding period and avoid big refactor of the project. Meanwhile, my understanding of Katib got clear gradually during the over-and-over rounds of re-think and re-design of the architecture.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Dive into the Source Code&lt;/strong&gt;: Engineering projects nowadays are extremely complex and need much effort to understand them. The best way to get familiar with the project is to dive into the source code and run several examples.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Communication&lt;/strong&gt;: Communication is the most important thing when collaborating with others. Expressing your idea precisely and making others understand you easily are significant skills not only in the open source community but also in various scenarios such as at a company and in group work.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;in-the-end&quot;&gt;In the End&lt;/h2&gt;

&lt;p&gt;Special Thanks:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;To my mentors &lt;a href=&quot;https://github.com/andreyvelich&quot;&gt;@andreyvelich&lt;/a&gt; &lt;a href=&quot;https://github.com/johnugeorge&quot;&gt;@johnugeorge&lt;/a&gt; &lt;a href=&quot;https://github.com/tenzen-y&quot;&gt;@tenzen-y&lt;/a&gt;, especially to Andrey. Your great knowledge about the code base and the industry impressed me a lot. Thanks for your timely response to my PRs and for always attending the weekly meetings to solve my pending problems, from which I benefited a lot. What’s more, I can well remember that, in that night, you explained the usage of Kubeflow in the industry to me with greate patience, and encouraged me not to doubt about myself, just do it and explore more, contribute more. You ignite the flame of my desire to contribute to cloud native AI.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To &lt;a href=&quot;https://github.com/gaocegege&quot;&gt;@gaocegege&lt;/a&gt;. You recommend me to the Kubeflow Community. Thanks for your patient answers for my endless silly questions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To Google. Thanks for offering such a precious opportunity for me to begin my journey in the open source world!&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I hold a firm belief that every small step counts, and everybody in the community is unique and of great significance. There is no doubt that our joint efforts will surely contribute to the flourishing of our Kubeflow Community, make it the world-best community managing AI lifecycle on Kubernetes, and attract much more attention from the industry. Then, more and more new comers will pour in and work along with us.&lt;/p&gt;

&lt;p&gt;Again, I’ll continue to contribute to Kubeflow.&lt;/p&gt;

&lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt;

&lt;p&gt;For more details about Kubeflow and the upcoming GSoC’25 event, please check:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kubeflow.org&quot;&gt;What is Kubeflow?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kubeflow.org/events/gsoc-2025/&quot;&gt;Kubeflow GSoC’25 Event&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Shao Wang(Electronic-Waste)</name></author><category term="gsoc" /><summary type="html">This summer, I gained a precious opportunity to participate in the Google Summer of Code(GSoC), in which I would contribute to Katib and fulfill a project named “Push-based Metrics Collection in Katib” within 12 weeks. Firstly, I got to know about GSoC and Kubeflow with the recommendation from the former active maintainer Ce Gao(gaocegege)’s personal blog. And I was deeply impressed by the idea of cloud native AI toolkits, I decided to dive into this area and learn some skills to enhance my career and future. In the blog, I’ll provide my personal insight into Katib, for those who are interested in cloud native, AI, and hyperparameters tuning.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.kubeflow.org/images/logo.png" /><media:content medium="image" url="https://blog.kubeflow.org/images/logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">LLM Hyperparameter Optimization API: My Google Summer of Code Journey with Kubeflow</title><link href="https://blog.kubeflow.org/gsoc-2024-project-4/" rel="alternate" type="text/html" title="LLM Hyperparameter Optimization API: My Google Summer of Code Journey with Kubeflow" /><published>2024-09-19T00:00:00-05:00</published><updated>2024-09-19T00:00:00-05:00</updated><id>https://blog.kubeflow.org/gsoc-2024-summary-llm-hyperparameter-optimization-api</id><content type="html" xml:base="https://blog.kubeflow.org/gsoc-2024-project-4/">&lt;p&gt;This summer, I had the opportunity to participate in the Google Summer of Code (GSoC) program, where I contributed to Kubeflow, an open-source machine learning toolkit. My project focused on developing a high-level API for optimizing hyperparameters in Large Language Models (LLMs) within Katib, Kubeflow’s automated hyperparameter tuning system. I’d like to share insights from this experience with others interested in Kubeflow, GSoC, or optimizing LLMs.&lt;/p&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;The rapid advancements and rising popularity of LLMs, such as GPT and BERT, have created a growing demand for efficient LLMOps in Kubernetes. To address this, we have developed a &lt;a href=&quot;https://www.kubeflow.org/docs/components/training/user-guides/fine-tuning/&quot;&gt;train API&lt;/a&gt; within the Training Python SDK, simplifying the process of fine-tuning LLMs using distributed PyTorchJob workers. However, hyperparameter optimization remains a crucial yet labor-intensive task for enhancing model performance.&lt;/p&gt;

&lt;h2 id=&quot;goal&quot;&gt;Goal&lt;/h2&gt;

&lt;p&gt;Hyperparameter optimization is essential but time-consuming, especially for LLMs with billions of parameters. This API simplifies the process by handling Kubernetes infrastructure, allowing data scientists to focus on model performance rather than system configuration.&lt;/p&gt;

&lt;p&gt;With this API, users can import pretrained models and datasets from Hugging Face and Amazon S3, define parameters including the hyperparameter search space, optimization objective, and resource configuration. The API then automates the creation of Experiment, which contains multiple Trials with different hyperparameter settings using PyTorch distributed training. It then collects and analyzes the metrics from each Trial to identify the optimal hyperparameter configuration.&lt;/p&gt;

&lt;p&gt;For detailed instruction on using the API, please refer to this &lt;a href=&quot;https://github.com/kubeflow/website/blob/b253c9402be94e7c5c044a0b1d2d9d86fd473149/content/en/docs/components/katib/user-guides/llm-hp-optimization.md&quot;&gt;guide&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/2024-09-19-gsoc-2024-llm-hyperparameter-optimization-api/design_tune_api.png&quot; alt=&quot;Design of API&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;my-contributions-to-the-gsoc-project&quot;&gt;My Contributions to the GSoC Project&lt;/h2&gt;

&lt;p&gt;My work on the project can be broadly divided into four stages:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Stage 1&lt;/strong&gt;: Designing the API, drafting the project proposal, and refining it into a Kubeflow Enhancement Proposal (KEP).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Stage 2&lt;/strong&gt;: Developing and implementing the high-level API.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Stage 3&lt;/strong&gt;: Implementing unit tests and end-to-end tests for the API.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Stage 4&lt;/strong&gt;: Creating documentation and presenting the work to the Kubeflow community.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition, I addressed several critical bugs in previous Katib and Training Operator releases and contributed new features, such as writing end-to-end tests for the train API.&lt;/p&gt;

&lt;p&gt;For those interested, here is a &lt;a href=&quot;https://github.com/kubeflow/katib/issues/2339&quot;&gt;detailed summary&lt;/a&gt; of all the pull requests I submitted during this process.&lt;/p&gt;

&lt;h2 id=&quot;lessons-learned&quot;&gt;Lessons Learned&lt;/h2&gt;

&lt;p&gt;This is my first experience contributing to an open source project, and I gained extensive technical knowledge throughout this project, including Docker, Kubernetes, and Kubeflow itself. Before developing and implementing the API, I invested significant time onboarding and familiarizing myself with Kubeflow. The &lt;a href=&quot;https://www.kubeflow.org/docs/&quot;&gt;official documentation&lt;/a&gt; and &lt;a href=&quot;https://github.com/kubeflow&quot;&gt;GitHub repository&lt;/a&gt; were invaluable resources during this process.&lt;/p&gt;

&lt;p&gt;Beyond these technical skills, I also learned several key lessons that extend into broader personal and professional growth.&lt;/p&gt;

&lt;h3 id=&quot;think-from-the-users-perspective&quot;&gt;Think from the User’s Perspective&lt;/h3&gt;

&lt;p&gt;One key lesson was the importance of considering the user’s needs. Discussing API design with my mentors taught me to focus on what functionalities users need and how they prefer to use them. Listening to users’ feedback is crucial for effective product design.&lt;/p&gt;

&lt;h3 id=&quot;dont-fear-bugs&quot;&gt;Don’t Fear Bugs&lt;/h3&gt;

&lt;p&gt;I used to feel overwhelmed by bugs and unsure how to tackle them. When a bug caused a container failure during a Katib trial, my mentor guided me through the debugging process, teaching me how to systematically trace and understand the issue. The key is to approach debugging methodically and think through each step of the problem.&lt;/p&gt;

&lt;h3 id=&quot;communication-is-important&quot;&gt;Communication is Important&lt;/h3&gt;

&lt;p&gt;Communication is important in collaboration, especially in open source projects. There are various ways of communicating in open-source projects, such as GitHub issues or PRs, Slack, and community meetings. And I’m grateful to my mentor for discussing my challenges during our weekly meetings and providing invaluable guidance.&lt;/p&gt;

&lt;h3 id=&quot;every-contribution-counts&quot;&gt;Every Contribution Counts&lt;/h3&gt;

&lt;p&gt;Initially, I thought contributing to open source was complex. I learned that every contribution, no matter how small, is valuable and appreciated. For example, contributing to documentation is crucial, especially for newcomers.&lt;/p&gt;

&lt;h2 id=&quot;in-the-end&quot;&gt;In The End&lt;/h2&gt;

&lt;p&gt;I am deeply grateful to everyone who supported me throughout this project. Your suggestions, advice, and encouragement were invaluable in helping me complete my work.&lt;/p&gt;

&lt;p&gt;I especially want to extend my heartfelt thanks to my mentor, Andrey Velichkevich. His deep knowledge of both the project and the industry, combined with his willingness to help, has been incredibly inspiring. I greatly appreciate the time and effort he dedicated to guiding me, from the high-level design of the API to the finer details like code formatting. I have learned so much from his mentorship.&lt;/p&gt;

&lt;p&gt;Looking ahead, I am excited to continue contributing to Kubeflow. I also look forward to helping future contributors by improving documentation and sharing my experiences with newcomers in the community.&lt;/p&gt;

&lt;p&gt;If you’re interested in open-source and want to be part of Kubeflow, GSoC 2025 applications are now open! Check out the details &lt;a href=&quot;https://www.kubeflow.org/events/gsoc-2025/&quot;&gt;here&lt;/a&gt;—we’d love to have you join us!&lt;/p&gt;</content><author><name>Hezhi(Helen) Xie</name></author><category term="gsoc" /><summary type="html">This summer, I had the opportunity to participate in the Google Summer of Code (GSoC) program, where I contributed to Kubeflow, an open-source machine learning toolkit. My project focused on developing a high-level API for optimizing hyperparameters in Large Language Models (LLMs) within Katib, Kubeflow’s automated hyperparameter tuning system. I’d like to share insights from this experience with others interested in Kubeflow, GSoC, or optimizing LLMs.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.kubeflow.org/images/logo.png" /><media:content medium="image" url="https://blog.kubeflow.org/images/logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Kubeflow 1.9: New Tools for Model Management and Training Optimization</title><link href="https://blog.kubeflow.org/kubeflow-1.9-release/" rel="alternate" type="text/html" title="Kubeflow 1.9: New Tools for Model Management and Training Optimization" /><published>2024-07-22T00:00:00-05:00</published><updated>2024-07-22T00:00:00-05:00</updated><id>https://blog.kubeflow.org/kubeflow-1.9-release</id><content type="html" xml:base="https://blog.kubeflow.org/kubeflow-1.9-release/">&lt;p&gt;Kubeflow 1.9 significantly simplifies the development, tuning and management of secure machine learning models and LLMs. Highlights include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Model Registry&lt;/strong&gt;: Centralized management for ML models, versions, and artifacts.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fine-Tune APIs for LLMs&lt;/strong&gt;: Simplifies fine-tuning of LLMs with custom datasets.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Pipelines&lt;/strong&gt;: Consolidation of Tekton and Argo Workflows backends for improved flexibility.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Security Enhancements&lt;/strong&gt;: Network policies, Oauth2-proxy, and CVE scanning.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Integration Upgrades&lt;/strong&gt;: Improved integrations with Ray, Seldon, BentoML, and KServe for LLM GPU optimizations.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Installation and Documentation&lt;/strong&gt;: Streamlined installation, updated platform dependencies, and enhanced documentation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These updates aim to simplify workflows, improve integration dependencies, and provide Kubernetes-native operational efficiencies for enterprise scale, security, and isolation.&lt;/p&gt;

&lt;h2 id=&quot;model-registry&quot;&gt;Model Registry&lt;/h2&gt;

&lt;p&gt;A model registry provides a central catalog for ML model developers to index and manage models, versions, and ML artifacts metadata. It fills a gap between model experimentation and production activities. It provides a central interface for all stakeholders in the ML lifecycle to collaborate on ML models. Model registry has been &lt;a href=&quot;https://blog.kubeflow.org/kubeflow-user-survey-2023/#:~:text=lifecycle%2C%20followed%20by-,model%20registry%20(44%25),-and%20initial%20setup&quot;&gt;asked by the community&lt;/a&gt; for a long time and we are delighted to introduce it to the Kubeflow ecosystem.&lt;/p&gt;

&lt;p&gt;This initial release includes REST APIs and a Python SDK to track model artifacts and model metadata with a standardized format that can be reused across Kubeflow components, such as to deploy Inference Servers. You can get started by following the &lt;a href=&quot;https://www.kubeflow.org/docs/components/model-registry/overview/&quot;&gt;Model Registry tutorial on the Kubeflow website&lt;/a&gt;, or see a short &lt;a href=&quot;https://www.youtube.com/watch?v=JVxUTkAKsMU&quot;&gt;demo video&lt;/a&gt; of the Model Registry in action.&lt;/p&gt;

&lt;p&gt;We are just getting started. This is an Alpha version and we look forward to feedback. The &lt;a href=&quot;https://docs.google.com/document/d/1DmMhcae081SItH19gSqBpFtPfbkr9dFhSMCgs-JKzNo/edit&quot;&gt;model registry working group&lt;/a&gt; meets biweekly: you can provide feedback by joining the meeting or directly on the &lt;a href=&quot;https://github.com/kubeflow/model-registry/issues&quot;&gt;repository&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;fine-tune-apis-for-llms&quot;&gt;Fine-Tune APIs for LLMs&lt;/h2&gt;

&lt;p&gt;In the rapidly evolving ML/AI landscape, the ability to fine-tune pre-trained models represents a significant leap towards achieving custom solutions with less effort and time. Fine-tuning with custom datasets allows practitioners to adapt large language models (LLMs) to their specific needs.&lt;/p&gt;

&lt;p&gt;However, fine-tuning tasks often require extensive manual intervention, including the configuration of training environments and the distribution of data across nodes. The new &lt;a href=&quot;https://www.kubeflow.org/docs/components/training/user-guides/fine-tuning/&quot;&gt;Fine-Tune API&lt;/a&gt; aims to simplify this process, offering an easy-to-use Python interface that abstracts away the complexity involved in setting up and executing fine-tuning tasks on distributed systems.&lt;/p&gt;

&lt;p&gt;By providing this API, Training Operator not only simplifies the user experience for ML practitioners but also leverages its existing infrastructure for distributed training. You can take advantage of Kubernetes’ ability to dynamically schedule GPU, thus saving on compute resources and cost. Training Operator also gives you fault tolerant guarantees, to save your training procedures from cluster node failures.&lt;/p&gt;

&lt;h2 id=&quot;pipelines&quot;&gt;Pipelines&lt;/h2&gt;

&lt;h3 id=&quot;v1-feature-parity&quot;&gt;v1 Feature Parity&lt;/h3&gt;

&lt;p&gt;We made significant progress towards KFPv1 feature parity by adding more Kubernetes resources to the Pipelines code with the new kfp-kubernetes 1.2.0 Python package. We encourage every KFP user to test the new V2 functionality and plan your migration from V1 to V2. We still have some outstanding features that need to be ported over to V2, please help us to identify what’s missing by openining a new issue in the KFP repository.&lt;/p&gt;

&lt;h3 id=&quot;argo-workflows-and-tekton-backends-consolidation&quot;&gt;Argo Workflows and Tekton Backends Consolidation&lt;/h3&gt;

&lt;p&gt;The Pipelines Tekton backend has been &lt;a href=&quot;https://github.com/kubeflow/pipelines/pull/10678&quot;&gt;merged&lt;/a&gt; into the main Kubeflow Pipelines repository. You can now choose what workflow engine to use from the same Pipelines version. This proves the extensibility and flexibility of the KFP v2 architecture, which encourages other contributors to bring support for other workflow engines.&lt;/p&gt;

&lt;p&gt;Both Argo Workflows and Tekton provide unique advantages. Argo Workflows is known for its simplicity and ease of use, making it a popular choice for many users. Tekton offers extensive customization options with its pipeline definitions and reusable components, which can be advantageous for integrating into various CI/CD systems. Depending on your specific requirements and preferences, you can leverage the strengths of either Argo Workflows or Tekton to optimize your machine learning workflows.&lt;/p&gt;

&lt;p&gt;In this &lt;a href=&quot;https://developer.ibm.com/blogs/awb-tekton-optimizations-for-kubeflow-pipelines-2-0/&quot;&gt;blog post&lt;/a&gt;, you can find more details about the benefits of running KFP with either Tekton or Argo Workflows.&lt;/p&gt;

&lt;h3 id=&quot;argo-workflows-upgrade&quot;&gt;Argo Workflows Upgrade&lt;/h3&gt;

&lt;p&gt;Kubeflow Pipelines’s Argo Workflows backend is &lt;a href=&quot;https://github.com/kubeflow/pipelines/issues/10469&quot;&gt;upgraded to 3.4.16&lt;/a&gt;. This upgrade moves the supported version closer to the latest upstream version and brings lots of CVE resolutions. The previous minor version was no longer being patched by the Argo community, so lots of security issues had accumulated over time.&lt;/p&gt;

&lt;h2 id=&quot;katib&quot;&gt;Katib&lt;/h2&gt;

&lt;p&gt;Kubeflow 1.9 ships with Katib 0.17, which brings &lt;a href=&quot;https://github.com/kubeflow/katib/pull/2315&quot;&gt;official support&lt;/a&gt; for ARM64, getting us one step closer to full ARM64 coverage.&lt;/p&gt;

&lt;p&gt;For Data Scientists who submit training jobs with the Python SDK, you can now set the &lt;a href=&quot;https://github.com/kubeflow/katib/pull/2227&quot;&gt;algorithm settings&lt;/a&gt; and &lt;a href=&quot;https://github.com/kubeflow/katib/pull/2235&quot;&gt;environment variables&lt;/a&gt; from the tune method. Previously, you had to rely directly on Kubernetes CRD submission for these. You can also take advantage of the latest features from TensorFlow 2.16 and PyTorch 2.2. The team also worked to resolve &lt;a href=&quot;https://github.com/kubeflow/katib/issues/2346&quot;&gt;environmental conflicts&lt;/a&gt; that prevented the Katib Python SDK to be installed alongside the Kubeflow Python SDK.&lt;/p&gt;

&lt;p&gt;There are tons of additional improvements and bug fixes. Check out the full changelog &lt;a href=&quot;https://github.com/kubeflow/katib/blob/master/CHANGELOG.md&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;central-dashboard&quot;&gt;Central Dashboard&lt;/h2&gt;

&lt;p&gt;This release bring several improvements to the Kubeflow Central Dashboard, including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Styling improvements to the sidebar, including &lt;a href=&quot;https://github.com/kubeflow/kubeflow/pull/7583&quot;&gt;grouping&lt;/a&gt; all Kubeflow Pipelines links to reduce clutter&lt;/li&gt;
  &lt;li&gt;Significant &lt;a href=&quot;https://github.com/kubeflow/kubeflow/pull/7582&quot;&gt;improvements&lt;/a&gt; to the “manage contributors” page, including the ability to manage contributors for all profiles that you are the owner of, and see which profiles you have access to, even when you are not the owner&lt;/li&gt;
  &lt;li&gt;Allow external services to &lt;a href=&quot;https://github.com/kubeflow/kubeflow/pull/7138&quot;&gt;parse&lt;/a&gt; the current profile (namespace) by sending the namespace selector value to non-iframed applications&lt;/li&gt;
  &lt;li&gt;Significant &lt;a href=&quot;https://github.com/kubeflow/kubeflow/pull/7578&quot;&gt;updates&lt;/a&gt; to dependencies to reduce CVEs&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../images/2024-07-22-kubeflow-1.9-release/dashboard.png&quot; alt=&quot;Kubeflow notebook images&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;notebooks&quot;&gt;Notebooks&lt;/h2&gt;

&lt;p&gt;With this release, we provide &lt;a href=&quot;https://github.com/kubeflow/kubeflow/pull/7590&quot;&gt;significant updates&lt;/a&gt; to all example notebook images including PyTorch 2.3.0, Tensorflow 2.15.1 and many other library updates. While you can continue to use the old images, we recommend updating to use the greatest and latest ML libraries.&lt;/p&gt;

&lt;p&gt;Additionally, notebooks images now run with a &lt;a href=&quot;https://github.com/kubeflow/kubeflow/pull/7622&quot;&gt;non-root SecurityContext&lt;/a&gt;, allowing for an improved security.&lt;/p&gt;

&lt;p&gt;Take a look at the &lt;a href=&quot;https://github.com/kubeflow/kubeflow/releases/tag/v1.9.0&quot;&gt;changelog&lt;/a&gt; for a full list of bug fixes and improvements.&lt;/p&gt;

&lt;p&gt;While this release was light on new Notebooks features, the Working Group is hard at work on an exciting new project: we are actively developing Notebooks V2, with contributions from various companies, in the &lt;a href=&quot;https://github.com/kubeflow/notebooks/tree/notebooks-v2&quot;&gt;new repository&lt;/a&gt;. Take a look &lt;a href=&quot;https://github.com/kubeflow/kubeflow/issues/7156&quot;&gt;here&lt;/a&gt; and join our Working Group meetings to get involved!&lt;/p&gt;

&lt;h2 id=&quot;kubeflow-platform-security-and-manifests&quot;&gt;Kubeflow Platform (Security and Manifests)&lt;/h2&gt;

&lt;h3 id=&quot;security&quot;&gt;Security&lt;/h3&gt;

&lt;h4 id=&quot;network-policies&quot;&gt;Network Policies&lt;/h4&gt;

&lt;p&gt;Network policies are enabled for the Kubeflow core services as a second layer of defense before Istio authorization policies. This gives administrators a better network overview and segmentation while also enforcing common enterprise security guidelines.
You can read more about the current implementation and architecture &lt;a href=&quot;https://github.com/kubeflow/manifests/tree/master/common/networkpolicies&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;authentication&quot;&gt;Authentication&lt;/h4&gt;

&lt;p&gt;Oauth2-proxy replaces oidc-authservice, which brings improved token-based authentication. Machine Learning engineers can now use tokens instead of insecure passwords for CI/CD automation of Kubeflow deployment and maintenance (e.g. using GitHub actions). 
You can read more about the current implementation and architecture &lt;a href=&quot;https://github.com/kubeflow/manifests/tree/master/common/oidc-client/oauth2-proxy&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;cve-scanning&quot;&gt;CVE Scanning&lt;/h4&gt;

&lt;p&gt;With this release we are introducing &lt;a href=&quot;https://github.com/kubeflow/manifests/blob/master/hack/trivy_scan.py&quot;&gt;automated CVE scanning&lt;/a&gt; with &lt;a href=&quot;https://github.com/aquasecurity/trivy&quot;&gt;Trivy&lt;/a&gt; on the manifests &lt;a href=&quot;https://github.com/kubeflow/manifests/blob/master/.github/workflows/trivy.yaml&quot;&gt;master branch&lt;/a&gt;. We appreciate contributions to reduce the number of CVEs, the Security Working Group needs help to build a more secure platform. You can find more details about our security scanning process and disclosure policy here. Here are is a summary from June 25th:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/2024-07-22-kubeflow-1.9-release/CVE_table.png&quot; alt=&quot;Kubeflow notebook images&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can find a detailed Security WG roadmap &lt;a href=&quot;https://github.com/kubeflow/manifests/issues/2598&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;manifests&quot;&gt;Manifests&lt;/h3&gt;

&lt;h4 id=&quot;installation-and-documentation-improvements&quot;&gt;Installation and documentation improvements&lt;/h4&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/kubeflow/manifests?tab=readme-ov-file#upgrading-and-extending&quot;&gt;documentation&lt;/a&gt; has been improved and now contains guidelines for upgrading and extending the Kubeflow Platform for administrators. New users can now install kubeflow on their laptop in just a few minutes&lt;/p&gt;

&lt;p&gt;Platform dependencies updates:&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Component&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;a href=&quot;https://kubernetes.io/releases/&quot;&gt;Kubernetes&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;a href=&quot;https://github.com/kubernetes-sigs/kustomize/releases&quot;&gt;Kustomize&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;a href=&quot;https://istio.io/latest/news/releases/&quot;&gt;Istio&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;a href=&quot;https://github.com/dexidp/dex/releases&quot;&gt;Dex&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;a href=&quot;https://cert-manager.io/docs/installation/supported-releases/&quot;&gt;Cert-Manager&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;a href=&quot;https://knative.dev/docs/reference/relnotes/&quot;&gt;Knative&lt;/a&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;KF 1.9 Version&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;1.27 - 1.29
   &lt;/td&gt;
   &lt;td&gt;5.2.1+
   &lt;/td&gt;
   &lt;td&gt;1.22.1
   &lt;/td&gt;
   &lt;td&gt;2.39.1
   &lt;/td&gt;
   &lt;td&gt;1.14.5
   &lt;/td&gt;
   &lt;td&gt;1.12.4
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt; Kubernetes 1.30+ is also expected to work, but was not officially tested.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt; Kustomize 5.2.1+ support with way less warnings. So platform engineers have a modern and supported installation tool chain.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;integration-with-third-party-ml-tools&quot;&gt;Integration with third-party ML tools&lt;/h4&gt;

&lt;p&gt;We have updated the third-party components in /contrib to provide integration with the broader ML ecosystem.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/kubeflow/manifests/tree/master/contrib/bentoml&quot;&gt;BentoML&lt;/a&gt; 1.2.28 and 1.1.21&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/kubeflow/manifests/tree/master/contrib/seldon&quot;&gt;Seldon&lt;/a&gt; 1.18.1&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/kubeflow/manifests/tree/master/contrib/ray&quot;&gt;Ray&lt;/a&gt; 2.23 and Kuberay 1.1.1&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Find a detailed Manifests WG roadmap &lt;a href=&quot;https://github.com/kubeflow/manifests/issues/2592&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;kserve&quot;&gt;KServe&lt;/h2&gt;

&lt;p&gt;We upgraded to &lt;a href=&quot;https://kserve.github.io/website/0.13/blog/articles/2024-05-15-KServe-0.13-release/&quot;&gt;KServe 0.13&lt;/a&gt;. This release includes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Enhanced Hugging Face Runtime Support&lt;/strong&gt;: Hugging Face models are supported out-of-the-box, implementing a &lt;a href=&quot;https://github.com/kserve/kserve/tree/master/python/huggingfaceserver&quot;&gt;KServe Hugging Face Serving Runtime&lt;/a&gt;. Currently supported tasks include sequence classification, token classification, fill mask, text generation, and text to text generation.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;vLLM Support&lt;/strong&gt;: Dedicated runtime support for &lt;a href=&quot;https://docs.vllm.ai/en/latest/&quot;&gt;vLLM&lt;/a&gt; is now included, streamlining the deployment process for LLMs.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;OpenAI Schema Integration&lt;/strong&gt;: KServe now supports endpoints for generative transformer models, following the OpenAI protocol.  This enables KServe to be used directly with OpenAI’s client libraries or third-party tools like LangChain and LlamaIndex.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;documentation&quot;&gt;Documentation&lt;/h2&gt;

&lt;p&gt;MLOps is a complex subject and users have asked for clear, up-to-date and comprehensive documentation. We are happy to announce that we started a restructuring process to better align the various components’ docs to have a similar structure. We are revamping our docs to better align with user expectations and how you expect technical docs to be organized. We will continue to improve the quality and completeness of our docs, by adding new user guides, tutorials, and reference architecture topics.&lt;/p&gt;

&lt;p&gt;We are looking for new members who can help us craft complete and high quality documentation. Please get involved by &lt;a href=&quot;https://github.com/kubeflow/website/pulls&quot;&gt;opening and reviewing PRs&lt;/a&gt; in the Kubeflow website.&lt;/p&gt;

&lt;h2 id=&quot;honorable-mentions&quot;&gt;Honorable Mentions&lt;/h2&gt;

&lt;h3 id=&quot;google-spark-operator-migration-to-kubeflow&quot;&gt;Google Spark Operator migration to Kubeflow&lt;/h3&gt;

&lt;p&gt;We’re excited to announce the migration of Google’s Spark Operator to the Kubeflow Spark Operator, marking the launch of a significant addition to the Kubeflow ecosystem. The Kubeflow Spark Operator simplifies the deployment and management of Apache Spark applications on Kubernetes. This announcement isn’t just about a new piece of technology, it’s about building a stronger, open-governed, and more collaborative community around Spark on Kubernetes.&lt;/p&gt;

&lt;p&gt;Kubeflow Spark Operator is not yet officially included in the Kubeflow release, but you can install it by following the instructions &lt;a href=&quot;https://www.kubeflow.org/docs/components/spark-operator/getting-started/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Read more about Kubeflow Spark operator in the announcement &lt;a href=&quot;https://blog.kubeflow.org/operators/2024/04/15/kubeflow-spark-operator.html&quot;&gt;blog post&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;google-summer-of-code&quot;&gt;Google Summer of Code&lt;/h3&gt;

&lt;p&gt;This year, Kubeflow was excited to participate in Google Summer of Code (GSoC), attracting a wave of enthusiastic students! Over 250 students joined our Slack channel, eager to learn about contributing to the Kubeflow community and crafting impactful proposals.&lt;/p&gt;

&lt;p&gt;We were also fortunate to have a dedicated group of 20 mentors ready to guide these talented individuals. From a pool of nearly 70 proposals, we have selected 10 proposals and were awarded with 8 outstanding students by Google. They are now actively contributing to various Kubeflow features, making a real difference in various Kubeflow components.&lt;/p&gt;

&lt;p&gt;We’ll be following their progress and sharing their accomplishments through a series of blog posts in the future, so stay tuned! A big thank you to all the mentors and students who are making Kubeflow’s GSoC 2024 a huge success!&lt;/p&gt;

&lt;p&gt;Thanks to all the studets: Adem Baccara, Biswajit Pattnaik, Hansini Karunarathne, Hezhi Xie, Sandipan Panda, Shao Wang, Shashank Mittal, SIVASUBRAMANIAM L. Visit &lt;a href=&quot;https://summerofcode.withgoogle.com/programs/2024/organizations/kubeflow&quot;&gt;this page&lt;/a&gt; for more details about each project and respective mentors.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next&lt;/h2&gt;

&lt;p&gt;The community continues to see growth, especially with the ever growing interest from the CloudNative community in AI topics. We have recently elected the first Kubeflow Steering Committee. This is the first step towards a more mature governance structure and a democratic and open community.&lt;/p&gt;

&lt;p&gt;If you want to take a peek into the Kubeflow 1.10 roadmap planning and contribute with your ideas, see &lt;a href=&quot;https://github.com/kubeflow/kubeflow/issues/7459&quot;&gt;Notebooks&lt;/a&gt;, &lt;a href=&quot;https://github.com/kubeflow/manifests/issues/2763&quot;&gt;Manifests &amp;amp; Security&lt;/a&gt;, &lt;a href=&quot;https://github.com/kubeflow/pipelines/discussions/10908&quot;&gt;Pipelines&lt;/a&gt;, &lt;a href=&quot;https://github.com/kubeflow/model-registry/issues/175&quot;&gt;Model Registry&lt;/a&gt;, &lt;a href=&quot;https://github.com/kubeflow/katib/issues/2386&quot;&gt;Katib&lt;/a&gt;, &lt;a href=&quot;https://github.com/kubeflow/training-operator/issues/2169&quot;&gt;Training Operator&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;how-to-get-started-with-19&quot;&gt;How to get started with 1.9&lt;/h2&gt;

&lt;p&gt;Visit the Kubeflow 1.9 &lt;a href=&quot;https://www.kubeflow.org/docs/releases/kubeflow-1.9/&quot;&gt;release page&lt;/a&gt; or head over to the &lt;a href=&quot;https://www.kubeflow.org/docs/started/&quot;&gt;Getting Started&lt;/a&gt; section to learn more about installation, architecture and quick start examples.&lt;/p&gt;

&lt;h2 id=&quot;join-the-community&quot;&gt;Join the Community&lt;/h2&gt;

&lt;p&gt;We would like to thank everyone for their contribution to Kubeflow 1.9, especially Ricardo Martinelli De Oliveira for his work as the v1.9 Release Manager, all the release team and the working group leads, who relentlessly dedicate their time to this great project.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Release team members&lt;/em&gt;: Ricardo Martinelli De Oliveira, Stefano Fioravanzo, Helber Belmiro, Diego Lovison, Ajay Nagar, Mathew Wicks, Steven Irvin, Milos Grubjesic, Andrew Scribner, Julius von Kohout.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Working Group leads&lt;/em&gt;: Andrey Velichkevich, Ce Gao, Chaoran Yu, Chen Sun, Christian Kadner, Ilias Katsakioris, James Liu, James Wu, Johnu George, Julius von Kohout, Kimonas Sotirchos, Mathew Wicks, Matteo Mortari, Ramesh Reddy, Stefano Fioravanzo, Tommy Li, Vara Bonthu, Yannis Zarkadas, Yuan Tang, Yuki Iwai.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Kubeflow Steering Committee&lt;/em&gt;: Andrey Velichkevich, Johnu George, Josh Bottum, James Wu, Yuan Tang.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Participating Distributions&lt;/em&gt;: Charmed Kubeflow (Canonical), IBM IKS, Nutanix, OpenShift AI (RedHat), Oracle Cloud Infrastructure, DeployKF, VMWare, QBO. You can find more details about Kubeflow distributions &lt;a href=&quot;https://www.kubeflow.org/docs/started/installing-kubeflow/#packaged-distributions&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;want-to-help&quot;&gt;Want to help?&lt;/h3&gt;

&lt;p&gt;The Kubeflow community Working Groups hold open meetings and are always looking for more volunteers and users to unlock the potential of machine learning. If you’re interested in becoming a Kubeflow contributor, please feel free to check out the resources below. We look forward to working with you!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Visit our &lt;a href=&quot;https://www.kubeflow.org/docs/about/community/&quot;&gt;Kubeflow website&lt;/a&gt; or Kubeflow GitHub Page&lt;/li&gt;
  &lt;li&gt;Join the &lt;a href=&quot;https://www.kubeflow.org/docs/about/community/&quot;&gt;Kubeflow Slack channel&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Join the &lt;a href=&quot;https://groups.google.com/g/kubeflow-discuss&quot;&gt;kubeflow-discuss&lt;/a&gt; mailing list&lt;/li&gt;
  &lt;li&gt;Attend our weekly &lt;a href=&quot;https://www.kubeflow.org/docs/about/community/#kubeflow-community-call&quot;&gt;community meeting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Kubeflow 1.9 Release Team, Stefano Fioravanzo</name></author><category term="release" /><summary type="html">Kubeflow 1.9 significantly simplifies the development, tuning and management of secure machine learning models and LLMs. Highlights include:</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.kubeflow.org/images/logo.png" /><media:content medium="image" url="https://blog.kubeflow.org/images/logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Announcing the Kubeflow Spark Operator: Building a Stronger Spark on Kubernetes Community</title><link href="https://blog.kubeflow.org/operators/2024/04/15/kubeflow-spark-operator.html" rel="alternate" type="text/html" title="Announcing the Kubeflow Spark Operator: Building a Stronger Spark on Kubernetes Community" /><published>2024-04-15T00:00:00-05:00</published><updated>2024-04-15T00:00:00-05:00</updated><id>https://blog.kubeflow.org/operators/2024/04/15/kubeflow-spark-operator</id><content type="html" xml:base="https://blog.kubeflow.org/operators/2024/04/15/kubeflow-spark-operator.html">&lt;p&gt;We’re excited to announce the migration of Google’s Spark Operator to
the &lt;a href=&quot;https://github.com/kubeflow/spark-operator&quot;&gt;Kubeflow Spark Operator&lt;/a&gt;,
marking the launch of a significant addition to the &lt;a href=&quot;https://www.kubeflow.org/&quot;&gt;Kubeflow&lt;/a&gt; ecosystem. The
Kubeflow Spark Operator simplifies the deployment and management of
&lt;a href=&quot;https://spark.apache.org/docs/latest/index.html&quot;&gt;Apache
Spark&lt;/a&gt;
applications on &lt;a href=&quot;https://kubernetes.io/&quot;&gt;Kubernetes&lt;/a&gt;. This
announcement isn’t just about a new piece of technology, it’s about
building a stronger, open-governed, and more collaborative community
around Spark on Kubernetes.&lt;/p&gt;

&lt;h2 id=&quot;the-journey-to-kubeflow-spark-operator&quot;&gt;The Journey to Kubeflow Spark Operator&lt;/h2&gt;

&lt;p&gt;The journey of the Kubeflow Spark Operator began with Google Cloud
Platform’s Spark on Kubernetes Operator
(https://cloud.google.com/blog/products/data-analytics/data-analytics-meet-containers-kubernetes-operator-for-apache-spark-now-in-beta).
With over 2.3k stars and 1.3k forks on GitHub, this project laid the
foundation for a robust Spark on Kubernetes experience, enabling users
to deploy Spark workloads seamlessly across Kubernetes clusters.&lt;/p&gt;

&lt;p&gt;Growth and innovation require not just code but also community.
Acknowledging the resource and time limitations faced by Google Cloud’s
original maintainers, Kubeflow has taken up the mantle.This transition
is not merely administrative but a strategic move towards fostering a
vibrant, diverse, and more actively engaged community.&lt;/p&gt;

&lt;h2 id=&quot;why-kubeflow&quot;&gt;Why Kubeflow?&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Enhanced Community Engagement:&lt;/strong&gt; Transitioning to Kubeflow opens
the door to a broader developer base, encouraging contributions and
collaboration. Since Kubeflow is a CNCF incubating project this
transition will help consolidate Cloud Native and Spark communities
to work more closely to build robust infrastructure to run Spark
applications on Kubernetes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Stronger Governance&lt;/strong&gt;: Kubeflow’s governance model provides a
structured environment for decision-making and project management,
ensuring sustainable growth for the Spark Operator.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Unified Ecosystem&lt;/strong&gt;: By bringing the Spark Operator under the
Kubeflow umbrella, we’re not just merging projects; we’re building
a cohesive ecosystem that enhances the Spark on Kubernetes
experience.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Integration with AI/ML:&lt;/strong&gt; Kubeflow provides several components to
address many stages of the AI/ML lifecycle. The Spark distributed
data processing capabilities are a natural expansion, allowing the
Spark community to closely collaborate and better integrate within
the end-to-end ML lifecycle.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s Next?&lt;/h2&gt;

&lt;p&gt;We are dedicated to not just maintaining but enhancing the Kubeflow
Spark Operator for the long term. Here’s what you can look forward to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Upcoming roadmap&lt;/strong&gt;: As part of the first release, we aim to update
the documentation with references to Kubeflow, address GitHub
workflow issues, and update the container registry with Kubeflow,
along with any other critical issues.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Ongoing Support and Enhancements&lt;/strong&gt;: At the time of migration to
the Kubeflow repository, the repository comprised 450+ issues and
60+ pull requests. We kindly request contributors to rebase their
code and update the PR with a comment indicating its continued
relevance. As for open issues, they will be considered for
resolution as the broader community and contributors engage in
upcoming releases.The operator will continue to evolve,
incorporating new features and improvements to stay at the forefront
of Kubernetes deployments.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Rich Community Resources&lt;/strong&gt;: From detailed documentation to
hands-on tutorials, we’re crafting resources to help you succeed
with the Spark Operator. We are planning to host regular Spark
Operator calls to discuss users issues, questions, and future
roadmaps.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Open Doors for Contributions&lt;/strong&gt;: This is a call to arms for
developers, writers, and enthusiasts! Your contributions are the
lifeblood of this project, and there’s a place for everyone to make
a mark.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Kubeflow Working Group Data:&lt;/strong&gt; To consolidate efforts around new
data tools in the Kubeflow ecosystem such as Spark Operator and
Model Registry the new Working Group Data will be formalized soon.
Feel free to review &lt;a href=&quot;https://github.com/kubeflow/community/pull/673&quot;&gt;this PR&lt;/a&gt; to
get involved and provide your feedback on the charter.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;join-the-movement&quot;&gt;Join the Movement&lt;/h2&gt;

&lt;p&gt;The Kubeflow Spark Operator is more than just software. It’s a
community endeavor. Here’s how you can be a part of this journey:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Dive In&lt;/strong&gt;: Visit our &lt;a href=&quot;https://github.com/kubeflow/spark-operator&quot;&gt;GitHub repository&lt;/a&gt;
to start your journey with the Kubeflow Spark Operator.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Contribute&lt;/strong&gt;: Every code snippet, documentation update, and piece
of feedback counts. Find out how you can contribute on GitHub.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Be Part of the Community&lt;/strong&gt;: Join the &lt;a href=&quot;https://www.kubeflow.org/docs/about/community/#kubeflow-slack-channels&quot;&gt;CNCF Slack Workspace&lt;/a&gt; 
and then join the conversation in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;#kubeflow-spark-operator&lt;/code&gt; channel. 
Whether you’re seeking advice, sharing insights, or just listening in, 
your presence enriches us. Follow &lt;a href=&quot;https://www.kubeflow.org/docs/about/community/&quot;&gt;this guide&lt;/a&gt;
to learn more about Kubeflow community.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Kubeflow Spark Operator Community Call&lt;/strong&gt;: We’re excited to announce Spark Operator Community Monthly Meetings for Open Source Contributors starting &lt;strong&gt;May 17th,   2024 (10-11 AM PST)&lt;/strong&gt;. These meetings, held every third Friday, are your chance to discuss project updates, share ideas, and collaborate with the community. You can find the Zoom call details and meeting notes in this &lt;a href=&quot;https://docs.google.com/document/d/1AnG6ptKLBY7O6ddyNm4SVsEbfu6jiyVyN3hDDgDUnxQ/edit#heading=h.pgrbsx5c3qqo&quot;&gt;Google Doc&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the spirit of collaboration fostered on platforms like Slack, and
with the generous support of the Google Cloud team, we’re set to sail
into a promising future. The Kubeflow Spark Operator isn’t just a tool,
it’s our collective step towards harnessing the true potential of Spark
on Kubernetes. Together, let’s shape the future of cloud-native big
data processing.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Reference Issues&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/kubeflow/spark-operator/issues/1928#issue-2066490838&quot;&gt;Action items for adoption of Spark Kubernetes Operator in Kubeflow&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/kubeflow/community/pull/673&quot;&gt;WG Data(name provisional)proposal&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/kubeflow/spark-operator/issues/1929&quot;&gt;Update Documentation: Redirect Helm Chart Installation Links to Kubeflow Repository&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/kubeflow/spark-operator/issues/1930&quot;&gt;Update Release Workflows: Change Container Registry to Kubeflow’s ghcr.io&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>&lt;a href='https://www.linkedin.com/in/varaprofile/'&gt;Vara Bonthu&lt;/a&gt;, &lt;a href='https://www.linkedin.com/in/yuchaoran/'&gt;Chaoran Yu&lt;/a&gt;, &lt;a href='https://www.linkedin.com/in/andrey-velichkevich/'&gt;Andrey Velichkevich&lt;/a&gt;, &lt;a href='https://www.linkedin.com/in/wielgusmarcin/'&gt;Marcin Wielgus&lt;/a&gt;</name></author><category term="operators" /><summary type="html">We’re excited to announce the migration of Google’s Spark Operator to the Kubeflow Spark Operator, marking the launch of a significant addition to the Kubeflow ecosystem. The Kubeflow Spark Operator simplifies the deployment and management of Apache Spark applications on Kubernetes. This announcement isn’t just about a new piece of technology, it’s about building a stronger, open-governed, and more collaborative community around Spark on Kubernetes.</summary></entry></feed>