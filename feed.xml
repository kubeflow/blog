<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://blog.kubeflow.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://blog.kubeflow.org/" rel="alternate" type="text/html" /><updated>2021-09-27T16:37:00-05:00</updated><id>https://blog.kubeflow.org/feed.xml</id><title type="html">Kubeflow</title><subtitle>The Machine Learning Toolkit for Kubernetes.</subtitle><entry><title type="html">KServe: The next generation of KFServing</title><link href="https://blog.kubeflow.org/release/official/2021/09/27/kfserving-transition.html" rel="alternate" type="text/html" title="KServe: The next generation of KFServing" /><published>2021-09-27T00:00:00-05:00</published><updated>2021-09-27T00:00:00-05:00</updated><id>https://blog.kubeflow.org/release/official/2021/09/27/kfserving-transition</id><content type="html" xml:base="https://blog.kubeflow.org/release/official/2021/09/27/kfserving-transition.html">&lt;h4 id=&quot;by-dan-sundsun20bloombergnet-and-animesh-singhsinghanusibmcom-on-behalf-of-the-kubeflow-serving-working-group&quot;&gt;By &lt;strong&gt;Dan Sun(dsun20@bloomberg.net)&lt;/strong&gt; and &lt;strong&gt;Animesh Singh(singhan@us.ibm.com)&lt;/strong&gt; on behalf of the Kubeflow Serving Working Group&lt;/h4&gt;

&lt;h3 id=&quot;kfserving-is-now-kserve&quot;&gt;&lt;strong&gt;KFServing is now KServe&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;We are excited to announce the next chapter for KFServing.
In coordination with the Kubeflow Project Steering Group, the &lt;a href=&quot;https://github.com/kubeflow/kfserving&quot;&gt;&lt;u&gt;KFServing GitHub repository&lt;/u&gt;&lt;/a&gt; has now been
transferred to an independent &lt;a href=&quot;https://github.com/kserve/kserve&quot;&gt;&lt;u&gt;KServe GitHub organization&lt;/u&gt;&lt;/a&gt; under the stewardship of the Kubeflow Serving Working
Group leads.&lt;/p&gt;

&lt;p&gt;The project has been rebranded from &lt;strong&gt;KFServing&lt;/strong&gt; to &lt;strong&gt;KServe&lt;/strong&gt;, and we are planning to graduate the project from Kubeflow Project later this year.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-09-27-kfserving-transition/image1.png&quot; style=&quot;width:6.5in;height:4in&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Developed collaboratively by Google, IBM, Bloomberg, NVIDIA, and Seldon in 2019, KFServing was published as open source in early 2019. 
The project sets out to provide the following features:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A simple, yet powerful, Kubernetes Custom Resource for deploying machine learning (ML) models on production across ML frameworks.&lt;/li&gt;
  &lt;li&gt;Provide performant, standardized inference protocol.&lt;/li&gt;
  &lt;li&gt;Serverless inference according to live traffic patterns, supporting “Scale-to-zero” on both CPUs and GPUs.&lt;/li&gt;
  &lt;li&gt;Complete story for production ML Model Serving including prediction, pre/post-processing, explainability, and monitoring.&lt;/li&gt;
  &lt;li&gt;Support for deploying thousands of models at scale and inference graph capability for multiple models.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;KFServing was created to address the challenges of deploying and monitoring machine learning models on production for organizations.
After publishing the open source project, we’ve seen an explosion in demand for the software, leading to strong adoption and community growth.
The scope of the project has since increaded, and we have developed multiple components along the way, including our own growing body of documentation
that needs it’s own website and independent GitHub organization.&lt;/p&gt;

&lt;h3 id=&quot;whats-next&quot;&gt;&lt;strong&gt;What’s Next&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Over the coming weeks, we will be releasing &lt;strong&gt;KServe 0.7&lt;/strong&gt; outside of the Kubeflow Project and will provide more details on how to migrate from KFServing to
KServe with minimal disruptions. KFServing 0.5.x/0.6.x releases are still supported in next six months after KServe 0.7 release. We are also working on
integrating core Kubeflow APIs and standards for &lt;a href=&quot;https://docs.google.com/document/d/1a9ufoe_6DB1eSjpE9eK5nRBoH3ItoSkbPfxRA0AjPIc&quot;&gt;the conformance program&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For contributors, please follow the KServe &lt;a href=&quot;https://github.com/kserve/website/blob/main/docs/developer/developer.md&quot;&gt;developer&lt;/a&gt; and 
&lt;a href=&quot;https://github.com/kserve/website/blob/main/docs/help/contributor/mkdocs-contributor-guide.md&quot;&gt;doc contribution&lt;/a&gt; guide to make code or doc contributions.
We are excited to work with you to make KServe better and promote its adoption by more and more users!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-09-27-kfserving-transition/kserve.png&quot; style=&quot;width:8in;height:4in&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;kserve-key-links&quot;&gt;&lt;strong&gt;KServe Key Links&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://kserve.github.io/website/&quot;&gt;&lt;u&gt;Website&lt;/u&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/kserve/kserve/&quot;&gt;&lt;u&gt;Github&lt;/u&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kubeflow.slack.com/join/shared_invite/zt-n73pfj05-l206djXlXk5qdQKs4o1Zkg#/&quot;&gt;&lt;u&gt;Slack(#kubeflow-kfserving)&lt;/u&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;contributor-acknowledgement&quot;&gt;&lt;strong&gt;Contributor Acknowledgement&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;We’d like to thank all the KServe contributors for this transition work!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/andyi2it&quot;&gt;Andrews Arokiam&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/animeshsingh&quot;&gt;Animesh Singh&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/chinhuang007&quot;&gt;Chin Huang&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://github.com/yuzisun&quot;&gt;Dan Sun&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/jagadeeshi2i&quot;&gt;Jagadeesh&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/jinchihe&quot;&gt;Jinchi He&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/njhill&quot;&gt;Nick Hill&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/pvaneck&quot;&gt;Paul Van Eck&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/Iamlovingit&quot;&gt;Qianshan Chen&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/Suresh-Nakkeran&quot;&gt;Suresh Nakkiran&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/sukumargaonkar&quot;&gt;Sukumar Gaonkar&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/theofpa&quot;&gt;Theofilos Papapanagiotou&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/Tomcli&quot;&gt;Tommy Li&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/js-ts&quot;&gt;Vedant Padwal&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/PatrickXYS&quot;&gt;Yao Xiao&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/yuzliu&quot;&gt;Yuzhui Liu&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="release" /><category term="official" /><summary type="html">By Dan Sun(dsun20@bloomberg.net) and Animesh Singh(singhan@us.ibm.com) on behalf of the Kubeflow Serving Working Group</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.kubeflow.org/images/2021-09-27-kserve-transition/image1.png" /><media:content medium="image" url="https://blog.kubeflow.org/images/2021-09-27-kserve-transition/image1.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Blog: Running Kubeflow at Intuit: Enmeshed in the service mesh</title><link href="https://blog.kubeflow.org/running-kubeflow-at-intuit/" rel="alternate" type="text/html" title="Blog: Running Kubeflow at Intuit: Enmeshed in the service mesh" /><published>2021-05-03T00:00:00-05:00</published><updated>2021-05-03T00:00:00-05:00</updated><id>https://blog.kubeflow.org/kubeflow-1.3-install-post</id><content type="html" xml:base="https://blog.kubeflow.org/running-kubeflow-at-intuit/">&lt;p&gt;Deploying Kubeflow 1.3 in an enterprise with existing Kubernetes infrastructure, a Service Mesh (Istio), and Argo, presents a host of challenges.
This blog will address how those challenges were overcome while retaining the best practices that both the organization and Kubeflow prescribe.&lt;/p&gt;

&lt;h2 id=&quot;lay-of-the-land-at-intuit&quot;&gt;Lay of the land at Intuit&lt;/h2&gt;

&lt;p&gt;Intuit has invested heavily in building out a robust Kubernetes infrastructure that powers all of Intuit’s products: TurboTax, QuickBooks, and Mint. There are thousands of services that run on over a hundred Kubernetes clusters. Managing these clusters is the Intuit Kubernetes Service (IKS) control plane. The IKS control plane provides services such as namespace management, role management, and isolation, etc. Connecting the services is an advanced, Istio-based service mesh, which complements Intuit’s API Gateway. In combination, they provide robust authentication, authorization, rate limiting, and other routing capabilities.&lt;/p&gt;

&lt;p&gt;The Intuit ML Platform is built on this ecosystem and provides model training, inference, and feature management capabilities, leveraging the best of Intuit’s Kubernetes infrastructure and AWS SageMaker. This is the backdrop against which we started exploring Kubeflow to provide advanced orchestration, experimentation, and other services.&lt;/p&gt;

&lt;h2 id=&quot;kubeflow-and-istio&quot;&gt;Kubeflow and Istio&lt;/h2&gt;

&lt;p&gt;Our first challenge with running Kubeflow was the compatibility of Kubeflow’s Istio with Intuit’s existing Service Mesh built on top of Istio. Two key problems emerged: version compatibility and operational maintenance.&lt;/p&gt;

&lt;p&gt;Kubeflow v1.3 defaults to Istio (v1.9), and luckily it is compatible with the older versions of Istio (v1.6), which is what Intuit runs on. Running two Istio versions is impractical, as that would defeat the benefit of a large, interconnected existing service mesh. Hence, we wanted Kubeflow to work seamlessly with Intuit’s service mesh running Istio v1.6.&lt;/p&gt;

&lt;p&gt;If you are new to Istio, you might want a primer on these key &lt;a href=&quot;https://istio.io/latest/docs/reference/config/networking/&quot;&gt;Traffic Management Components&lt;/a&gt; and &lt;a href=&quot;https://istio.io/latest/docs/reference/config/security/&quot;&gt;Security Components&lt;/a&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;VirtualService&lt;/li&gt;
  &lt;li&gt;DestinationRule&lt;/li&gt;
  &lt;li&gt;Gateway&lt;/li&gt;
  &lt;li&gt;EnvoyFilter&lt;/li&gt;
  &lt;li&gt;AuthorizationPolicy&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;step-1-remove-default-istio-configurations-and-argo-from-kubeflow&quot;&gt;Step 1: Remove default Istio configurations and Argo from Kubeflow&lt;/h3&gt;

&lt;p&gt;The first step to running Kubeflow was to remove the Istio and Argo bundled with Kubeflow so that it could be integrated with the Intuit service mesh.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To Remove Kubeflow’s default Istio&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We have used Kustomize to build the manifest we need for our Kubeflow installation and we are using &lt;a href=&quot;https://argoproj.github.io/argo-cd/&quot;&gt;ArgoCD&lt;/a&gt; to deploy the Kubeflow Kubernetes manifests.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;.
├── base                        # Base folder for the kubeflow out of the box manifests
│   ├── kustomization.yaml      
│   ├── pipelines               # Folder for Kubeflow Pipelines module
│   │   ├── kustomization.yaml
│   ├── other modules           # Similar to the Pipelines module you can bring other modules as well
│       ├── kustomization.yaml
├── envs                        # Folder for all the Kubeflow environments
│   ├── prod               
│   │   ├── kustomization.yaml
│   ├── dev               
│       ├── kustomization.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;base -&amp;gt; kustomization.yaml&lt;/code&gt;&lt;/p&gt;
&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kustomize.config.k8s.io/v1beta1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Kustomization&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;github.com/kubeflow/manifests/common/kubeflow-roles/base?ref=v1.3.0&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;github.com/kubeflow/manifests/common/kubeflow-namespace/base?ref=v1.3.0&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;github.com/kubeflow/manifests/common/oidc-authservice/base?ref=v1.3.0&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;github.com/kubeflow/manifests/apps/admission-webhook/upstream/overlays/cert-manager?ref=v1.3.0&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;github.com/kubeflow/manifests/apps/profiles/upstream/overlays/kubeflow?ref=v1.3.0&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;github.com/kubeflow/manifests/apps/centraldashboard/upstream/overlays/istio?ref=v1.3.0&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;pipelines&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;base -&amp;gt; pipelines -&amp;gt; kustomization.yaml&lt;/code&gt;&lt;/p&gt;
&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kustomize.config.k8s.io/v1beta1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Kustomization&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;bases&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;github.com/kubeflow/pipelines/manifests/kustomize/base/installs/multi-user?ref=1.5.0&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;github.com/kubeflow/pipelines/manifests/kustomize/base/metadata/base?ref=1.5.0&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;github.com/kubeflow/pipelines/manifests/kustomize/base/metadata/options/istio?ref=1.5.0&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# To remove the default Argo from Pipelines module&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# - github.com/kubeflow/pipelines/manifests/kustomize/third-party/argo/installs/cluster?ref=1.5.0&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;github.com/kubeflow/pipelines/manifests/kustomize/third-party/mysql/base?ref=1.5.0&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;github.com/kubeflow/pipelines/manifests/kustomize/third-party/mysql/options/istio?ref=1.5.0&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;github.com/kubeflow/pipelines/manifests/kustomize/third-party/minio/base?ref=1.5.0&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;github.com/kubeflow/pipelines/manifests/kustomize/third-party/minio/options/istio?ref=1.5.0&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;github.com/kubeflow/pipelines/manifests/kustomize/third-party/metacontroller/base?ref=1.5.0&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Identifier for application manager to apply ownerReference.&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# The ownerReference ensures the resources get garbage collected&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# when application is deleted.&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;commonLabels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;application-crd-id&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubeflow-pipelines&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# !!! If you want to customize the namespace,&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# please also update base/cache-deployer/cluster-scoped/cache-deployer-clusterrolebinding.yaml&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubeflow&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note: we had to create a separate folder for pipelines because we didn’t want to use Argo, which comes with the Pipelines module. If you can use default Argo, then you can simply use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https://github.com/kubeflow/manifests/apps/pipeline/upstream/env/platform-agnostic-multi-user-pns?ref=v1.3.0&lt;/code&gt; instead of the pipelines folder.&lt;/p&gt;

&lt;p&gt;If you don’t want to use ArgoCD, you can build the manifest using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kustomize build&lt;/code&gt; command, which is essentially what ArgoCD does. The configuration above has been tested for Kustomize 3.8.x and 4.0.x, and it works with both.&lt;/p&gt;

&lt;h3 id=&quot;step-2-kustomize-the-kubeflow-manifests&quot;&gt;Step 2: Kustomize the Kubeflow manifests&lt;/h3&gt;

&lt;p&gt;Given the managed Kubernetes ecosystem at Intuit, protocols for service to service communication and namespace isolation is opinionated, and we had to make the following changes:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Enable Kubeflow namespace for &lt;a href=&quot;https://istio.io/latest/docs/setup/additional-setup/sidecar-injection/#automatic-sidecar-injection&quot;&gt;Istio injection&lt;/a&gt; by adding the label &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-injection: enabled&lt;/code&gt; in the namespace specification. This label is then used by Istio to add the sidecar into the namespace.&lt;/li&gt;
  &lt;li&gt;Enable sidecar injection to all the deployments and statefulsets in Kubeflow by adding the annotation &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sidecar.istio.io/inject: &quot;true&quot;&lt;/code&gt;, along with some Intuit-specific custom labels and annotations to the Deployments and StatefulSets.&lt;/li&gt;
  &lt;li&gt;Intuit’s security policies forbid the direct use of external container registries. Intuit’s internal container registry runs regular vulnerability scans and certifies Docker images for use in various environments. The internal container registry also has an allow list that enables external registries to be proxied and held to the same, high-security standards. We enabled it for all Kubeflow containers.&lt;/li&gt;
  &lt;li&gt;Changes in VirtualService to route all the traffic from one central gateway instead of using Kubeflow gateway.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We have used &lt;a href=&quot;https://kustomize.io/&quot;&gt;Kustomize&lt;/a&gt; to modify the Kubeflow application manifest.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;For adding labels, we have used LabelTransformer
    &lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;     &lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;builtin&lt;/span&gt;
     &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;LabelTransformer&lt;/span&gt;
     &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
       &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;deployment-labels&lt;/span&gt;
     &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&amp;lt;Intuit custom labels&amp;gt;&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;istio-injected&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;true&quot;&lt;/span&gt;
     &lt;span class=&quot;na&quot;&gt;fieldSpecs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
     &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;spec/template/metadata/labels&lt;/span&gt;
       &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Deployment&lt;/span&gt;
       &lt;span class=&quot;na&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
     &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;spec/template/metadata/labels&lt;/span&gt;
       &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;StatefulSet&lt;/span&gt;
       &lt;span class=&quot;na&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For adding annotations, we have used AnnotationsTransformer&lt;/p&gt;

    &lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;builtin&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;AnnotationsTransformer&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;deployment-annotations&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;annotations&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;s&quot;&gt;&amp;lt;Intuit custom annotations&amp;gt;&lt;/span&gt;
   &lt;span class=&quot;s&quot;&gt;sidecar.istio.io/inject&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;true&quot;&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;fieldSpecs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
 &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;spec/template/metadata/annotations&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Deployment&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
 &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;spec/template/metadata/annotations&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;StatefulSet&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For replacing docker image URLs, we used ImageTagTransformer&lt;/p&gt;

    &lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;builtin&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ImageTagTransformer&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;image-transformer-1&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;imageTag&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;gcr.io/ml-pipeline/cache-deployer&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;newName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;docker.intuit.com/gcr-rmt/ml-pipeline/cache-deployer&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;It will be helpful for any organization which has a proxy for accessing the internet, cloning all the container images local to your org is the way to go as the internet will not be required to access those images.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For transforming VirtualServices&lt;/p&gt;

    &lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;remove&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/spec/hosts/0&lt;/span&gt;
 &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;replace&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/spec/gateways/0&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;custom gateway&amp;gt;&lt;/span&gt;
 &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;add&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/spec/hosts/0&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;kubflow host name&amp;gt;&lt;/span&gt;
 &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;add&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/spec/exportTo&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;.&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Putting it all together&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;envs -&amp;gt; prod/dev -&amp;gt; kustomization.yaml&lt;/code&gt;&lt;/p&gt;
    &lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kustomize.config.k8s.io/v1beta1&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Kustomization&lt;/span&gt;

 &lt;span class=&quot;na&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
 &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;../base&lt;/span&gt;

 &lt;span class=&quot;na&quot;&gt;transformers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
 &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;transformers/image-transformers.yaml&lt;/span&gt;
 &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;transformers/label-transformers.yaml&lt;/span&gt;
 &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;transformers/annotations-transformers.yaml&lt;/span&gt;

 &lt;span class=&quot;na&quot;&gt;patchesJson6902&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
 &lt;span class=&quot;c1&quot;&gt;# patch VirtualService with explicit host&lt;/span&gt;
 &lt;span class=&quot;c1&quot;&gt;# add multiple targets like below for all the VirtualServices which you need&lt;/span&gt;
 &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;patches/virtual-service-hosts.yaml&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
     &lt;span class=&quot;na&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;networking.istio.io&lt;/span&gt;
     &lt;span class=&quot;na&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1alpha3&lt;/span&gt;
     &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;VirtualService&lt;/span&gt;
     &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;centraldashboard&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;You might face issues with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;metadata_envoy&lt;/code&gt; service, in our case we were getting the following error
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; [debug][init] [external/envoy/source/common/init/watcher_impl.cc:27] init manager Server destroyed
 unable to bind domain socket with id=0 (see --base-id option)
 2021-01-29T23:32:26.680310Z error Epoch 0 exited with error: exit status 1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;After looking up, we found that, when you run this docker image with Istio Sidecar injection, this problem occurs.
 The reason is, both these containers are essentially envoyproxy containers and the default base-id for both containers is set to 0.&lt;/p&gt;

    &lt;p&gt;So to make it work, we had to change CMD in this &lt;a href=&quot;https://github.com/kubeflow/pipelines/blob/1.4.1/third_party/metadata_envoy/Dockerfile#L27&quot;&gt;Dockerfile&lt;/a&gt;&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; CMD [&quot;/etc/envoy.yaml&quot;, &quot;--base-id&quot;, &quot;1&quot;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;step-3-custom-changes-needed-for-sso&quot;&gt;Step 3: Custom changes needed for SSO&lt;/h3&gt;

&lt;p&gt;There are two major components around authentication using SSO:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Authservice: It is a StatefulSet that runs the oidc-auth service. It runs in the istio-system namespace and directly talks to an OIDC service for authentication&lt;/li&gt;
  &lt;li&gt;Authn-filter: It’s an EnvoyFilter that filters the traffic to authservice and checks the Kubeflow auth header and redirects to authservice if the request is not authorized, check the presence of header called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubeflow-userid&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Note: Intuit SSO supports OIDC, so we did not need to use dex for the integration. If your org’s SSO does not support OIDC, then you can use dex in the middle; details can be found &lt;a href=&quot;https://github.com/kubeflow/manifests#dex&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For our installation, we needed the authservice to be mesh-enabled, and it made more sense to move authservice to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubeflow&lt;/code&gt; namespace as well, which was already enabled for Istio sidecar injection.&lt;/p&gt;

&lt;p&gt;After enabling Istio mesh on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;authservice&lt;/code&gt;, some more changes were required in the default manifest for it to work. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;authservice&lt;/code&gt; pod was not able to communicate with the Intuit SSO HTTPS URL, because outbound traffic from the main container pod is intercepted by Istio sidecar to enforce mtls (default behavior). So, we had to exclude the HTTPS port (443) to disable mtls. This can be done using the annotation &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;traffic.sidecar.istio.io/excludeOutboundPorts: &quot;443&quot;&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;step-4-setting-up-ingress&quot;&gt;Step 4: Setting up ingress&lt;/h3&gt;

&lt;p&gt;We exposed the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-ingressgateway&lt;/code&gt; service as LoadBalancer using the following mechanism:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Setting up public hosted zone in Route 53, add hostname you would like to use, like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;example.com&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Setting up an ACM certificate for the hostname you want to use for the Kubeflow installation, the hostname can be &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubeflow.example.com&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Updating the service manifest by adding a few annotations:
    &lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Note that the backend talks over HTTP.&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;service.beta.kubernetes.io/aws-load-balancer-backend-protocol&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;http&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# TODO: Fill in with the ARN of your certificate.&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;service.beta.kubernetes.io/aws-load-balancer-ssl-cert&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;cert arn from step 2&amp;gt;&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;service.beta.kubernetes.io/aws-load-balancer-security-groups&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;to restrict access within org&amp;gt;&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Only run SSL on the port named &quot;https&quot; below.&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;service.beta.kubernetes.io/aws-load-balancer-ssl-ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;https&quot;&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;external-dns.alpha.kubernetes.io/hostname&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubeflow.example.com&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;After applying the new manifest, AWS will automatically add the appropriate A and TXT entries in your hosted zone (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;example.com&lt;/code&gt;) and Kubeflow will be accessible at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubeflow.example.com&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;To secure the &lt;a href=&quot;https://www.kubeflow.org/docs/started/k8s/kfctl-istio-dex/#secure-with-https&quot;&gt;Gateway with https&lt;/a&gt;, you can also change the gateway port and add the key and certificate in the Gateway.&lt;/p&gt;

&lt;p&gt;More about these annotations can be found at &lt;a href=&quot;https://aws.amazon.com/premiumsupport/knowledge-center/terminate-https-traffic-eks-acm/&quot;&gt;Terminate HTTPS traffic on Amazon EKS&lt;/a&gt; and &lt;a href=&quot;https://kubernetes.io/docs/concepts/services-networking/service/#ssl-support-on-aws&quot;&gt;SSL support on AWS&lt;/a&gt; blog.&lt;/p&gt;

&lt;h3 id=&quot;step-5-using-an-external-argo-installation&quot;&gt;Step 5: Using an external Argo installation&lt;/h3&gt;

&lt;p&gt;Kubfelow uses Argo workflows internally to run the pipeline in a workflow fashion. Argo generates artifacts after the workflow steps and all we need to do is configure the artifact store if we are planning to use the external Argo:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/argoproj/argo-workflows/blob/master/docs/quick-start.md#install-argo-workflows&quot;&gt;Install Argo workflows&lt;/a&gt; in your cluster, it gets installed in a namespace called argo.&lt;/li&gt;
  &lt;li&gt;Remove all the Argo-related manifests from Kubeflow.&lt;/li&gt;
  &lt;li&gt;To override the artifact store, you need to change the ConfigMap &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;workflow-controller-configmap&lt;/code&gt; which comes with the &lt;a href=&quot;https://github.com/kubeflow/manifests/blob/v1.3.0/apps/pipeline/upstream/third-party/argo/base/workflow-controller-configmap-patch.yaml&quot;&gt;Kubeflow manifest&lt;/a&gt;. It uses minio as the store but you can configure it to use S3 as well. More details can be found from the &lt;a href=&quot;https://github.com/argoproj/argo-workflows/blob/master/docs/workflow-controller-configmap.md&quot;&gt;Argo&lt;/a&gt;&lt;a href=&quot;https://github.com/argoproj/argo-workflows/blob/master/docs/workflow-controller-configmap.md&quot;&gt;Workflow Controller Configmap GitHub page&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;The latest version of Argo has the option to override &lt;a href=&quot;https://argoproj.github.io/argo-workflows/artifact-repository-ref/&quot;&gt;artifact store for namespace&lt;/a&gt; as well.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Debugging tricks&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Check if EnvoyFilter is getting applied: you should have the &lt;strong&gt;istioctl&lt;/strong&gt; cmd tool:&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istioctl proxy-config listeners &amp;lt;pod name&amp;gt; --port 15001 -o json&lt;/code&gt;&lt;/p&gt;

    &lt;p&gt;See if the envoy filter is getting listed in the output. More about Istio proxy debugging can be found &lt;a href=&quot;https://istio.io/latest/docs/ops/diagnostic-tools/proxy-cmd/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Check istio-ingressgateway:&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; # Port forward to the first istio-ingressgateway pod
 kubectl -n istio-system port-forward $(kubectl -n istio-system get pods -listio=ingressgateway -o=jsonpath=&quot;{.items[0].metadata.name}&quot;) 15000

 # Get the http routes from the port-forwarded ingressgateway pod (requires jq)
 curl --silent http://localhost:15000/config_dump | jq '\''.configs.routes.dynamic_route_configs[].route_config.virtual_hosts[]| {name: .name, domains: .domains, route: .routes[].match.prefix}'\''

 # Get the logs of the first istio-ingressgateway pod
 # Shows what happens with incoming requests and possible errors
 kubectl -n istio-system logs $(kubectl -n istio-system get pods -listio=ingressgateway -o=jsonpath=&quot;{.items[0].metadata.name}&quot;) --tail=300

 # Get the logs of the first istio-pilot pod
 # Shows issues with configurations or connecting to the Envoy proxies
 kubectl -n istio-system logs $(kubectl -n istio-system get pods -listio=pilot -o=jsonpath=&quot;{.items[0].metadata.name}&quot;) discovery --tail=300
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Check the authservice connectivity: istio-ingressgateway pod should be able to access authservice. You can check that using the following command:&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl -n istio-system exec $(kubectl -n istio-system get pods -listio=pilot -o=jsonpath=&quot;{.items[0].metadata.name}&quot;) -- curl -v http://authservice.istio-system.svc.cluster.local:8080&lt;/code&gt;&lt;/p&gt;

    &lt;p&gt;Also, make sure authservice can reach dex:&lt;/p&gt;

    &lt;p&gt;In our case, authservice is in the kubeflow namespace so we made changes accordingly using the command below:&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl -n kubeflow exec authservice-0 -- wget -q -S -O '-' &amp;lt;oidc auth url&amp;gt;/.well-known/openid-configuration&lt;/code&gt;&lt;/p&gt;

    &lt;p&gt;It should look something similar to:&lt;/p&gt;
    &lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
   &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;issuer&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;http://dex.kubeflow.svc.cluster.local:5556/dex&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
   &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;authorization_endpoint&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;http://dex.kubeflow.svc.cluster.local:5556/dex/auth&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
   &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;token_endpoint&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;http://dex.kubeflow.svc.cluster.local:5556/dex/token&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
   &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;jwks_uri&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;http://dex.kubeflow.svc.cluster.local:5556/dex/keys&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
   &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;userinfo_endpoint&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;http://dex.kubeflow.svc.cluster.local:5556/dex/userinfo&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
   &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;response_types_supported&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;code&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
   &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;subject_types_supported&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;public&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
   &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;id_token_signing_alg_values_supported&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;RS256&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
   &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;scopes_supported&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;openid&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;email&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;groups&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;profile&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;offline_access&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
   &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;token_endpoint_auth_methods_supported&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;client_secret_basic&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
   &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;claims_supported&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;aud&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;email&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;email_verified&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;iat&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;iss&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;locale&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;sub&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Check connectivity between services: try using &lt;strong&gt;curl&lt;/strong&gt; or &lt;strong&gt;wget&lt;/strong&gt; from one service to another. Usually one or the other is always available, otherwise, you can always install using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apt-get&lt;/code&gt; command. Example use case: from the ml-pipeline deployment pod you can check if pipeline APIs are accessible.&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl -n kubeflow exec $(kubectl -n kubeflow get pods -lapp=ml-pipeline-ui -o=jsonpath=&quot;{.items[0].metadata.name}&quot;)  -- wget -q -S -O '-' ml-pipeline.kubeflow.svc.cluster.local:8888/apis/v1beta1/pipelines&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;asks-for-the-kubeflow-community&quot;&gt;Asks for the Kubeflow Community&lt;/h2&gt;

&lt;p&gt;The challenges that we encountered at Intuit are not unique and will be faced by any enterprise that wants to adopt Kubeflow.&lt;/p&gt;

&lt;p&gt;It would be nice to have Kubeflow play well with the available Kubernetes infrastructure in an enterprise, rather than mandating its own set of infrastructure. Here are some suggestions/bugs for improving the ecosystem, some of which Intuit will work with the community to build out:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;We saw Kubeflow manifest repo went through &lt;a href=&quot;https://github.com/kubeflow/manifests/issues/1735&quot;&gt;major folder restructuring&lt;/a&gt; for v1.3 but we think there is still room for improvements.&lt;/li&gt;
  &lt;li&gt;Multi-Cluster / Multi-Region support. &lt;a href=&quot;https://github.com/kubeflow/kubeflow/issues/5467&quot;&gt;#5467&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Upgrade seems to be an issue in general, should figure out a way to manage this better. &lt;a href=&quot;https://github.com/kubeflow/kubeflow/issues/5440&quot;&gt;#5440&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Multi-tenancy with group support. &lt;a href=&quot;https://github.com/kubeflow/kubeflow/issues/4188&quot;&gt;#4188&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Installing Kubeflow in any custom namespace. &lt;a href=&quot;https://github.com/kubeflow/kubeflow/issues/5647&quot;&gt;#5647&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Existing metadata service is not performant, we did try some settings with more resources and horizontal scaling. The community is already working on &lt;a href=&quot;https://docs.google.com/document/d/1fHU29oScMEKPttDA1Th1ibImAKsFVVt2Ynr4ZME05i0/edit&quot;&gt;KFP v2.0&lt;/a&gt;, which might address a lot of concerns around metadata service.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.google.com/document/d/1fHU29oScMEKPttDA1Th1ibImAKsFVVt2Ynr4ZME05i0/edit&quot;&gt;Kubeflow Pipelines (KFP) v2 System Design&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://istio.io/latest/docs/reference/config/networking/&quot;&gt;Traffic Management Components&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://istio.io/v1.6/docs/ops/deployment/architecture/&quot;&gt;Istio 1.6 Architecture&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://istio.io/v1.3/docs/concepts/what-is-istio/#architecture&quot;&gt;Istio 1.3 Architecture&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/premiumsupport/knowledge-center/terminate-https-traffic-eks-acm/&quot;&gt;Terminate HTTPS traffic on Amazon EKS&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kubernetes.io/docs/concepts/services-networking/service/#ssl-support-on-aws&quot;&gt;SSL support on AWS&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.developermarch.com/developersummit/downloadPDF/Intuit%20Modern%20SaaS%20Platform%20-%20GIDS.pdf&quot;&gt;Intuit’s Modern SaaS Platform&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=EWyNbBn1vns&quot;&gt;Stitching a Service Mesh Across Hundreds of Discrete Networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://istio.io/latest/blog/2020/multi-cluster-mesh-automation/&quot;&gt;Multicluster Istio configuration and service discovery using Admiral&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/intuit-engineering/genius-of-admiral-3307e63e3ab6&quot;&gt;Genius of Admiral&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>&lt;a href='https://www.linkedin.com/in/deepk2u/'&gt;Deepak Kumar&lt;/a&gt;</name></author><category term="kubeflow" /><category term="kubeflow 1.3" /><category term="install kubeflow" /><category term="kubeflow pipelines" /><category term="intuit" /><category term="istio" /><category term="service mesh" /><category term="argo" /><summary type="html">Deploying Kubeflow 1.3 in an enterprise with existing Kubernetes infrastructure, a Service Mesh (Istio), and Argo, presents a host of challenges. This blog will address how those challenges were overcome while retaining the best practices that both the organization and Kubeflow prescribe.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.kubeflow.org/images/logo.png" /><media:content medium="image" url="https://blog.kubeflow.org/images/logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">The Kubeflow 1.3 software release streamlines ML workflows and simplifies ML platform operations</title><link href="https://blog.kubeflow.org/kubeflow-1.3-release/" rel="alternate" type="text/html" title="The Kubeflow 1.3 software release streamlines ML workflows and simplifies ML platform operations" /><published>2021-04-23T00:00:00-05:00</published><updated>2021-04-23T00:00:00-05:00</updated><id>https://blog.kubeflow.org/kubeflow-1.3-release</id><content type="html" xml:base="https://blog.kubeflow.org/kubeflow-1.3-release/">&lt;p&gt;The Kubeflow 1.3 release delivers simplified ML workflows and additional Kubernetes integrated features to optimize operational and infrastructure efficiencies. 
In addition to new User Interfaces (UIs), which improve ML workflows for pipeline building, model tuning, serving and monitoring, 1.3 also enables “headless” GitOps-inspired installation patterns.
The latest version of Kubeflow provides users with a mature foundation and delivers a modern ML platform with best-in-class Key Performance Indicators (KPIs).&lt;/p&gt;

&lt;p&gt;The Kubeflow user community is growing quickly, which was demonstrated in our recent survey results.  When compared to &lt;a href=&quot;https://www.youtube.com/watch?v=4228OEenuGc&quot;&gt;last year’s survey&lt;/a&gt;, the &lt;a href=&quot;https://blog.kubeflow.org/kubeflow-continues-to-move-to-production&quot;&gt;2021 Survey&lt;/a&gt; showed a 50% increase in responses and a whopping 300% increase in users supporting production deployments.   As shown below, the user survey responses, especially from ML engineers, architects and data scientists, have identified where the Kubeflow contributors should focus their efforts.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://blog.kubeflow.org/kubeflow-continues-to-move-to-production#using-kubeflow-goes-beyond-just-training&quot;&gt;Kubeflow User Survey Results&lt;/a&gt; - March 2021&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-04-23-kubeflow-1.3/image1.png&quot; alt=&quot;survey_results&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;streamlined-ml-workflows-delivered-via-new-uis&quot;&gt;Streamlined ML workflows delivered via new UIs&lt;/h2&gt;

&lt;p&gt;Data scientists will like the new and updated user interfaces (UIs) for Katib, TensorBoard, Persistent Volumes, Pipelines and Kale. These new UIs address many of the ML tasks that are time consuming and technically challenging. The UIs reduce the need for a data scientist to learn kfctl or docker CLI commands.&lt;/p&gt;

&lt;p&gt;Below please find details on the UIs’ benefits for ML workflows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Katib (&lt;a href=&quot;https://www.youtube.com/watch?v=VDINH5WkBhA&quot;&gt;Video Tour&lt;/a&gt;)
    &lt;ul&gt;
      &lt;li&gt;The Katib UI is integrated with the central dashboard and streamlines hyperparameter tuning by presenting a visualization graph and a table that compares each trial’s performance along with its hyperparameters. You can also review the details of each trial’s algorithm, metrics collector and yaml. (Project &lt;a href=&quot;https://github.com/kubeflow/katib/projects/1&quot;&gt;1&lt;/a&gt;)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;TensorBoard (&lt;a href=&quot;https://www.youtube.com/watch?v=eMDF2Bk8YRY&quot;&gt;Video tour&lt;/a&gt;)
    &lt;ul&gt;
      &lt;li&gt;The TensorBoard UI streamlines the TensorBoard configuration tasks, especially for logging of training jobs which are running in Notebooks or Pipelines. It simplifies accessibility to metrics, which helps you to improve model accuracy , identify performance bottlenecks, and reduce unproductive training jobs.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Volume Manager (&lt;a href=&quot;https://www.youtube.com/watch?v=jU2DtSWahdA&quot;&gt;Video tour&lt;/a&gt;)
    &lt;ul&gt;
      &lt;li&gt;The Volume Manager enables you to manage your data and persistent volumes. For the volumes in your namespace, it streamlines the creation and deletion of volumes, which then can be easily attached to your notebooks. PR &lt;a href=&quot;https://github.com/kubeflow/kubeflow/pull/5684&quot;&gt;5684&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Kale (&lt;a href=&quot;https://www.youtube.com/watch?v=ANBkUySirGg&quot;&gt;Video tour&lt;/a&gt;)
    &lt;ul&gt;
      &lt;li&gt;The updated Kale UI, a JupyterLab extension, simplifies your hyperparameter tuning trial set-up. The UI walks you through these steps: enter your hyperparameters as a list or a range, pick your search algorithm (Grid, Random, Bayesian) and the parameter to be optimized i.e. minimize loss. Then with a click of a button, your Katib trials are set-up, snapshotted, tracked, and run.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Kubeflow Pipelines (KFP)
    &lt;ul&gt;
      &lt;li&gt;The KFP UI has been reorganized for a more unified experience (PR &lt;a href=&quot;https://github.com/kubeflow/pipelines/pull/4925&quot;&gt;4925&lt;/a&gt;), and includes the ability to manage recurring runs via new “JobsList” and “AllJobslist” pages (PR &lt;a href=&quot;https://github.com/kubeflow/pipelines/pull/5131&quot;&gt;5131&lt;/a&gt;) and simplified view of dependency graphs.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Beyond the UIs, data scientists can also tie Notebooks with Serving more closely than ever before. In addition to the aforementioned integration with TensorBoard, Kubeflow Notebooks also now support first class deployments with TensorFlow 2.0, PyTorch, VS Code and RStudio.&lt;/p&gt;

&lt;p&gt;KFServing enhancements include simplified canary rollouts with traffic splitting at the Knative revisions level. It also delivers extended ML framework support for:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;TorchServe predict and PyTorch Captum Explain&lt;/li&gt;
  &lt;li&gt;PMMLServer, PR &lt;a href=&quot;https://github.com/kubeflow/kfserving/pull/1141&quot;&gt;1141&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;LightGBM&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;infrastructure-and-operational-efficiencies&quot;&gt;Infrastructure and operational efficiencies&lt;/h2&gt;

&lt;p&gt;ML engineers will like 1.3’s delivery of operational and infrastructure efficiencies, which are coupled with streamlined installation patterns and upgraded Istio version support. The following chart provides a summary of the top production features in 1.3.&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Feature&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Benefits&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;a href=&quot;https://github.com/yuzliu/kfserving/blob/master/docs/MULTIMODELSERVING_GUIDE.md&quot;&gt;Multi-model serving&lt;/a&gt; (Alpha)
   &lt;/td&gt;
   &lt;td&gt;More models on same infra and workaround cluster limits        i.e. # of pods &amp;amp; ip addresses
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Pod affinity
   &lt;/td&gt;
   &lt;td&gt;Avoid unnecessary usage on GPU or large CPU nodes
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;gRPC support 
   &lt;/td&gt;
   &lt;td&gt;Fewer messages, less bandwidth for KFServing workloads
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Katib trial templates
   &lt;/td&gt;
   &lt;td&gt;Simplifies hyperparameter tuning set-up for custom model types
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Katib early stopping
   &lt;/td&gt;
   &lt;td&gt;Stops hyperparameter tuning trials that are unproductive 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Pipelines step caching
   &lt;/td&gt;
   &lt;td&gt;Re-use results from previously run steps
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Multi-user pipelines
   &lt;/td&gt;
   &lt;td&gt;User and resource isolation for non-GCP environments.
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Manifests refactoring
   &lt;/td&gt;
   &lt;td&gt;Simplifies Kubeflow installation and upgrades
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Istio upgradability
   &lt;/td&gt;
   &lt;td&gt;Improved security, day 2 operations, compatibility and support
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;We are pleased to announce that the user documentation on Kubeflow.org has also been updated (PR &lt;a href=&quot;https://github.com/kubeflow/website/issues/2546&quot;&gt;2546&lt;/a&gt;). Additional detailed documentation, especially on the valuable working group deliveries, can be found here:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Kubeflow Pipeline 1.3 Project (PR &lt;a href=&quot;https://github.com/kubeflow/pipelines/projects/12&quot;&gt;12&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kubeflow.org/docs/components/pipelines/sdk/pipelines-with-tekton/&quot;&gt;Kubeflow Pipelines SDK with Tekton&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.kubeflow.org/release/official/2021/03/08/kfserving-0.5.html&quot;&gt;Operationalize, scale and infuse trust in AI models using KFServing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.kubeflow.org/katib/&quot;&gt;Kubeflow Katib: Scalable, portable and cloud native system for AutoML&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;simplified-installation-and-improved-documentation&quot;&gt;Simplified installation and improved documentation&lt;/h2&gt;

&lt;p&gt;ML Engineers, who are installing Kubeflow, have a clear path to installation success as Kubeflow 1.3 includes new manifests and upgraded Istio support. For more information on installation patterns for each distribution, please visit the &lt;a href=&quot;https://www.kubeflow.org/docs/started/installing-kubeflow/&quot;&gt;Getting Started&lt;/a&gt; page on Kubeflow.org. If you are supporting a distribution or just interested in low-level details, please review the Kubeflow 1.3 Manifest &lt;a href=&quot;https://github.com/kubeflow/manifests/tree/v1.3.0-rc.0#readme&quot;&gt;readme&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;kubeflow-13-tutorials&quot;&gt;Kubeflow 1.3 tutorials&lt;/h2&gt;

&lt;p&gt;Kubeflow 1.3 new features are easy to try on these tutorials:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Open Vaccine &lt;a href=&quot;https://arrik.to/democ2p&quot;&gt;Tutorial&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Use the new UIs to build an ML Pipeline, tune your model, and then deploy and monitor it. This tensorflow-based example was modified from a Kaggle tutorial for building a Covid 19 vaccine from bases in an mRNA molecule. The tutorial is easy to run on AWS and GCP in about 1 hour.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Model Risk Management &lt;a href=&quot;https://www.fairly.ai/tutorial-kubeflow&quot;&gt;Tutorial&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;This model produces a SR11-7 compliance report for financial institutions who are regulated by the Federal Reserve. The example provides reporting on bias in a home mortgage lending model. The tutorial is easy to run on AWS and GCP in about 1 hour.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;join-the-community&quot;&gt;Join the community&lt;/h2&gt;

&lt;p&gt;We would like to thank everyone for their efforts on Kubeflow 1.3, especially the code contributors and working group leads. As you can see from the extensive contributions to Kubeflow 1.3, the Kubeflow Community is vibrant and diverse, and solving real world problems for organizations around the world.&lt;/p&gt;

&lt;p&gt;Want to help? The Kubeflow Community &lt;a href=&quot;https://github.com/kubeflow/community/blob/master/wg-list.md&quot;&gt;Working Groups&lt;/a&gt; hold open meetings, public lists, and are always looking for more volunteers and users to unlock the potential of machine learning. If you’re interested in becoming a Kubeflow contributor, please feel free to check out the resources below, we look forward to working with you!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Visit our&lt;a href=&quot;https://www.kubeflow.org/&quot;&gt; Kubeflow website&lt;/a&gt; or&lt;a href=&quot;https://github.com/kubeflow&quot;&gt; Kubeflow GitHub Page&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Join the&lt;a href=&quot;https://join.slack.com/t/kubeflow/shared_invite/enQtMjgyMzMxNDgyMTQ5LWUwMTIxNmZlZTk2NGU0MmFiNDE4YWJiMzFiOGNkZGZjZmRlNTExNmUwMmQ2NzMwYzk5YzQxOWQyODBlZGY2OTg&quot;&gt; Kubeflow Slack channel&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Join the&lt;a href=&quot;https://groups.google.com/forum/#!forum/kubeflow-discuss&quot;&gt; kubeflow-discuss&lt;/a&gt; mailing list&lt;/li&gt;
  &lt;li&gt;Attend a&lt;a href=&quot;https://www.kubeflow.org/docs/about/community/&quot;&gt; weekly community meeting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Josh Bottum, Thea Lamkin, David Aronchick</name></author><category term="release" /><summary type="html">The Kubeflow 1.3 release delivers simplified ML workflows and additional Kubernetes integrated features to optimize operational and infrastructure efficiencies. In addition to new User Interfaces (UIs), which improve ML workflows for pipeline building, model tuning, serving and monitoring, 1.3 also enables “headless” GitOps-inspired installation patterns. The latest version of Kubeflow provides users with a mature foundation and delivers a modern ML platform with best-in-class Key Performance Indicators (KPIs).</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.kubeflow.org/images/logo.png" /><media:content medium="image" url="https://blog.kubeflow.org/images/logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Kubeflow Continues to Move into Production</title><link href="https://blog.kubeflow.org/kubeflow-continues-to-move-to-production" rel="alternate" type="text/html" title="Kubeflow Continues to Move into Production" /><published>2021-03-19T00:00:00-05:00</published><updated>2021-03-19T00:00:00-05:00</updated><id>https://blog.kubeflow.org/kubeflow-continues-to-move-to-production</id><content type="html" xml:base="https://blog.kubeflow.org/kubeflow-continues-to-move-to-production">&lt;p&gt;Kubeflow Users are maturing and the community is growing, forty eight percent of users are supporting deployments in production.&lt;/p&gt;

&lt;p&gt;The Spring 2021 &lt;a href=&quot;http://kubeflow.org/&quot;&gt;Kubeflow&lt;/a&gt; Community User Survey collected input from Kubeflow users on the benefits, gaps and requirements for machine learning use cases. It is the largest survey to date with 179 responses—a 50% increase from the &lt;a href=&quot;https://medium.com/kubeflow/kubeflow-community-user-survey-fall-2019-a84776c71743&quot;&gt;Kubeflow 1.0 Community User Survey&lt;/a&gt; a year ago. The Survey respondents span a spectrum of skill sets. While 42% are machine learning (ML) engineers, and 24% are ML architects, the titles of the respondents vary from DevOps Engineers to data scientists and product managers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;images/2021-03-11-survey/image1.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;kubeflow-continues-to-move-into-production&quot;&gt;Kubeflow Continues to Move Into Production&lt;/h1&gt;

&lt;p&gt;Forty eight percent of users are supporting deployments that are in production, up from 15% last year. Further, one question that many folks have is “do people upgrade a production deployment or just install a new cluster and start over?” It appears that the latter is far more common: just 8% have upgraded their environment.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;images/2021-03-11-survey/image2.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Similar to previous years, Kubeflow Pipelines and Notebooks are the most popular components, but other components are now being widely deployed as well. Interest in TensorBoard has grown, joining KFServing, Katib (AutoML), and Distributed Training as top additional services.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;images/2021-03-11-survey/image3.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Although the usage patterns for Kubeflow components are mixed, the vast majority of users need at least two Kubeflow components in their ML Platform.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;images/2021-03-11-survey/image4.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;TensorFlow is the leading ML Framework, followed by Scikit-learn, PyTorch, Keras, and XGBoost. However, with Kubeflow’s built-in extensibility, the type of ML tools people use in Kubeflow go beyond just training frameworks, and include MLFlow, Airflow, and Spark.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;images/2021-03-11-survey/image5.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;vs-code-and-jupyter-lead-dev-environments&quot;&gt;VS Code and Jupyter Lead Dev Environments&lt;/h1&gt;

&lt;p&gt;From an Integrated Development Environment (IDE) perspective, most users are developing models in Jupyter Notebooks and Visual Studio Code, and about one third are using PyCharm.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;images/2021-03-11-survey/image6.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From a gap perspective, the users would like improved documentation, tutorials, and installation, along with more automation, support and security.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;images/2021-03-11-survey/image7.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;using-kubeflow-goes-beyond-just-training&quot;&gt;Using Kubeflow Goes Beyond Just Training&lt;/h1&gt;

&lt;p&gt;Users identified that data preprocessing and transformation are both the most time consuming and challenging steps. We also received feedback that pipeline building and feature engineering are both time consuming and challenging. Distributed training, model serving and monitoring appear to be more technically challenging than time consuming.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;images/2021-03-11-survey/image8.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ML model delivery commonly requires multiple teams to work together i.e. data engineers, data scientists, ML engineers and devops engineers. ML workflows often include manual processes and there can be gaps in the handoffs between these groups. In particular, connecting data pipelines to ML pipelines is an example of a process that could be better automated, along with pipeline building and model monitoring.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;images/2021-03-11-survey/image9.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The vast majority of Kubeflow users are self-reliant in solving complex problems:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;images/2021-03-11-survey/image10.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And many are using tutorials created by Cloud Service Providers (i.e. Google, AWS and Azure) and MiniKF from Arrikto.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;images/2021-03-11-survey/image11.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The majority of ML models have a fairly short life: ~50% run in production for 3 months or less. On the other end of the spectrum, 25% of the models remain in production for 6 months or longer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;images/2021-03-11-survey/image12.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And ~70% of all models take up to 15 iterations to produce  a final model suitable for production.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;images/2021-03-11-survey/image13.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Users have a wide range of success with their models: ~43% are getting more than half of their models to deliver business value. On the other side, 39% are getting a very small percentage (10%) of their models into production and delivering business value.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;images/2021-03-11-survey/image14.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;user-requests&quot;&gt;User Requests&lt;/h1&gt;

&lt;p&gt;We provided a section for free-form responses and we received a great deal of feedback. Here are some good examples of user requests:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Metadata storage and versioning&lt;/li&gt;
  &lt;li&gt;More robust access control and permission granularity for model/data sharing&lt;/li&gt;
  &lt;li&gt;More visibility on your roadmap&lt;/li&gt;
  &lt;li&gt;Installation patterns, stability, multi tenancy&lt;/li&gt;
  &lt;li&gt;More real life case studies&lt;/li&gt;
  &lt;li&gt;Updated and more in-depth documentation&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;key-takeaways&quot;&gt;Key Takeaways&lt;/h1&gt;

&lt;p&gt;In addition to learning about how users are operating Kubeflow in production clusters, the Community Survey has given us important data that we can use to enhance our processes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Improvements to the release management process, which is being driven by better inter-Working Group collaboration. This, coupled with core upgrades to Istio and a clean-up of the installation manifests, will improve the testing, documentation, and installation patterns.&lt;/li&gt;
  &lt;li&gt;Automation of pipeline building and feature engineering tasks, especially with continued integrations of Kubeflow with Kale and Feast, which are enabling new end-to-end workflows and tutorials.&lt;/li&gt;
  &lt;li&gt;Enhancements for data preprocessing and transformation as well as streamlined connections of Data Pipelines to ML Pipelines.  Additionally, a renewed effort to develop a Spark operator.&lt;/li&gt;
  &lt;li&gt;Several new UIs are under development i.e. Katib, Model Management, Volumes Management and TensorBoard Management, which will help the user experience.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more details on the Community’s and specific Working Group’s deliveries, please review the Kubeflow 1.3 Release Blog post.&lt;/p&gt;

&lt;h1 id=&quot;join-the-community&quot;&gt;Join the Community&lt;/h1&gt;

&lt;p&gt;We would like to thank everyone for their participation in the survey. As you can see from the survey results, the Kubeflow Community is vibrant and diverse, solving real world problems for organizations around the world.&lt;/p&gt;

&lt;p&gt;Want to help? The Kubeflow Community &lt;a href=&quot;https://github.com/kubeflow/community/blob/master/wg-list.md&quot;&gt;Working Groups&lt;/a&gt; hold open meetings, public lists, and are always looking for more volunteers and users to unlock the potential of machine learning. If you’re interested in becoming a Kubeflow contributor, please feel free to check out the resources below. We look forward to working with you!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Visit our&lt;a href=&quot;https://www.kubeflow.org/&quot;&gt; Kubeflow website&lt;/a&gt; or&lt;a href=&quot;https://github.com/kubeflow&quot;&gt; Kubeflow GitHub Page&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Join the&lt;a href=&quot;https://join.slack.com/t/kubeflow/shared_invite/enQtMjgyMzMxNDgyMTQ5LWUwMTIxNmZlZTk2NGU0MmFiNDE4YWJiMzFiOGNkZGZjZmRlNTExNmUwMmQ2NzMwYzk5YzQxOWQyODBlZGY2OTg&quot;&gt; Kubeflow Slack channel&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Join the&lt;a href=&quot;https://groups.google.com/forum/#!forum/kubeflow-discuss&quot;&gt; kubeflow-discuss&lt;/a&gt; mailing list&lt;/li&gt;
  &lt;li&gt;Attend a&lt;a href=&quot;https://www.kubeflow.org/docs/about/community/&quot;&gt; weekly community meeting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Josh Bottum, David Aronchick, Thea Lamkin</name></author><category term="release" /><summary type="html">Kubeflow Users are maturing and the community is growing, forty eight percent of users are supporting deployments in production.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.kubeflow.org/images/logo.png" /><media:content medium="image" url="https://blog.kubeflow.org/images/logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Elastic Training with MPI Operator and Practice</title><link href="https://blog.kubeflow.org/elastic%20training/operators/2021/03/15/elastic-training.html" rel="alternate" type="text/html" title="Elastic Training with MPI Operator and Practice" /><published>2021-03-15T00:00:00-05:00</published><updated>2021-03-15T00:00:00-05:00</updated><id>https://blog.kubeflow.org/elastic%20training/operators/2021/03/15/elastic-training</id><content type="html" xml:base="https://blog.kubeflow.org/elastic%20training/operators/2021/03/15/elastic-training.html">&lt;p&gt;With increase in the size of dataset and deep learning models, distributed training emerges as the mainstream approach for training neural network models in industry. While it is feasible now to launch a massive distributed training job on Kubernetes with Kubeflow, advanced features like elastic workload and other cost mitigation approaches remain leashed when we talk about deep learning jobs on Kubernetes.&lt;/p&gt;

&lt;p&gt;To address issues on cost and resource utilization, the &lt;a href=&quot;https://intl.cloud.tencent.com/product/tke&quot;&gt;TKE (Tencent Kubernetes Engine)&lt;/a&gt; AI team designs and implements &lt;strong&gt;Elastic Training&lt;/strong&gt; in Kubeflow community.&lt;/p&gt;

&lt;p&gt;Here we present how the elastic training is performed on Kubernetes. Validated with experiments under circumstances, elastic training lowers cost for distributed training on cloud.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;Let’s first recap training deep learning models. When we talk about ‘training’, it refers generally to iteratively optimizing parameters in a neural network model with its gradient descent. Accelerated with GPUs, the training can speed up for 10-100 times.&lt;/p&gt;

&lt;p&gt;When manufacturers try to integrate more computational resources like GPUs into a single machine, to hold training experiments with more and more data or model parameters, the cost grows exponentially. Therefore, after initially proposed by &lt;a href=&quot;https://www.usenix.org/conference/osdi14/technical-sessions/presentation/li_mu&quot;&gt;Mu Li on OSDI’14&lt;/a&gt;, distributed training takes over training on a single machine when researchers play with massive dataset or large model.&lt;/p&gt;

&lt;p&gt;For distributed training in data-parallelism, &lt;a href=&quot;https://github.com/horovod/horovod&quot;&gt;Horovod&lt;/a&gt; is widely adopted given its excellent support on deep learning frameworks like &lt;a href=&quot;https://www.tensorflow.org&quot;&gt;TensorFlow&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org&quot;&gt;PyTorch&lt;/a&gt;, communication optimization and easier programming pattern. In Horovod, all training processes are equal participants, each of which process the gradient calculation and communication.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-03-15-elastic-training/horovod-allreduce.png&quot; width=&quot;&quot; alt=&quot;alt_text&quot; title=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Because of significant acceleration of training speed as well as the programming pattern that are easier to understand, data-parallelism distributed training, represented by Horovod, is getting more and more attention. However, there still remain some issues:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The cost of training on the cloud is still the hurdle. While researchers no longer face the complexity when training on cloud thanks to Kubernetes and Kubeflow, the cost of training on cloud quells some users.&lt;/li&gt;
  &lt;li&gt;Compared with training on a single machine, multi-node distributed training accumulates the probability of training failure. The entire training experiment fails when any of its training process issues an error. This problem becomes even severer when some experiments take days or even weeks.&lt;/li&gt;
  &lt;li&gt;When collocating training tasks with other workloads (with higher priority), the resources demand fluctuates as the request for these other workloads may change periodically. This unbalance of resources availability throws cold water on the idea of using hybrid-deployment to maximize resource utilization.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;elastic-training&quot;&gt;Elastic Training&lt;/h2&gt;

&lt;p&gt;Researchers and engineers proposed &lt;strong&gt;Elastic Training&lt;/strong&gt; as the key to solve the puzzle.&lt;/p&gt;

&lt;p&gt;Traditionally, the resource configuration for a distributed training job is fixed. Elastic training breaks this rule and enables users to change the number of instances participating in a distributed training job, bringing the following benefits to clusters with distributed training jobs:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Fault Tolerance: any worker instance can fail as long as at least one is surviving.&lt;/li&gt;
  &lt;li&gt;Resources Utilization: when the resources stress piles, the cluster is able to reduce the replicas of workloads with lower priority (distributed training workloads), releasing resource to other workloads (such as prediction service), ensuring SLA for business; after resources released from workloads, elastic training job is able to absorb these resource by scaling up workload replicas.&lt;/li&gt;
  &lt;li&gt;Training on Cloud: there is a type of resource on the cloud that is called “spot” or “preemptible” instances; it comes with unexpected low price tags but may be retrieved after guaranteed hour expires.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Elastic training appears a perfect match to public cloud. Combined with spot instances, we cut the cost for GPUs from ¥16.21/hour to ¥1.62/hour, reducing the overall cost for the training job by nearly 70%. Under the same budget, elastic training employs more GPUs and accelerates the training speed by 5 to 10 times.&lt;/p&gt;

&lt;h2 id=&quot;elastic-horovod&quot;&gt;Elastic Horovod&lt;/h2&gt;

&lt;p&gt;As the major player in distributed training framework, Horovod v0.20.0 offers its solution to elastic training, &lt;a href=&quot;https://horovod.readthedocs.io/en/stable/elastic_include.html&quot;&gt;Elastic Horovod&lt;/a&gt;. Here we quotes the architecture differences between Elastic Horovod and existing Horovod from &lt;a href=&quot;https://docs.google.com/document/d/15ZoHA5AeSI_boeyIBapg9WPXKrYXMRvPytPzQWTCTn4/edit#&quot;&gt;RFC Elastic Horovod&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-03-15-elastic-training/horovod-elastic.png&quot; width=&quot;&quot; alt=&quot;alt_text&quot; title=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;All collective operations are coordinated within a hvd.elastic.run function.&lt;/li&gt;
  &lt;li&gt;State is synchronized between workers before the user’s training function is executed.&lt;/li&gt;
  &lt;li&gt;Worker failure or worker added events will result in a reset event on other workers.&lt;/li&gt;
  &lt;li&gt;Reset events act as barriers to:
    &lt;ul&gt;
      &lt;li&gt;Determine whether the job should continue based on worker exit codes.&lt;/li&gt;
      &lt;li&gt;Blacklist failing hosts.&lt;/li&gt;
      &lt;li&gt;Launch workers on new hosts.&lt;/li&gt;
      &lt;li&gt;Update the rank information on existing workers.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;State is synchronized following a reset event.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When launching an elastic training job, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;horovodrun&lt;/code&gt; requires a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;discover_hosts.sh&lt;/code&gt; script to detect available hosts and slots in real time. In the following section, we refer this script as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;discover_hosts.sh&lt;/code&gt;. Nevertheless the script needs not to be named as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;discover_hosts.sh&lt;/code&gt;. An example of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;discover_hosts.sh&lt;/code&gt; can be found &lt;a href=&quot;https://horovod.readthedocs.io/en/stable/elastic_include.html#running-with-horovodrun&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;elastic-horovod-on-kubernetes&quot;&gt;Elastic Horovod on Kubernetes&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/kubeflow/mpi-operator&quot;&gt;MPI-Operator&lt;/a&gt; is designed to deploy Horovod jobs on Kubernetes. While the operator releases multiple versions, the general idea stays unchanged. It includes:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-03-15-elastic-training/mpi-operator.png&quot; width=&quot;&quot; alt=&quot;alt_text&quot; title=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MPIJob Controller creates a launcher pod and worker pods according to the replicas configuration in MPIJobs&lt;/li&gt;
  &lt;li&gt;For each MPIJob, the controller creates a &lt;a href=&quot;https://kubernetes.io/docs/concepts/configuration/configmap/&quot;&gt;ConfigMap&lt;/a&gt;, which delivers two files: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hostfile&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubexec.sh&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;With all worker pods ready, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mpirun&lt;/code&gt; on launcher pod (granted with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pod/exec&lt;/code&gt; permission) uses &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubexec.sh&lt;/code&gt; to launch processes on worker pods&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Launching an Elastic Horovod job is not feasible as there exist several incompatibilities between Elastic Horovod and MPIJob Controller. We take controller-v1 as the example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;No built-in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;discover_hosts.sh&lt;/code&gt; available on launcher pod&lt;/li&gt;
  &lt;li&gt;After worker replica number is turned down, worker pods that are no longer wanted will not be deleted by the controller, leaving the size of the distributed training unchanged&lt;/li&gt;
  &lt;li&gt;After worker replica number is turned up, the controller does not update rule in the &lt;a href=&quot;https://kubernetes.io/docs/reference/access-authn-authz/rbac/&quot;&gt;Role&lt;/a&gt; binded to the launcher pod, preventing the launcher pod from creating processes on newly created pods&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To address these compatibility issues, we pushed multiple pull requests regarding Horovod and MPI-Operator, including &lt;a href=&quot;https://github.com/kubeflow/mpi-operator/pull/335&quot;&gt;mpi-operator-pull-335&lt;/a&gt; and &lt;a href=&quot;https://github.com/horovod/horovod/pull/2199&quot;&gt;horovod-pull-2199&lt;/a&gt;. As providing an MPI-Operator-specific &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;discover_hosts.sh&lt;/code&gt; is most critical to the launcher pod for Elastic Horovod, we consider two scenarios for converting worker pods with a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Running&lt;/code&gt; phase into a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;discover_hosts.sh&lt;/code&gt; script.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A &lt;strong&gt;dynamic&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;discover_hosts.sh&lt;/code&gt; composed by MPIJob controller and synchronized to the launcher pod via ConfigMap
    &lt;ul&gt;
      &lt;li&gt;MPIJob controller has a podLister, which can be used to list worker pods readily&lt;/li&gt;
      &lt;li&gt;the controller filters worker pods with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;status.phase == Running&lt;/code&gt; and encode the result into the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;discover_hosts.sh&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;the ConfigMap is updated when &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;discover_hosts.sh&lt;/code&gt; is modified and the change will be propagated to the launcher pod by Kubernetes&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;static&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;discover_hosts.sh&lt;/code&gt; using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl&lt;/code&gt; in the launcher pod to list all running worker pods from &lt;a href=&quot;https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/&quot;&gt;APIServer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Scenario 2 changes the delivery image instead of the controller. However, as we cannot limit how frequently users will execute the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;discover_hosts.sh&lt;/code&gt; script, it poses a potential threat to the APIServer, especially when the count of worker pods is massive.&lt;/p&gt;

&lt;p&gt;An fixture to scenario 2 is to replace the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl&lt;/code&gt; with a podLister process, removing extra stress from the APIServer. In this way, we install two processes in launcher pod but lack a proper mechanism to keep the podLister alive. Once the podLister dies, there leaves no elasticity for the training job.&lt;/p&gt;

&lt;p&gt;Therefore we choose the first scenario and map the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;disocver_hosts.sh&lt;/code&gt; under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/mpi/&lt;/code&gt;. We also fixed the other compatibility issues after the worker replica configuration changes. For users choose non-elastic mode, just simply ignore &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/mpi/discover_hosts.sh&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Concerns comes to scenario 1 as well. There is a delay between the ConfigMap and what &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;horovodrun&lt;/code&gt; sees from the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;discover_hosts.sh&lt;/code&gt; in the launcher pod. This delay, on one hand, can be tweaked by cluster admin and on the other hand, can be considered as tiny compared to the training elapsed time or the time for Elastic Horovod to handle worker changes.&lt;/p&gt;

&lt;h2 id=&quot;demo&quot;&gt;Demo&lt;/h2&gt;

&lt;p&gt;We present a demo to show how to operate an Elastic Horovod job with MPI Operator.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash-5.0&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; ./tensorflow-mnist-elastic.yaml
mpijob.kubeflow.org/tensorflow-mnist-elastic 
createdbash-5.0&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get po
NAME    READY   STATUS    RESTARTS  AGE
tensorflow-mnist-elastic-launcher   1/1     Running   0          14s
tensorflow-mnist-elastic-worker-0   1/1     Running   0          14s
tensorflow-mnist-elastic-worker-1   1/1     Running   0          14s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The job is created with two workers. After the training begins, we change &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MPIJob.Spec.MPIReplicaSpecs[&quot;Worker&quot;].Replicas&lt;/code&gt; to &lt;strong&gt;3&lt;/strong&gt;, adding another worker. Let’s check how the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;discover_hosts.sh&lt;/code&gt; changes:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash-5.0&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl &lt;span class=&quot;nb&quot;&gt;exec &lt;/span&gt;tensorflow-mnist-elastic-launcher &lt;span class=&quot;nt&quot;&gt;--&lt;/span&gt; /etc/mpi/discover_hosts.sh
tensorflow-mnist-elastic-worker-0:1
tensorflow-mnist-elastic-worker-1:1
bash-5.0&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; ./patch_r3.yaml
spec:
  mpiReplicaSpecs:
    &lt;span class=&quot;s2&quot;&gt;&quot;Worker&quot;&lt;/span&gt;:
      replicas: 3
bash-5.0&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl patch mpijob tensorflow-mnist-elastic &lt;span class=&quot;nt&quot;&gt;--patch&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cat &lt;/span&gt;patch_r3.yaml&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;merge
mpijob.kubeflow.org/tensorflow-mnist-elastic patched
bash-5.0&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl &lt;span class=&quot;nb&quot;&gt;exec &lt;/span&gt;tensorflow-mnist-elastic-launcher &lt;span class=&quot;nt&quot;&gt;--&lt;/span&gt; /etc/mpi/discover_hosts.sh
tensorflow-mnist-elastic-worker-0:1
tensorflow-mnist-elastic-worker-1:1
tensorflow-mnist-elastic-worker-2:1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We reduce the replica count to 1, retrieving 2 worker instances.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash-5.0&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; ./patch_r1.yaml
spec:
  mpiReplicaSpecs:
    &lt;span class=&quot;s2&quot;&gt;&quot;Worker&quot;&lt;/span&gt;:
      replicas: 1
bash-5.0&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl patch mpijob tensorflow-mnist-elastic &lt;span class=&quot;nt&quot;&gt;--patch&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cat &lt;/span&gt;patch_r1.yaml&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;merge
mpijob.kubeflow.org/tensorflow-mnist-elastic patched
bash-5.0&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get po
NAME               READY   STATUS        RESTARTS   AGE
tensorflow-mnist-elastic-launcher   1/1     Running       0          4m48s
tensorflow-mnist-elastic-worker-0   1/1     Running       0          4m48s
tensorflow-mnist-elastic-worker-1   1/1     Terminating   0          4m48s
tensorflow-mnist-elastic-worker-2   1/1     Terminating   0          2m21s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The elastic training persists.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Thu Mar 11 01:53:18 2021[1]&amp;lt;stdout&amp;gt;:Step &lt;span class=&quot;c&quot;&gt;#40    Loss: 0.284265&lt;/span&gt;
Thu Mar 11 01:53:18 2021[0]&amp;lt;stdout&amp;gt;:Step &lt;span class=&quot;c&quot;&gt;#40    Loss: 0.259497&lt;/span&gt;
Thu Mar 11 01:53:18 2021[2]&amp;lt;stdout&amp;gt;:Step &lt;span class=&quot;c&quot;&gt;#40    Loss: 0.229993&lt;/span&gt;
Thu Mar 11 01:54:27 2021[2]&amp;lt;stderr&amp;gt;:command terminated with &lt;span class=&quot;nb&quot;&gt;exit &lt;/span&gt;code 137
Process 2 &lt;span class=&quot;nb&quot;&gt;exit &lt;/span&gt;with status code 137.
Thu Mar 11 01:54:27 2021[0]&amp;lt;stderr&amp;gt;:command terminated with &lt;span class=&quot;nb&quot;&gt;exit &lt;/span&gt;code 137
Process 0 &lt;span class=&quot;nb&quot;&gt;exit &lt;/span&gt;with status code 137.
Thu Mar 11 01:54:57 2021[1]&amp;lt;stderr&amp;gt;:[2021-03-11 01:54:57.532928: E /tmp/pip-install-2jy0u7mn/horovod/horovod/common/operations.cc:525] Horovod background loop uncaught exception: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/tmp/pip-install-2jy0u7mn/horovod/third_party/compatible_gloo/gloo/transport/tcp/pair.cc:575] Connection closed by peer &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;10.244.2.27]:54432
WARNING:root:blacklist failing host: tensorflow-mnist-elastic-worker-2
WARNING:root:blacklist failing host: tensorflow-mnist-elastic-worker-1
Thu Mar 11 01:54:58 2021[1]&amp;lt;stdout&amp;gt;:Step &lt;span class=&quot;c&quot;&gt;#50    Loss: 0.207741&lt;/span&gt;
Thu Mar 11 01:55:00 2021[1]&amp;lt;stdout&amp;gt;:Step &lt;span class=&quot;c&quot;&gt;#60    Loss: 0.119361&lt;/span&gt;
Thu Mar 11 01:55:02 2021[1]&amp;lt;stdout&amp;gt;:Step &lt;span class=&quot;c&quot;&gt;#70    Loss: 0.131966&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As we can see, Elastic Horovod on MPI-Operator now supports tweaking worker replicas dynamically. As a future work, we aim to support  &lt;a href=&quot;https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Horizontal Pod Autoscaler&lt;/code&gt;&lt;/a&gt; to MPIJob as well as other features like designated worker deletion.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;When the concept of cloud native and distributed training fuse to elastic training on Kubernetes, it lowers the cost and gives robustness and flexibility. As a team, we are working with PyTorch, Horovod and other communities to propel elastic training. We wish to further introduce our work on elasticity with PS/Worker training mode, optimization for resource and job priority and other topics on cloud native AI.&lt;/p&gt;</content><author><name>&lt;a href='https://www.linkedin.com/in/gaocegege/'&gt;Ce Gao&lt;/a&gt;, &lt;a href='https://www.linkedin.com/in/wang-zhang'&gt;Wang Zhang&lt;/a&gt;</name></author><category term="elastic training" /><category term="operators" /><summary type="html">With increase in the size of dataset and deep learning models, distributed training emerges as the mainstream approach for training neural network models in industry. While it is feasible now to launch a massive distributed training job on Kubernetes with Kubeflow, advanced features like elastic workload and other cost mitigation approaches remain leashed when we talk about deep learning jobs on Kubernetes.</summary></entry><entry><title type="html">Kubeflow Katib: Scalable, Portable and Cloud Native System for AutoML</title><link href="https://blog.kubeflow.org/katib/" rel="alternate" type="text/html" title="Kubeflow Katib: Scalable, Portable and Cloud Native System for AutoML" /><published>2021-03-10T00:00:00-06:00</published><updated>2021-03-10T00:00:00-06:00</updated><id>https://blog.kubeflow.org/katib-0.10</id><content type="html" xml:base="https://blog.kubeflow.org/katib/">&lt;p&gt;As machine learning (ML) architectures are increasing in complexity, it is
becoming important to find the optimal hyperparameters and architecture for
ML models. &lt;a href=&quot;https://en.wikipedia.org/wiki/Automated_machine_learning&quot;&gt;Automated machine learning (AutoML)&lt;/a&gt;
has become a crucial step in the ML lifecycle.
Katib provides AutoML features in Kubeflow in a Kubernetes native way.&lt;/p&gt;

&lt;p&gt;Katib is an open source project which is agnostic to ML frameworks. It can tune
hyperparameters in applications written in any language of the user’s choice and
natively supports many ML frameworks, such as TensorFlow, Keras, PyTorch, MPI,
MXNet, XGBoost, scikit-learn, and others. Katib improves business results by
efficiently building more accurate models and lowering operational and
infrastructure costs. Katib can be deployed on local machines, or hosted as a
service in on-premise data centers, or in private/public clouds.&lt;/p&gt;

&lt;p&gt;Katib offers a rich set of features accessible via APIs. By using these APIs,
Katib is natively integrated to Kubeflow Notebooks and Pipelines. Katib supports
&lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperparameter_optimization&quot;&gt;Hyperparameter optimization (HP)&lt;/a&gt;,
&lt;a href=&quot;https://en.wikipedia.org/wiki/Neural_architecture_search&quot;&gt;Neural Architecture Search (NAS)&lt;/a&gt;,
and &lt;a href=&quot;https://en.wikipedia.org/wiki/Early_stopping&quot;&gt;Early Stopping&lt;/a&gt;. Early Stopping
feature can be used without any significant changes in the current Katib Experiments.&lt;/p&gt;

&lt;p&gt;Furthermore, Katib is a unique system which supports all
&lt;a href=&quot;https://kubernetes.io/docs/concepts/workloads/&quot;&gt;Kubernetes workloads&lt;/a&gt; and
Kubernetes &lt;a href=&quot;https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/&quot;&gt;custom resource definition (CRD)&lt;/a&gt;
to perform Katib Experiments. Since Katib can execute various Kubernetes resources,
users are able to run not only ML models optimization Experiments. They can also
enhance any software, code or program to make it more efficient with optimization
algorithms provided by Katib.&lt;/p&gt;

&lt;p&gt;We are continually working on the new Katib UI to provide a better User
Experience and native integration with Kubeflow central dashboard.
Please check &lt;a href=&quot;https://youtu.be/OKqx3IS2_G4?list=PLmzRWLV1CK_ypvsQu10SGRmhf2S7mbYL5&quot;&gt;this presentation&lt;/a&gt;
to know more about the new UI.&lt;/p&gt;

&lt;p&gt;All of the above mentioned features allow users to easily integrate Katib in
their ML infrastructure pipeline.&lt;/p&gt;

&lt;h2 id=&quot;system-architecture&quot;&gt;System Architecture&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-03-10-katib-0.10/architecture.png&quot; width=&quot;&quot; alt=&quot;Architecture&quot; title=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There are 3 main concepts in Katib which are Kubernetes CRDs:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Experiment&lt;/strong&gt; - a single optimization run with objective, search space, and
search algorithm.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Suggestion&lt;/strong&gt; - set of hyperparameters, which are produced by a user’s
selected search algorithm. Katib creates Trials to evaluate them.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Trial&lt;/strong&gt; - one iteration of the hyperparameters tuning process. Trial runs
the worker job which corresponds to the training job. Since Trial is an
abstraction on top of the worker job, any Kubernetes resource can be used to
perform the training job. For example, &lt;a href=&quot;https://github.com/kubeflow/tf-operator&quot;&gt;&lt;strong&gt;TFJob&lt;/strong&gt;&lt;/a&gt;,
&lt;a href=&quot;https://github.com/kubeflow/mpi-operator&quot;&gt;&lt;strong&gt;MPIJob&lt;/strong&gt;&lt;/a&gt; or even
&lt;a href=&quot;https://github.com/tektoncd/pipeline&quot;&gt;Tekton &lt;strong&gt;Pipeline&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By using above resources Katib follows the following steps, which are marked
in the diagram above:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Once an Experiment is submitted, the Experiment controller creates an
appropriate Suggestion object.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The Suggestion controller creates an AutoML algorithm service based on this
Suggestion object. When the algorithm service is ready, the Suggestion controller
calls the service to get new parameters and appends them to the Suggestion object.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The Experiment controller finds that Suggestion object has been updated and
creates a corresponding Trial object for each set of parameters.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The Trial controller generates a worker job for each Trial object and watches
for the status of each job. The worker job based on the Trial template.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Once the worker job has been completed, the metrics collector gets the metrics
from the job and persists them in the database.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The Experiment controller sends the metrics results to the algorithm service
and gets new parameters from the Suggestion object.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;custom-kubernetes-resources-support&quot;&gt;Custom Kubernetes resources support&lt;/h2&gt;

&lt;p&gt;Katib version 0.10 implements a new feature to support any Kubernetes
CRDs or Kubernetes workloads as a Katib Trial template. Therefore, there is no
need to manually modify the Katib controller to use CRD as a Trial. As long as
the CRD creates Kubernetes Pods, allows injecting the
&lt;a href=&quot;https://kubernetes.io/docs/concepts/workloads/pods/&quot;&gt;sidecar container&lt;/a&gt;
on these Pods, and has success and failure status, the CRD can be used in Katib.&lt;/p&gt;

&lt;p&gt;Here are the motivations behind this feature:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Katib Trial template supports only a limited type of Kubernetes resource
(&lt;a href=&quot;https://kubernetes.io/docs/concepts/workloads/controllers/job/&quot;&gt;&lt;strong&gt;BatchJob&lt;/strong&gt;&lt;/a&gt;,
&lt;a href=&quot;https://www.kubeflow.org/docs/components/training/tftraining/&quot;&gt;&lt;strong&gt;TFJob&lt;/strong&gt;&lt;/a&gt; and
&lt;a href=&quot;https://www.kubeflow.org/docs/components/training/pytorch/&quot;&gt;&lt;strong&gt;PyTorchJob&lt;/strong&gt;&lt;/a&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Many Katib users might have their own CRDs which they want to use as a Trial template.
Thus, the approach of updating the Katib controller for the new CRD is not scalable.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Some CRDs might have Go packages versions which are incompatible with the Katib controller
packages. For such cases, it is impossible to build a Katib controller image.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Users have to build and maintain a custom image version for the Katib controller
if they want to implement a new CRD in Katib.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The above problems led to the creation of a scalable and portable solution for the
Trial template. This solution allows users to modify Katib components and to add
their CRDs without changing the Katib controller image.&lt;/p&gt;

&lt;p&gt;Katib now supports Tekton &lt;strong&gt;Pipeline&lt;/strong&gt; and &lt;strong&gt;MPIJob&lt;/strong&gt; in addition to
&lt;strong&gt;BatchJob&lt;/strong&gt;, &lt;strong&gt;TFJob&lt;/strong&gt; and &lt;strong&gt;PyTorchJob&lt;/strong&gt;. In the case of Tekton &lt;strong&gt;Pipeline&lt;/strong&gt;,
a user is able to build a complex workflow inside the Trial’s worker job.
The user also can implement data preprocessing and postprocessing with all of
the Tekton &lt;strong&gt;Pipeline&lt;/strong&gt; features. Eventually, Katib’s metrics collector parses
and saves the appropriate metrics from the training processes to the database.&lt;/p&gt;

&lt;h3 id=&quot;support-new-kubernetes-crd-in-katib&quot;&gt;Support new Kubernetes CRD in Katib&lt;/h3&gt;

&lt;p&gt;To support new Kubernetes CRD, Katib components need to be modified before installing in
the Kubernetes cluster. To make this modification, it is necessary to know:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;what API group, version, and kind the Kubernetes CRD has, and&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;which Kubernetes resources the CRD’s controller creates.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Check the
&lt;a href=&quot;https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/&quot;&gt;Kubernetes guide&lt;/a&gt;
to know more about CRDs.&lt;/p&gt;

&lt;p&gt;Follow these two simple steps to integrate new CRD in Katib:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Modify
the Katib controller &lt;a href=&quot;https://github.com/kubeflow/katib/blob/master/manifests/v1beta1/components/controller/controller.yaml#L26&quot;&gt;Deployment’s arguments&lt;/a&gt;
with the new flag:&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;--trial-resources=&amp;lt;object-kind&amp;gt;.&amp;lt;object-API-version&amp;gt;.&amp;lt;object-API-group&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;For example, to support Tekton &lt;strong&gt;Pipeline&lt;/strong&gt;:&lt;/p&gt;

    &lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;. . .&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;katib-controller&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;docker.io/kubeflowkatib/katib-controller&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;./katib-controller&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;--webhook-port=8443&quot;&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;--trial-resources=Job.v1.batch&quot;&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;--trial-resources=TFJob.v1.kubeflow.org&quot;&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;--trial-resources=PyTorchJob.v1.kubeflow.org&quot;&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;--trial-resources=MPIJob.v1.kubeflow.org&quot;&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;--trial-resources=PipelineRun.v1beta1.tekton.dev&quot;&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;. . .&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Modify the Katib controller
&lt;a href=&quot;https://github.com/kubeflow/katib/blob/master/manifests/v1beta1/components/controller/rbac.yaml#L5&quot;&gt;ClusterRole’s rules&lt;/a&gt;
with the new rule to give Katib an access to all Kubernetes resources that
are created by the CRD’s controller. To know more about ClusterRole,
please check the &lt;a href=&quot;https://kubernetes.io/docs/reference/access-authn-authz/rbac/#role-and-clusterrole&quot;&gt;Kubernetes guide&lt;/a&gt;.&lt;/p&gt;

    &lt;p&gt;For example, for the Tekton &lt;strong&gt;Pipeline&lt;/strong&gt;, Trial creates Tekton &lt;strong&gt;PipelineRun&lt;/strong&gt;,
then Tekton &lt;strong&gt;PipelineRun&lt;/strong&gt; creates Tekton &lt;strong&gt;TaskRun&lt;/strong&gt;.
Therefore, Katib controller ClusterRole should have an access to the &lt;em&gt;pipelineruns&lt;/em&gt; and &lt;em&gt;taskruns&lt;/em&gt;:&lt;/p&gt;

    &lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ClusterRole&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io/v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;katib-controller&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;rules&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;apiGroups&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;tekton.dev&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;pipelineruns&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;taskruns&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;verbs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;*&quot;&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;apiGroups&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubeflow.org&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;tfjobs&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;pytorchjobs&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mpijobs&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;verbs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;*&quot;&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;. . .&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Install Katib by following
&lt;a href=&quot;https://www.kubeflow.org/docs/components/katib/hyperparameter/#installing-katib&quot;&gt;the getting started guide&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;At this point, the Kubernetes CRD can be used in the Katib Trial template. Check
&lt;a href=&quot;https://github.com/kubeflow/katib/tree/master/examples/v1beta1/tekton&quot;&gt;this guide&lt;/a&gt;
to know more about Tekton and Katib integration.&lt;/p&gt;

&lt;h2 id=&quot;early-stopping&quot;&gt;Early Stopping&lt;/h2&gt;

&lt;p&gt;Early Stopping is now supported in the Katib 0.10 release. Early Stopping is one
of the essential steps for doing HP tuning. It helps to avoid overfitting when
the model is training during Katib Experiments.&lt;/p&gt;

&lt;p&gt;Using Early Stopping helps to save compute resources and to reduce the
Experiment execution time by stopping the Experiment’s Trials when the target
metric(s) no longer improves before the training process is complete.&lt;/p&gt;

&lt;p&gt;The major advantage of using Early Stopping in Katib is that
&lt;a href=&quot;https://www.kubeflow.org/docs/components/katib/experiment/#packaging-your-training-code-in-a-container-image&quot;&gt;the training container package&lt;/a&gt;
doesn’t need to be modified. Basically, the Experiment’s YAML has to be extended
with the new entity - &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;earlyStopping&lt;/code&gt;, which is similar to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;algorithm&lt;/code&gt; YAML section:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;kubeflow.org/v1beta1&quot;&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Experiment&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubeflow&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;median-stop&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;algorithm&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;algorithmName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;random&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;earlyStopping&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;algorithmName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;medianstop&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;algorithmSettings&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;min_trials_required&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;3&quot;&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;start_step&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;5&quot;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;objective&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;maximize&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;goal&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.99&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;objectiveMetricName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Validation-accuracy&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;additionalMetricNames&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Train-accuracy&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;parallelTrialCount&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;maxTrialCount&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;15&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;maxFailedTrialCount&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;. . .&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Currently, Katib supports the Median Stopping Rule. The Medium Stopping rule
stops a running Trial at the step &lt;strong&gt;S&lt;/strong&gt; if the Trial’s best objective value is
lower than the median value of all succeeded Trials’ objectives reported up to
that step &lt;strong&gt;S&lt;/strong&gt;. Readers interested in learning more about the Median Stopping
Rule can check the
&lt;a href=&quot;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46180.pdf&quot;&gt;Google Vizier: A Service for Black-Box Optimization&lt;/a&gt;
paper.&lt;/p&gt;

&lt;p&gt;To know more about using Early Stopping in Katib please follow the
&lt;a href=&quot;https://www.kubeflow.org/docs/components/katib/early-stopping/&quot;&gt;official guide&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;getting-involved&quot;&gt;Getting Involved&lt;/h2&gt;

&lt;p&gt;First of all, thanks a lot to our contributors
(&lt;a href=&quot;https://github.com/sperlingxx&quot;&gt;Alfred Xu (Nvidia)&lt;/a&gt;,
&lt;a href=&quot;https://github.com/andreyvelich&quot;&gt;Andrey Velichkevich (Cisco)&lt;/a&gt;,
&lt;a href=&quot;https://github.com/akirillov&quot;&gt;Anton Kirillov (Mesosphere)&lt;/a&gt;,
&lt;a href=&quot;https://github.com/gaocegege&quot;&gt;Ce Gao (Tencent Cloud)&lt;/a&gt;,
&lt;a href=&quot;https://github.com/ChenjunZou&quot;&gt;Chenjun Zou (Alibaba)&lt;/a&gt;,
&lt;a href=&quot;https://github.com/eliaskoromilas&quot;&gt;Elias Koromilas (InAccel)&lt;/a&gt;,
&lt;a href=&quot;https://github.com/xuhdev&quot;&gt;Hong Xu (IBM)&lt;/a&gt;,
&lt;a href=&quot;https://github.com/johnugeorge&quot;&gt;Johnu George (Cisco)&lt;/a&gt;,
&lt;a href=&quot;https://github.com/c-bata&quot;&gt;Masashi Shibata&lt;/a&gt;,
&lt;a href=&quot;https://github.com/vpavlin&quot;&gt;Vaclav Pavlin (Red Hat)&lt;/a&gt;,
&lt;a href=&quot;https://github.com/PatrickXYS&quot;&gt;Yao Xiao (AWS)&lt;/a&gt;,
&lt;a href=&quot;https://github.com/terrytangyuan&quot;&gt;Yuan Tang (Ant Group)&lt;/a&gt;) who helped with the
0.10 release. Our community is growing and we are inviting new users and AutoML
enthusiasts to contribute to the Katib project. The following links provide
information about getting involved in the community:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Subscribe
&lt;a href=&quot;https://calendar.google.com/calendar/u/0/r?cid=ZDQ5bnNpZWZzbmZna2Y5MW8wdThoMmpoazRAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&quot;&gt;to the calendar&lt;/a&gt;
to attend the AutoML WG community meeting.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Check
&lt;a href=&quot;https://docs.google.com/document/d/1MChKfzrKAeFRtYqypFbMXL6ZIc_OgijjkvbqmwRV-64/edit&quot;&gt;the AutoML WG meeting notes&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Join
&lt;a href=&quot;https://kubeflow.slack.com/archives/C018PMV53NW&quot;&gt;the AutoML WG Slack channel&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Check
&lt;a href=&quot;https://github.com/kubeflow/katib/blob/master/ADOPTERS.md&quot;&gt;the Katib adopters list&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Learn more about Katib in
&lt;a href=&quot;https://github.com/kubeflow/katib/blob/master/docs/presentations.md&quot;&gt;the presentations and demos list&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Please let us know about the active use-cases, feature requests and questions
in the AutoML Slack channel or submit a new
&lt;a href=&quot;https://github.com/kubeflow/katib/issues/new/choose&quot;&gt;GitHub issue&lt;/a&gt;. To know
more about the new Katib UI or to track the current integration process
please check &lt;a href=&quot;https://github.com/kubeflow/katib/projects/1&quot;&gt;the GitHub project&lt;/a&gt;.
We are planning to arrange a webinar and tutorial session for using AutoML in
Kubeflow soon. Please join the
&lt;a href=&quot;https://groups.google.com/g/kubeflow-discuss&quot;&gt;kubeflow-discuss mailing list&lt;/a&gt;
to know more about it.&lt;/p&gt;

&lt;p&gt;Special Thanks to Amit Saha (Cisco), Ce Gao (Tencent Cloud), Johnu George (Cisco),
Jorge Castro (Arrikto), Josh Bottum (Arrikto) for their help on this blog.&lt;/p&gt;</content><author><name>&lt;a href='https://www.linkedin.com/in/andrey-velichkevich/'&gt;Andrey Velichkevich&lt;/a&gt;</name></author><category term="katib" /><category term="release" /><category term="automl" /><category term="hyperparameter tuning" /><summary type="html">As machine learning (ML) architectures are increasing in complexity, it is becoming important to find the optimal hyperparameters and architecture for ML models. Automated machine learning (AutoML) has become a crucial step in the ML lifecycle. Katib provides AutoML features in Kubeflow in a Kubernetes native way.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.kubeflow.org/images/logo.png" /><media:content medium="image" url="https://blog.kubeflow.org/images/logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Operationalize, Scale and Infuse Trust in AI Models using KFServing</title><link href="https://blog.kubeflow.org/release/official/2021/03/08/kfserving-0.5.html" rel="alternate" type="text/html" title="Operationalize, Scale and Infuse Trust in AI Models using KFServing" /><published>2021-03-08T00:00:00-06:00</published><updated>2021-03-08T00:00:00-06:00</updated><id>https://blog.kubeflow.org/release/official/2021/03/08/kfserving-0.5</id><content type="html" xml:base="https://blog.kubeflow.org/release/official/2021/03/08/kfserving-0.5.html">&lt;h4 id=&quot;by-animesh-singh-and-dan-sun&quot;&gt;By &lt;strong&gt;Animesh Singh&lt;/strong&gt; and &lt;strong&gt;Dan Sun&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;With inputs from : KFServing WG, including Yuzhui Liu, Tommy Li, Paul Vaneck, Andrew Butler, Srinivasan Parthasarathy etc.&lt;/p&gt;

&lt;p&gt;Machine Learning has become a key technology in a wide range of industries and organizations. One key aspect in ML landscape is that more and more models are getting produced, but are they actually getting deployed? And if they are getting deployed, are there enough robust operational mechanisms in place to understand model predictions, and monitor for drift, accuracy, anamoly, bias etc.? One key aspect of deploying models in production is being able to monitor the predictions for various metrics, and explaining the decisions the model is making, and producing quality metrics, more so in regulated industries like finance, healthcare, government sector etc. Additionally based on those metrics do we have a technology in place to understand the metrics and take corrective actions e.g. doing canary rollouts?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/kubeflow/kfserving&quot;&gt;&lt;u&gt;KFServing&lt;/u&gt;&lt;/a&gt;, a project which originated in the Kubeflow community, has been hard at work solving production model serving use cases by providing performant, high abstraction interfaces for common ML frameworks like Tensorflow, XGBoost, ScikitLearn, PyTorch, and ONNX. It encapsulates the complexity of autoscaling, networking, health checking, and server configuration to bring cutting edge serving features like GPU Autoscaling, Scale to Zero, and Canary Rollouts to model deployments. We just released KFServing v0.5 with other various features to address the model operationalization and trust needs. Additionally, the team has been hard at work to make AI explainability a core construct of the deployed models, by integrating with various industry leading technologies.&lt;/p&gt;

&lt;h3 id=&quot;kfserving-beta-api-and-v2-next-gen-inference-protocol&quot;&gt;&lt;strong&gt;KFServing Beta API and V2 (next gen) Inference Protocol&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;KFServing 0.5 has promoted the control plane API from v1alpha2 to &lt;a href=&quot;https://github.com/kubeflow/kfserving/blob/master/docs/apis/v1beta1/README.md&quot;&gt;&lt;u&gt;stable v1beta1&lt;/u&gt;&lt;/a&gt; and started to support the data plane &lt;a href=&quot;https://github.com/kubeflow/kfserving/tree/master/docs/predict-api/v2&quot;&gt;&lt;u&gt;V2 inference protocol&lt;/u&gt;&lt;/a&gt;. The v1beta1 control plane API enables a simple, data scientist-friendly interface, while providing the flexibility of specifying container and pod template fields for pre-packaged model servers. The V2 inference protocol pushes a standard and easy-to-use high performance REST/gRPC API across multiple model servers, such as Triton and MLServer, to increase the portability of the model ensuring the client/server can operate seamlessly.&lt;/p&gt;

&lt;p&gt;KFServing 0.5 also introduces an optional model agent for request/response logging, request batching, and model pulling. The model agent sits alongside as a sidecar to the model server. Pre-packaged Model servers plugged onto KFServing can benefit from these common model serving features, as well as the model servers built using custom frameworks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-03-08-kfserving-0.5/image1.png&quot; style=&quot;width:6.5in;height:3.26389in&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;whats-new&quot;&gt;What’s New?&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://twitter.com/PyTorch/status/1346466660744511492&quot;&gt;&lt;u&gt;TorchServe integration&lt;/u&gt;&lt;/a&gt;: TorchServe now is used as implementation for KFServing PyTorch model server, it also enables model explanability with &lt;a href=&quot;https://captum.ai/&quot;&gt;&lt;u&gt;Captum&lt;/u&gt;&lt;/a&gt;, see TorchServe examples &lt;a href=&quot;https://github.com/kubeflow/kfserving/tree/master/docs/samples/v1beta1/torchserve&quot;&gt;&lt;u&gt;here&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Triton Inference Server V2 inference REST/gRPC protocol support, see examples of serving &lt;a href=&quot;https://github.com/kubeflow/kfserving/tree/master/docs/samples/v1beta1/triton/bert&quot;&gt;&lt;u&gt;BERT&lt;/u&gt;&lt;/a&gt; and &lt;a href=&quot;https://github.com/kubeflow/kfserving/tree/master/docs/samples/v1beta1/triton/torchscript&quot;&gt;&lt;u&gt;TorchScript&lt;/u&gt;&lt;/a&gt; models on GPUs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Tensorflow &lt;a href=&quot;https://github.com/kubeflow/kfserving/tree/master/docs/samples/v1beta1/tensorflow#create-the-inferenceservice-with-grpc&quot;&gt;&lt;u&gt;gRPC support&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/kubeflow/kfserving/tree/master/docs/samples/v1beta1/sklearn&quot;&gt;&lt;u&gt;SKLearn&lt;/u&gt;&lt;/a&gt;/&lt;a href=&quot;https://github.com/kubeflow/kfserving/tree/master/docs/samples/v1beta1/xgboost&quot;&gt;&lt;u&gt;XGBoost&lt;/u&gt;&lt;/a&gt; model server now uses &lt;a href=&quot;https://github.com/SeldonIO/MLServer&quot;&gt;&lt;u&gt;MLServer&lt;/u&gt;&lt;/a&gt; which supports v2 inference protocol.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;New model servers added for &lt;a href=&quot;https://github.com/kubeflow/kfserving/tree/master/docs/samples/v1beta1/pmml&quot;&gt;&lt;u&gt;pmml&lt;/u&gt;&lt;/a&gt; and &lt;a href=&quot;https://github.com/kubeflow/kfserving/tree/master/docs/samples/v1beta1/lightgbm&quot;&gt;&lt;u&gt;lightgbm&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;You can now specify container or pod template level fields on the pre-packaged model servers (e.g., &lt;a href=&quot;https://github.com/kubeflow/kfserving/blob/master/docs/samples/v1beta1/triton/torchscript/torchscript.yaml&quot;&gt;&lt;u&gt;env variables&lt;/u&gt;&lt;/a&gt;, &lt;a href=&quot;https://github.com/kubeflow/kfserving/blob/master/docs/samples/v1beta1/advanced/probes.yaml&quot;&gt;&lt;u&gt;readiness/liveness probes&lt;/u&gt;&lt;/a&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Allow specifying &lt;a href=&quot;https://github.com/kubeflow/kfserving/blob/master/docs/samples/v1beta1/advanced/timeout.yaml&quot;&gt;&lt;u&gt;timeouts on the component spec&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/kubeflow/kfserving/tree/master/docs/samples/v1beta1/rollout&quot;&gt;&lt;u&gt;Simplified canary rollout&lt;/u&gt;&lt;/a&gt;, you no longer need to specify both the default and canary specs on the InferenceService spec; KFServing now automatically tracks the last rolled out revision and automatically splits the traffic between the latest ready revision and last rolled out revision.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The transformer to predictor call now defaults to using &lt;a href=&quot;https://github.com/kubeflow/kfserving/blob/master/python/kfserving/kfserving/kfmodel.py#L59&quot;&gt;&lt;u&gt;AsyncIO&lt;/u&gt;&lt;/a&gt;, which significantly improves the latency/throughput for high concurrent workload use cases.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;kfserving-multi-model-serving-to-enable-massive-scalability&quot;&gt;&lt;strong&gt;KFServing Multi-Model Serving to enable massive scalability&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;With machine learning approaches becoming more widely adopted in organizations, there is a trend to deploy a large number of models. The original design of KFServing deploys one model per InferenceService. But when dealing with a large number of models, its ‘one model, one server’ paradigm presents challenges on a Kubernetes cluster to deploy hundreds of thousands of models. To scale the number of models, we have to scale the number of InferenceServices, something that can quickly challenge the cluster’s limits.&lt;/p&gt;

&lt;p&gt;Multi-model serving is an &lt;a href=&quot;https://kubernetes.io/docs/reference/using-api/#api-versioning&quot;&gt;&lt;u&gt;alpha&lt;/u&gt;&lt;/a&gt; feature added in 0.5 to increase KFServing’s scalability. To learn more about multi-model serving motivations and implementation deatils, &lt;a href=&quot;https://github.com/kubeflow/kfserving/blob/master/docs/MULTIMODELSERVING_GUIDE.md&quot;&gt;dive into the details in KFServing github&lt;/a&gt;. Please assume that the interface is subject to change. The experimental feature must be enabled from the inference service configmap.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-03-08-kfserving-0.5/image2.png&quot; style=&quot;width:6.5in;height:2.31944in&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Multi-model serving will work with any model server that implements KFServing’s &lt;a href=&quot;https://github.com/triton-inference-server/server/tree/master/docs/protocol&quot;&gt;&lt;u&gt;V2 protocol&lt;/u&gt;&lt;/a&gt;. More specifically, if the model server implements the &lt;a href=&quot;https://github.com/triton-inference-server/server/blob/master/docs/protocol/extension_model_repository.md#load&quot;&gt;&lt;u&gt;load&lt;/u&gt;&lt;/a&gt; and &lt;a href=&quot;https://github.com/triton-inference-server/server/blob/master/docs/protocol/extension_model_repository.md#unload&quot;&gt;&lt;u&gt;unload&lt;/u&gt;&lt;/a&gt; endpoint, then it can use KFServing’s TrainedModel. Currently, the supported model servers are Triton, SKLearn, and XGBoost. Click on &lt;a href=&quot;https://github.com/kubeflow/kfserving/tree/master/docs/samples/multimodelserving/triton&quot;&gt;&lt;u&gt;Triton&lt;/u&gt;&lt;/a&gt; or &lt;a href=&quot;https://github.com/kubeflow/kfserving/tree/master/docs/samples/multimodelserving/sklearn&quot;&gt;&lt;u&gt;SKLearn&lt;/u&gt;&lt;/a&gt; for examples on how to run Multi-Model Serving.&lt;/p&gt;

&lt;h3 id=&quot;kfserving-on-openshift&quot;&gt;&lt;strong&gt;KFServing on OpenShift&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;RedHat OpenShift is a market leader for enterprise Kubernetes distribution, and by enabling KFServing for OpenShift we have ensured that enterprises running battle hardened OpenShift platform can leverage KFServing to bring serverless model inferencing on OpenShift, including how to leverage OpenShift Service Mesh. Please follow the details here to get &lt;a href=&quot;https://github.com/kubeflow/kfserving/blob/master/docs/OPENSHIFT_GUIDE.md&quot;&gt;&lt;u&gt;KFServing running on OpenShift&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-03-08-kfserving-0.5/image3.png&quot; style=&quot;width:6.5in;height:2.86111in&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;lfai-trusted-ai-projects-on-ai-fairness-ai-explainability-and-adversarial-robustness-in-kfserving&quot;&gt;&lt;strong&gt;LFAI Trusted AI Projects on AI Fairness, AI Explainability and Adversarial Robustness in KFServing&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Trust and responsibility should be core principles of AI. &lt;a href=&quot;https://lfaidata.foundation/projects/trusted-ai/&quot;&gt;&lt;u&gt;The LF AI &amp;amp; Data Trusted AI Committee&lt;/u&gt;&lt;/a&gt; is a global group working on policies, guidelines, tools and projects to ensure the development of trustworthy AI solutions, and we have integrated LFAI AI Explainability 360, Adversarial Robustness 360 in KFServing to provide production level trusted AI capabilities. Please find more details on these integration in the following links&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/kubeflow/kfserving/blob/master/docs/samples/explanation/aix/mnist/README.md&quot;&gt;&lt;u&gt;AI Explainability 360-KFServing Integration&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/kubeflow/kfserving/tree/master/docs/samples/explanation/aif/germancredit&quot;&gt;&lt;u&gt;AI Fairness 360-KFServing Integration&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/kubeflow/kfserving/tree/master/docs/samples/explanation/art/mnist&quot;&gt;&lt;u&gt;Adversarial Robustness Toolbox-KFServing Integration&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-03-08-kfserving-0.5/image4.png&quot; style=&quot;width:6.5in;height:2.63889in&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;metrics-driven-automated-rollouts-using-iter8-in-kfserving&quot;&gt;&lt;strong&gt;Metrics driven automated rollouts using Iter8 in KFServing&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Iter8-KFServing enables metrics-driven experiments, progressive delivery, and automated rollouts for ML models served over Kubernetes and OpenShift clusters. Iter8 &lt;em&gt;experiments&lt;/em&gt; can be used to safely expose competing versions of a model to traffic while gathering and assessing metrics to intelligently shift traffic to the &lt;em&gt;winning&lt;/em&gt; version of your model. Discover how to set it up and get it running in the &lt;a href=&quot;https://github.com/iter8-tools/iter8-kfserving&quot;&gt;&lt;u&gt;Iter8-KFServing repository&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-03-08-kfserving-0.5/image5.png&quot; style=&quot;width:6.5in;height:3.65278in&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;join-us-to-build-trusted-model-inferencing-platform-on-kubernetes&quot;&gt;&lt;strong&gt;Join us to build Trusted Model Inferencing Platform on Kubernetes&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Please join us on the &lt;a href=&quot;https://github.com/kubeflow/kfserving&quot;&gt;&lt;u&gt;KFServing GitHub repository&lt;/u&gt;&lt;/a&gt;, try it out, give feedback, and raise issues. Additionally, you can connect with us via the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;To contribute and build an enterprise-grade, end-to-end machine learning platform on OpenShift and Kubernetes, please &lt;a href=&quot;https://www.kubeflow.org/&quot;&gt;&lt;u&gt;join the Kubeflow community&lt;/u&gt;&lt;/a&gt; and reach out with any questions, comments, and feedback!&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you want help deploying and managing Kubeflow on your on-premises Kubernetes platform, OpenShift, or on IBM Cloud, please &lt;a href=&quot;https://mail.google.com/mail/u/0/?view=cm&amp;amp;fs=1&amp;amp;tf=1&amp;amp;source=mailto&amp;amp;su=Kubeflow%25sInquiry&amp;amp;to=singhan@us.ibm.com&quot;&gt;&lt;u&gt;connect with us&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://opendatahub.io/&quot;&gt;&lt;u&gt;Check out the OpenDataHub&lt;/u&gt;&lt;/a&gt; if you are interested in open source projects in the Data and AI portfolio, namely Kubeflow, Kafka, Hive, Hue, and Spark, and how to bring them together in a cloud-native way.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;contributor-acknowledgement&quot;&gt;&lt;strong&gt;Contributor Acknowledgement&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;We’d like to thank all the KFServing contributors for the awesome work!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/animeshsingh&quot;&gt;Animesh Singh&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/jinchihe&quot;&gt;Jinchi He&lt;/a&gt; &lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/cliveseldon&quot;&gt;Clive Cox&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/ellistarn&quot;&gt;Ellis Tarn&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/pugangxa&quot;&gt;Pu Gang&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/Iamlovingit&quot;&gt;Qianshan Chen&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/yuzliu&quot;&gt;Yuzhui Liu&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/salanki&quot;&gt;Peter Salanki&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/jagadeeshi2i&quot;&gt;Jagadeesh&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/deadeyegoodwin&quot;&gt;David Goodwin&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/ifilonenko&quot;&gt;Ilan Filonenko&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/jazzsir&quot;&gt;Hanbae Seo&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/ryandawsonuk&quot;&gt;Ryan Dawson&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/pvaneck&quot;&gt;Paul Van Eck&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/wengyao04&quot;&gt;Weng Yao&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/theofpa&quot;&gt;Theofilos Papapanagiotou&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/tduffy000&quot;&gt;Tom Duffy&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/drewbutlerbb4&quot;&gt;Andrew Butler&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/adriangonz&quot;&gt;Adrian Gonzalez-Martin&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/njhill&quot;&gt;Nick Hill&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/PatrickXYS&quot;&gt;Yao Xiao&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/AnyISalIn&quot;&gt;AnyISalIn&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/abchoo&quot;&gt;Aaron Choo&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://github.com/mszacillo&quot;&gt;Michas Szacillo&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://github.com/yuzisun&quot;&gt;Dan Sun&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/chauhang&quot;&gt;Geeta Chauhan&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/Tomcli&quot;&gt;Tommy Li&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="release" /><category term="official" /><summary type="html">By Animesh Singh and Dan Sun</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.kubeflow.org/images/2021-03-08-kfserving-0.5/image4.png" /><media:content medium="image" url="https://blog.kubeflow.org/images/2021-03-08-kfserving-0.5/image4.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Kubeflow 1.2 release announcement</title><link href="https://blog.kubeflow.org/release/official/2020/11/18/kubeflow-1.2-blog-post.html" rel="alternate" type="text/html" title="Kubeflow 1.2 release announcement" /><published>2020-11-18T00:00:00-06:00</published><updated>2020-11-18T00:00:00-06:00</updated><id>https://blog.kubeflow.org/release/official/2020/11/18/kubeflow-1.2-blog-post</id><content type="html" xml:base="https://blog.kubeflow.org/release/official/2020/11/18/kubeflow-1.2-blog-post.html">&lt;h2 id=&quot;special-message-from-kubeflow-founders&quot;&gt;Special Message from Kubeflow Founders&lt;/h2&gt;

&lt;p&gt;Three years (!!) ago, we (Jeremy Lewi, Vish Kannan and David Aronchick) &lt;a href=&quot;https://www.youtube.com/watch?v=R3dVF5wWz-g&quot;&gt;stood on stage at Kubecon&lt;/a&gt; to introduce Kubeflow for the first time. We could not have possibly imagined what would have come about - thousands of GitHub stars, tens of thousands of commits and a community that has built the most flexible and scalable platform for machine learning. And, best of all, it’s not backed by an enormous company that requires you to “upgrade” in order to use it; we gave it all away for free! Here’s to everything you all have done and we could not be more excited about the NEXT three years (and the three years beyond that). Thank you!&lt;/p&gt;

&lt;h2 id=&quot;announcing-kubeflow-v12-release&quot;&gt;Announcing Kubeflow v1.2 release&lt;/h2&gt;

&lt;p&gt;The Kubeflow Community’s delivery of the Kubeflow 1.2 software release includes ~100 user requested enhancements to improve model building, training, tuning, ML pipelining and serving.  This post includes a Release Highlights Section, which details significant 1.2 features as contributed by the Kubeflow application working groups (WG), SIGs, and ecosystem partners.  The Kubeflow 1.2 &lt;a href=&quot;https://github.com/kubeflow/manifests/releases/tag/v1.2-rc.0&quot;&gt;changelog&lt;/a&gt; provides a quick view of the 1.2 deliveries.&lt;/p&gt;

&lt;p&gt;The Release was validated, tested and documented by the developers, and the Release is now being validated, tested and documented by users, cloud providers and commercial support partners on popular platforms i.e. AWS, Azure, GCP, IBM, etc.   The Community is working on a more sustainable approach to owning and maintaining test infrastructure.&lt;/p&gt;

&lt;p&gt;For Release 1.2, AWS has built and contributed a shared test-infra, which provides WG owners with enough permissions to identify problems, and test proposed solutions to completion. Currently, most WGs (AutoML, Training-Operators, KFServing, Deployments, Manifests) have already migrated their tests on this solution. As a result, the test-infra blocking time has fallen significantly, which is good for users and contributors.&lt;/p&gt;

&lt;h2 id=&quot;getting-involved&quot;&gt;Getting Involved&lt;/h2&gt;

&lt;p&gt;The Community continues to grow and we invite new users and contributors to join the Working Groups and Community Meetings. The following provides some helpful links to those looking to get involved with the Kubeflow Community:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Join the &lt;a href=&quot;https://app.slack.com/client/T7QLHSH6U/C7REE0EHK/thread/C7REE0EHK-1554222405.030500&quot;&gt;Kubeflow Slack channel&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Join the &lt;a href=&quot;https://groups.google.com/g/kubeflow-discuss&quot;&gt;kubeflow-discuss mailing list&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Attend a &lt;a href=&quot;https://github.com/kubeflow/community&quot;&gt;Weekly Community Meeting&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Review the Working Group Meeting Notes in Release Highlights Section (as the Notes include great discussions and meeting times)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you have questions and/or run into issues, please leverage the Kubeflow Slack channel and/or submit bugs via &lt;a href=&quot;https://github.com/kubeflow&quot;&gt;Kubeflow on GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next&lt;/h2&gt;

&lt;p&gt;The Community has started discussions on Kubeflow 1.3. Arrikto has agreed to lead the 1.3 Release Management process and the Community will continue to capture input from users and contributors as features are defined, developed and delivered. Onward and upward!&lt;/p&gt;

&lt;p&gt;Special thanks to Constantinos Venetsanopoulos (Arrikto), Animesh Singh (IBM), Jiaxin Shan (ByteDance),  Yao Xiao (AWS), David Aronchick (Azure), Dan Sun (Bloomberg), Andrey Velichkevich (Cisco), Matthew Wicks (Eliiza), Willem Pienaar (Feast), Yuan Gong (Google), James Wu (Google), Jeremy Lewi (Google), Josh Bottum (Arrikto), Chris Pavlou (Arrikto), Kimonas Sotirchos (Arrikto), Rui Vasconcelos (Canonical), Jeff Fogarty (US Bank) , Karl Shriek (AlexanderThamm), and Clive Cox (Seldon) for their help on 1.2 and this post.&lt;/p&gt;

&lt;h2 id=&quot;release-highlights-section&quot;&gt;Release Highlights Section&lt;/h2&gt;

&lt;h3 id=&quot;working-group-automl--katib&quot;&gt;Working Group: AutoML / Katib&lt;/h3&gt;

&lt;p&gt;Working Group Meeting Notes: &lt;a href=&quot;https://docs.google.com/document/d/1MChKfzrKAeFRtYqypFbMXL6ZIc_OgijjkvbqmwRV-64/edit#heading=h.yvypq06ot57p&quot;&gt;Katib Working Group Meeting Notes&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Overall benefit: Better model accuracy, Better infrastructure utilization&lt;/p&gt;

&lt;p&gt;Overall description: Katib 0.10 with the new v1beta1 API has been released in Kubeflow 1.2. Automated configuration of Hyperparameters to deliver more accuracy models that use less infrastructure, AutoML / Katib simplified the process of finding the optimized set of parameters for your model with Early Stopping techniques. Possibility to orchestrate complex pipeline during Katib Experiment with custom Kubernetes CRD support.&lt;/p&gt;

&lt;h4 id=&quot;feature-name-early-stopping&quot;&gt;Feature Name: &lt;a href=&quot;https://github.com/kubeflow/katib/pull/1344&quot;&gt;Early Stopping&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;Feature Description: Save your cluster resources by using Katib Early Stopping techniques. Allow to use the Median Stopping Rule algorithm.&lt;/p&gt;

&lt;p&gt;Feature Benefit: You don’t need to modify your training source code to use the feature! Early Stopping can be used with every Katib algorithm.&lt;/p&gt;

&lt;h4 id=&quot;feature-name-support-custom-crd-in-the-new-trial-template&quot;&gt;Feature Name: Support &lt;a href=&quot;https://github.com/kubeflow/katib/issues/1214&quot;&gt;custom CRD&lt;/a&gt; in the &lt;a href=&quot;https://github.com/kubeflow/katib/issues/1208&quot;&gt;new Trial template&lt;/a&gt;.&lt;/h4&gt;

&lt;p&gt;Feature Description: You are able to follow two simple steps to integrate your custom Kubernetes resource in Katib. Flexible way to send your hyperparameters in the new Trial template design, which is a valid YAML.&lt;/p&gt;

&lt;p&gt;Feature Benefit: Define &lt;a href=&quot;https://github.com/tektoncd/pipeline&quot;&gt;Tekton Pipeline&lt;/a&gt; in your Katib experiment. You are able to pass hyperparameters even if your model config is a JSON scikit learn Pipeline.&lt;/p&gt;

&lt;h4 id=&quot;feature-name-resume-experiments&quot;&gt;Feature Name: &lt;a href=&quot;https://github.com/kubeflow/katib/issues/1250&quot;&gt;Resume Experiments&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;Feature Description: Implementation of the various methods to resume Katib Experiments. Save the Experiment’s Suggestion data in the custom volume. Use Katib config to modify your volume settings.&lt;/p&gt;

&lt;p&gt;Feature Benefit: Free your cluster resources after your Experiment is finished.&lt;/p&gt;

&lt;h4 id=&quot;feature-name-multiple-ways-to-extract-metrics&quot;&gt;Feature Name: &lt;a href=&quot;https://github.com/kubeflow/katib/pull/1140&quot;&gt;Multiple Ways to Extract Metrics&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;Feature Description: You can specify metrics strategies for your Experiment. Katib computes the Experiment objective based on these values. You are able to view detailed metric info for each Trial.&lt;/p&gt;

&lt;p&gt;Feature Benefit: Get correct optimisation results when your model produces necessary value at the final training step.&lt;/p&gt;

&lt;h3 id=&quot;working-group-kfserving&quot;&gt;Working Group: KFServing&lt;/h3&gt;

&lt;p&gt;Working Group Meeting Notes: &lt;a href=&quot;https://docs.google.com/document/d/1KZUURwr9MnHXqHA08TFbfVbM8EAJSJjmaMhnvstvi-k/edit&quot;&gt;KFServing Working Group Meeting Notes&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Overall benefit: KFServing enables serverless inferencing on Kubernetes and provides performant, high abstraction interfaces for common machine learning (ML) frameworks like TensorFlow, XGBoost, scikit-learn, PyTorch, and ONNX to solve production model serving use cases.&lt;/p&gt;

&lt;p&gt;Overall description:  Kubeflow 1.2 includes KFServing v0.4.1, where the focus has been on enabling KFServing on OpenShift and additionally providing more features, such as adding batcher module as sidecar, Triton inference server renaming and integrations, upgrading Alibi explainer to 0.4.0, updating logger to CloudEvents V1 protocol and allowing customized URL paths on data plane. Additionally, the minimum Istio is now v1.3.1, and KNative version has been moved to KNative 0.14.3. More details can be found &lt;a href=&quot;https://github.com/kubeflow/kfserving/releases/tag/v0.4.0&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://github.com/kubeflow/kfserving/releases/tag/v0.4.1&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;feature-name-add-batcher-module-as-sidecar-847-zhangrongguo&quot;&gt;Feature Name: Add batcher module as sidecar &lt;a href=&quot;https://github.com/kubeflow/kfserving/pull/847&quot;&gt;#847&lt;/a&gt; &lt;a href=&quot;https://github.com/zhangrongguo&quot;&gt;@zhangrongguo&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;Feature Description: KFServer Batcher accepts user requests, batch them and then send to the “InferenceService”.  &lt;a href=&quot;https://docs.google.com/document/d/14aa5zRtwK1zqcXdpQc9YClTduhtj8vr9waJcm0vR2GQ/edit#&quot;&gt;Batcher Feature Description&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Feature Benefit: Faster response time to inference requests, and Improve infrastructure utilization&lt;/p&gt;

&lt;h4 id=&quot;feature-name-alibi-explainer-upgrade-to-040-803-cliveseldon&quot;&gt;Feature Name: Alibi explainer upgrade to 0.4.0 &lt;a href=&quot;https://github.com/kubeflow/kfserving/pull/803&quot;&gt;#803&lt;/a&gt; &lt;a href=&quot;https://github.com/cliveseldon&quot;&gt;@cliveseldon&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;Feature Description: The enhancements include a KernelSHAP explainer for black-box model SHAP scores and documentation for the LinearityMeasure algorithm.   This delivery includes a new API for explainer and explanation objects, which provide a variety of improvements, but are breaking changes.&lt;/p&gt;

&lt;p&gt;Feature Benefit: This delivery improves the ability to understand which features impact model accuracy along with improving operations.&lt;/p&gt;

&lt;h4 id=&quot;feature-namedescription--triton-inference-server-rename-and-integrations-747-deadeyegoodwin&quot;&gt;Feature Name/Description : Triton inference server rename and integrations &lt;a href=&quot;https://github.com/kubeflow/kfserving/pull/747&quot;&gt;#747&lt;/a&gt; &lt;a href=&quot;https://github.com/deadeyegoodwin&quot;&gt;@deadeyegoodwin&lt;/a&gt;&lt;/h4&gt;

&lt;h3 id=&quot;working-group-pipelines&quot;&gt;Working Group: Pipelines&lt;/h3&gt;

&lt;p&gt;Working Group Meeting Notes: &lt;strong&gt;&lt;a href=&quot;http://bit.ly/kfp-meeting-notes&quot;&gt;http://bit.ly/kfp-meeting-notes&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Overall benefit: Simplify process of creating a model when you have new data and new code&lt;/p&gt;

&lt;p&gt;Overall description: Kubeflow Pipelines is a platform for building and deploying portable, scalable machine learning (ML) workflows based on containers.  The Kubeflow Pipelines platform consists of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A user interface (UI) for managing and tracking experiments, jobs, and runs.&lt;/li&gt;
  &lt;li&gt;An engine for scheduling multi-step ML workflows.&lt;/li&gt;
  &lt;li&gt;An SDK for defining and manipulating pipelines and components.&lt;/li&gt;
  &lt;li&gt;Notebooks for interacting with the system using the SDK.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following are the goals of Kubeflow Pipelines:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;End-to-end orchestration: enabling and simplifying the orchestration of machine learning pipelines.&lt;/li&gt;
  &lt;li&gt;Easy experimentation: making it easy for you to try numerous ideas and techniques and manage your various trials/experiments.&lt;/li&gt;
  &lt;li&gt;Easy re-use: enabling you to re-use components and pipelines to quickly create end-to-end solutions without having to rebuild each time&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Kubeflow Pipelines is stabilizing over a few patch releases. At the same time, we made a lot of progress at standardizing the &lt;a href=&quot;https://github.com/kubeflow/pipelines/blob/master/api/v2alpha1/pipeline_spec.proto&quot;&gt;pipeline IR (intermediate representation) &lt;/a&gt; which will serve as a unified pipeline definition for different execution engines.&lt;/p&gt;

&lt;h4 id=&quot;feature-name-kubeflow-pipelines-with-tekton-backend-available&quot;&gt;Feature Name: Kubeflow Pipelines with Tekton backend available&lt;/h4&gt;

&lt;p&gt;Feature Description:  After an extensive effort, we have &lt;a href=&quot;https://github.com/kubeflow/kfp-tekton&quot;&gt;Kubeflow Pipelines running on Tekton end-to-end and available in open source&lt;/a&gt;. Additionally it’s available as default with &lt;a href=&quot;https://www.kubeflow.org/docs/ibm/pipelines/&quot;&gt;Kubeflow deployment on IBM Cloud&lt;/a&gt;, and &lt;a href=&quot;https://github.com/kubeflow/kfp-tekton/blob/master/guides/kfp_tekton_install.md#openshift&quot;&gt;can be deployed on OpenShift.&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;feature-benefit-tekton-support&quot;&gt;Feature Benefit: Tekton support&lt;/h4&gt;

&lt;p&gt;If you are an existing user of Tekton, or are a fan of Tekton, or running OpenShift Pipelines, get Kubeflow Pipelines running on top of it.  More details here &lt;br /&gt;
&lt;a href=&quot;https://developer.ibm.com/blogs/kubeflow-pipelines-with-tekton-and-watson/&quot;&gt;https://developer.ibm.com/blogs/kubeflow-pipelines-with-tekton-and-watson/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-11-18-kubeflow-1-2-announcement/kfp-tekton-support.png&quot; width=&quot;&quot; alt=&quot;Teckton Support&quot; title=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;feature-name-stabilizing-kubeflow-pipelines-10x&quot;&gt;Feature Name: stabilizing Kubeflow Pipelines 1.0.x&lt;/h4&gt;

&lt;p&gt;Feature Description: We are stabilizing Kubeflow Pipelines over a few patch releases: &lt;a href=&quot;https://github.com/kubeflow/pipelines/blob/1.0.4/CHANGELOG.md&quot;&gt;Kubeflow Pipelines 1.0.4&lt;/a&gt; Changelog ~20 fixes and ~5 minor features.&lt;/p&gt;

&lt;h3 id=&quot;working-group-notebooks&quot;&gt;Working Group: Notebooks&lt;/h3&gt;

&lt;p&gt;Working Group Meeting Notes: coming soon&lt;/p&gt;

&lt;p&gt;Overall benefit: Interactive, experimental coding environment for model development&lt;/p&gt;

&lt;p&gt;Overall description: Notebooks provide an advanced, interactive coding environment that users and teams can share and leverage kubernetes namespaces for isolation and resource utilization&lt;/p&gt;

&lt;h4 id=&quot;feature-name-affinitytoleration-configs-5237&quot;&gt;Feature Name: Affinity/Toleration configs, &lt;a href=&quot;https://github.com/kubeflow/kubeflow/pull/5237&quot;&gt;#5237&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;Feature Description: Adds the ability for Kubeflow administrators to set groups of Affinity/Toleration configs which users can pick from a dropdown.&lt;/p&gt;

&lt;p&gt;Feature Benefit: Allows more fine-grained selection of how Notebook pods are scheduled.&lt;/p&gt;

&lt;h4 id=&quot;feature-name-refactor-notebooks-web-app&quot;&gt;Feature Name: Refactor Notebooks Web App&lt;/h4&gt;

&lt;p&gt;Feature Description: The details of the refactoring are defined in these deliveries:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Common code between the different python backends,  &lt;a href=&quot;https://github.com/kubeflow/kubeflow/pull/5164&quot;&gt;#5164&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Create an Angular Library with common frontend code, &lt;a href=&quot;https://github.com/kubeflow/kubeflow/pull/5252&quot;&gt;#5252&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Refactor the JWA backend to utilize common code, &lt;a href=&quot;https://github.com/kubeflow/kubeflow/pull/5316&quot;&gt;#5316&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Initialize the Jupyter web app frontend in crud-web-apps, &lt;a href=&quot;https://github.com/kubeflow/kubeflow/pull/5332&quot;&gt;#5332&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Feature Benefit : Refactoring will enable an easier future integration with other web apps - Tensorboard, volume manager.&lt;/p&gt;

&lt;h4 id=&quot;feature-name-stop-and-restart-notebooks-while-maintaining-state-4857-5332&quot;&gt;Feature Name: Stop and Restart Notebooks while maintaining state, &lt;a href=&quot;https://github.com/kubeflow/kubeflow/issues/4857&quot;&gt;#4857&lt;/a&gt; &lt;a href=&quot;https://github.com/kubeflow/kubeflow/pull/5332&quot;&gt;#5332&lt;/a&gt;&lt;/h4&gt;
&lt;h5 id=&quot;note-the-artifacts-for-the-updated-notebooks-web-app-will-be-available-in-121-or-later&quot;&gt;NOTE: The artifacts for the updated Notebooks web app will be available in 1.2.1 or later&lt;/h5&gt;

&lt;p&gt;Feature Description: Implementation of a “shut down server” button in the central dashboard that scales the stateful set for the server down to zero and a “start server” button that scales it back up again.&lt;/p&gt;

&lt;p&gt;Feature Benefit: Save work, save infrastructure resources&lt;/p&gt;

&lt;h3 id=&quot;working-group-training-operators&quot;&gt;Working Group: Training-Operators&lt;/h3&gt;

&lt;p&gt;Working Group Meeting Notes: coming soon&lt;/p&gt;

&lt;p&gt;Overall benefit: Faster model development using operators that simplify distributed computing&lt;/p&gt;

&lt;h4 id=&quot;feature-name-the-training-operator-contributors-provided-the-following-fixes-and-improvements-in-kubeflow-12&quot;&gt;Feature Name: The Training Operator contributors provided the following fixes and improvements in Kubeflow 1.2:&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Update mxnet-operator manifest to v1 (&lt;a href=&quot;https://github.com/kubeflow/manifests/pull/1326&quot;&gt;#1326&lt;/a&gt;, &lt;a href=&quot;https://github.com/Jeffwan&quot;&gt;@Jeffwan&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Correct XGBoostJob CRD group name and add singular name (&lt;a href=&quot;https://github.com/kubeflow/manifests/pull/1313&quot;&gt;#1313&lt;/a&gt;, &lt;a href=&quot;https://github.com/terrytangyuan&quot;&gt;@terrytangyuan&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Fix XGBoost Operator manifest issue (&lt;a href=&quot;https://github.com/kubeflow/manifests/pull/1463&quot;&gt;#1463&lt;/a&gt;, &lt;a href=&quot;https://github.com/Jeffwan&quot;&gt;@Jeffwan&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Move Pytorch operator e2e tests to AWS Prow (&lt;a href=&quot;https://github.com/kubeflow/pytorch-operator/pull/305&quot;&gt;#305&lt;/a&gt;, &lt;a href=&quot;https://github.com/Jeffwan&quot;&gt;@Jeffwan&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Support BytePS in MXNet Operator (&lt;a href=&quot;https://github.com/kubeflow/mxnet-operator/pull/82&quot;&gt;#82&lt;/a&gt;, &lt;a href=&quot;https://github.com/jasonliu747&quot;&gt;@jasonliu747&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Fix error when conditions is empty in tf-operator (&lt;a href=&quot;https://github.com/kubeflow/tf-operator/pull/1185&quot;&gt;#1185&lt;/a&gt;, &lt;a href=&quot;https://github.com/Corea&quot;&gt;@Corea&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Fix successPolicy logic in MXNet Operator (&lt;a href=&quot;https://github.com/kubeflow/mxnet-operator/pull/85&quot;&gt;#85&lt;/a&gt;, &lt;a href=&quot;https://github.com/jasonliu747&quot;&gt;@jasonliu747&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;sig-model-management&quot;&gt;SIG: Model Management&lt;/h3&gt;

&lt;p&gt;Overall benefit: The ability to find model versions and their subcomponents including metadata&lt;/p&gt;

&lt;p&gt;SIG Meeting Notes: &lt;a href=&quot;https://docs.google.com/document/d/1R2r1z5O4USpn3BW29-4blCoPueB3p2LAjvdKa97fT7U/edit?usp=sharing&quot;&gt;Model Management SIG Meeting Notes&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Overall description: The SIG was initiated to define and develop a Kubeflow solution for model management, which will make it easier to organize and find models and their artifacts.   In addition, several contributors are submitting proposals on how to define data types for ML model and data, with the goal of driving wider metadata standards, and interoperability of models between ML platforms, clouds, and frameworks.  The proposals are working to define an ontology for model and data types and tooling to search and organize that metadata.&lt;/p&gt;

&lt;p&gt;Proposals from Kubeflow Pipelines contributors, the Model Management SIG, Seldon and a MLSpec from David Aronchick (Azure) are under discussion.   Please find links to those proposals below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.google.com/document/d/1YhnsMK_ktaeUKLS2XShocq4u2bcUCUcztT9rioRssOM/edit#&quot;&gt;ML Data in Kubeflow Pipelines&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/mlspec/schemas&quot;&gt;ML Spec from David Aronchick&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.google.com/document/d/1R2r1z5O4USpn3BW29-4blCoPueB3p2LAjvdKa97fT7U/edit&quot;&gt;Model Management Proposal from Karl Schriek, SIG Tech Lead&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.google.com/document/d/1yhSWRAwyB8AnxPz0FtJACHrMnj1m0g2uRiQC6GaT4UE/edit?usp=sharing&quot;&gt;Seldon’s Proposal for Initial Metadata Types&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ecosystem-seldon&quot;&gt;EcoSystem: Seldon&lt;/h3&gt;

&lt;p&gt;Overall benefit: Deploy, Scale, Update models built with Kubeflow.&lt;/p&gt;

&lt;p&gt;Overall description: Seldon handles scaling of production machine learning models and provides advanced machine learning capabilities out of the box including Advanced Metrics, Request Logging, Explainers, Outlier Detectors, A/B Tests, and Canaries.&lt;/p&gt;

&lt;p&gt;Kubeflow 1.2 comes with Seldon’s 1.4 release. This release of Seldon adds further capabilities for model deployment and inferencing including the addition of batch and streaming interfaces to a deployed model. It also allows for fine grained control of how a deployed model interfaces with Kubernetes with the addition of KEDA and Pod Disruption Budget options. Finally, it begins a process of compatibility with KFServing by allowing the usage of the V2 Dataplane supported by Seldon, KFServing and NVIDIA Triton.&lt;/p&gt;

&lt;p&gt;Version: 1.4.0&lt;/p&gt;

&lt;h4 id=&quot;feature-name--stream-and-batch-support&quot;&gt;Feature Name:  Stream and Batch support&lt;/h4&gt;

&lt;p&gt;Feature Description: Streaming support for native Kafka integration. Batch prediction support from and to cloud storage.&lt;/p&gt;

&lt;p&gt;Feature Benefit: Allows Seldon users to interact with their models via RPC, Streaming or Batch as needed.&lt;/p&gt;

&lt;h4 id=&quot;feature-name-extended-kubernetes-control-via-keda-and-pdbs&quot;&gt;Feature Name: Extended kubernetes control via KEDA and &lt;a href=&quot;https://docs.seldon.io/projects/seldon-core/en/v1.4.0_a/graph/disruption-budgets.html&quot;&gt;PDBs&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;Feature Description: Allows fine grained control of deployed models via autoscaling with KEDA metrics and addition of pod disruption budgets.&lt;/p&gt;

&lt;p&gt;Feature Benefit: Manage models at scale in a production cluster.&lt;/p&gt;

&lt;h4 id=&quot;feature-name-alpha-v2-dataplane&quot;&gt;Feature Name: &lt;a href=&quot;https://docs.seldon.io/projects/seldon-core/en/v1.4.0_a/graph/protocols.html#v2-kfserving-protocol&quot;&gt;Alpha V2 Dataplane&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;Feature Description: Run custom python models using an updated python server along with support for the V2 Dataplane (NVIDIA Triton, KFServing, Seldon)&lt;/p&gt;

&lt;p&gt;Feature Benefit: Utilize a standard powerful protocol that is supported cross project.&lt;/p&gt;

&lt;h3 id=&quot;ecosystem-kale&quot;&gt;EcoSystem: Kale&lt;/h3&gt;

&lt;p&gt;Overall benefit: Kubeflow Workflow tool that simplifies ML pipeline building and versioning directly from a Notebook or IDE i.e. VSCode&lt;/p&gt;

&lt;p&gt;Kale GitHub repo: https://github.com/kubeflow-kale/kale&lt;/p&gt;

&lt;p&gt;Kale Tutorials:  &lt;a href=&quot;https://www.arrikto.com/tutorials/&quot;&gt;https://www.arrikto.com/tutorials/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Overall description: Kale lets you deploy Jupyter Notebooks that run on your laptop or on the cloud to Kubeflow Pipelines, without requiring any of the Kubeflow SDK boilerplate. You can define pipelines just by annotating Notebook’s code cells and clicking a deployment button in the Jupyter UI. Kale will take care of converting the Notebook to a valid Kubeflow Pipelines deployment, taking care of resolving data dependencies and managing the pipeline’s lifecycle&lt;/p&gt;

&lt;h4 id=&quot;feature-name-dog-breed-classification-example&quot;&gt;Feature Name: Dog Breed Classification example&lt;/h4&gt;

&lt;p&gt;Feature Description: Tutorial for simplified pipeline to build a model for Image Classification&lt;/p&gt;

&lt;p&gt;Feature Benefit: Faster understanding of ML workflows to deliver models with hyperparameter tuning&lt;/p&gt;

&lt;h4 id=&quot;feature-name-katib-integration-with-kale&quot;&gt;Feature Name: Katib integration with Kale&lt;/h4&gt;

&lt;p&gt;Feature Description: Automated hyperparameter tuning and reproducible katib trials using pipelines&lt;/p&gt;

&lt;p&gt;Feature Benefit: Better model accuracy and easy reproducibility and debugging&lt;/p&gt;

&lt;h4 id=&quot;feature-name-pipeline-step-caching-for-katib-trials-using-kales-integration-with-rok&quot;&gt;Feature Name: Pipeline Step Caching for Katib Trials using Kale’s integration with Rok&lt;/h4&gt;

&lt;p&gt;Feature Description: Kale recognizes when a pipeline step has been run before and fetches complete results from Rok and inserts into pipeline processing&lt;/p&gt;

&lt;p&gt;Feature Benefit: Faster hyperparameter tuning, reduced infrastructure utilization&lt;/p&gt;

&lt;h3 id=&quot;ecosystem-feast&quot;&gt;EcoSystem: Feast&lt;/h3&gt;

&lt;p&gt;Overall benefit: Feast allows teams to register, ingest, serve, and monitor machine learning features in production.&lt;/p&gt;

&lt;p&gt;Working Group Meeting Notes: &lt;a href=&quot;https://tinyurl.com/kf-feast-sig&quot;&gt;https://tinyurl.com/kf-feast-sig&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Overall description: The latest release of Feast was a concerted effort by the Feast community to make Feast available in more environments than Google Cloud. We’ve removed all hard couplings to managed services and made it possible to run Feast both on AWS and locally.&lt;/p&gt;

&lt;p&gt;Version: Feast 0.8&lt;/p&gt;

&lt;h4 id=&quot;feature-name-support-for-aws&quot;&gt;Feature Name: Support for AWS&lt;/h4&gt;

&lt;p&gt;Feature Description: Feast 0.8 now comes with support for deployment on AWS, with native support for job management on EMR, and support for both S3 and Kinesis as data sources.&lt;/p&gt;

&lt;p&gt;Feature Benefit: Finally makes it possible for Kubeflow users on AWS to run Feast&lt;/p&gt;

&lt;h4 id=&quot;feature-name-batch-only-ingestion&quot;&gt;Feature Name: Batch-only ingestion&lt;/h4&gt;

&lt;p&gt;Feature Description: Allows teams to ingest data into stores without passing the data through a stream.&lt;/p&gt;

&lt;p&gt;Feature Benefit: Allows for a more performant ingestion compared to the stream-first approach.&lt;/p&gt;

&lt;h4 id=&quot;feature-name-local-only-mode&quot;&gt;Feature Name: Local-only mode&lt;/h4&gt;

&lt;p&gt;Feature Description: Makes it possible to run Feast without any external infrastructure, using only Docker Compose or Minikube&lt;/p&gt;

&lt;p&gt;Feature Benefit: Lowers the barrier to entry for new users, and makes it easier to test and develop Feast&lt;/p&gt;

&lt;h3 id=&quot;ecosystem-on-prem-sig&quot;&gt;EcoSystem: On-Prem SIG&lt;/h3&gt;

&lt;p&gt;Description: The on-prem SIG was officially created during this release with the intent to develop best practices for Kubeflow deployment in on-prem installations. With the new release, the SIG has also secured testing infrastructure in order to provide a well-tested reference architecture.&lt;/p&gt;

&lt;p&gt;SIG Meeting Notes: &lt;a href=&quot;https://bit.ly/2LyTh14&quot;&gt;https://bit.ly/2LyTh14&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Slack channel: https://kubeflow.slack.com/archives/C01C9NPD15H&lt;/p&gt;

&lt;h3 id=&quot;platform-aws&quot;&gt;Platform: AWS&lt;/h3&gt;

&lt;p&gt;Description: Better reliability, better testing coverage by enabling E2E tests for Kubeflow AWS deployment, &lt;a href=&quot;https://www.kubeflow.org/docs/aws/notebook-server/&quot;&gt;better Kubeflow notebook user experience&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;platform-ibm&quot;&gt;Platform: IBM&lt;/h3&gt;

&lt;p&gt;Description: Pipelines and Security have been the key focus for Kubeflow on IBM Cloud for this release. On the Pipelines side, &lt;a href=&quot;https://www.kubeflow.org/docs/ibm/pipelines/&quot;&gt;Kubeflow Pipelines with Tekton is available for deployment on IBM Cloud Kubernetes Service&lt;/a&gt; and is included by default with Kubeflow deployment on IBM Cloud. On the security side, &lt;a href=&quot;https://www.kubeflow.org/docs/ibm/deploy/install-kubeflow/#multi-user-auth-enabled&quot;&gt;we have enabled integration with IBM Cloud AppId&lt;/a&gt; as an authentication provider instead of Dex. When using AppID, it delegates the identity provider to IBM Cloud with builtin identity providers (Cloud Directory, SAML, social log-in with Google or Facebook etc.) or custom providers. Additionally for securing the &lt;a href=&quot;https://www.kubeflow.org/docs/ibm/deploy/authentication/&quot;&gt;Kubeflow authentication with HTTPS we have provided integration instructions&lt;/a&gt; using the &lt;a href=&quot;https://cloud.ibm.com/docs/containers?topic=containers-loadbalancer-about&quot;&gt;IBM Cloud Network Load Balancer&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;platform-gcp&quot;&gt;Platform: GCP&lt;/h3&gt;

&lt;p&gt;Description: Better UX and reliability for installation and upgrade. Upgrade Cloud Config Connector in management cluster to latest.&lt;/p&gt;

&lt;h3 id=&quot;platform-azure&quot;&gt;Platform: Azure&lt;/h3&gt;

&lt;p&gt;Description: We added &lt;a href=&quot;https://www.kubeflow.org/docs/azure/authentication-oidc/&quot;&gt;instructions for deploying Kubeflow with multi-tenancy backed by Azure Active Directory&lt;/a&gt;. Additionally, we documented the &lt;a href=&quot;https://www.kubeflow.org/docs/azure/azuremysql/&quot;&gt;steps to replace the Metadata store with a managed Azure MySQL datatabase instance&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;platform-openshift&quot;&gt;Platform: OpenShift&lt;/h3&gt;

&lt;p&gt;Description: Our focus for this release was to create the OpenShift stack that can install Kubeflow components on OpenShift 4.x . We architected the stack so users can pick and choose components they would like to install by adding or removing kustomizeConfig entries in the kfdef. Components currently supported are istio, single user pipeline, Jupyter notebooks with a custom Tensorflow notebook image, profile controller with custom image, Katib, pytorch and Tensorflow job operators and Seldon. You can install Kubeflow 1.2 on Openshift from the &lt;a href=&quot;https://opendatahub.io/&quot;&gt;Open Data Hub&lt;/a&gt; community operator in OpenShift Catalog using the OpenShift &lt;a href=&quot;https://github.com/kubeflow/manifests/blob/master/kfdef/kfctl_openshift.v1.2.0.yaml&quot;&gt;kfdef&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;platform-microk8s&quot;&gt;Platform: MicroK8s&lt;/h3&gt;

&lt;p&gt;Description: Kubeflow is a built-in add-on to MicroK8s, and now includes Istio v1.5 as default.&lt;/p&gt;

&lt;h3 id=&quot;platform-minikf&quot;&gt;Platform: MiniKF&lt;/h3&gt;

&lt;p&gt;Description: MiniKF is currently testing with Kubeflow 1.2 and will provide an updated MiniKF version based after validation testing and documentation has completed.    Please find more information on MiniKF here: &lt;a href=&quot;https://www.arrikto.com/get-started/&quot;&gt;https://www.arrikto.com/get-started/&lt;/a&gt; . You can also find tutorials that will guide you through end-to-end data science examples here: &lt;a href=&quot;https://www.arrikto.com/tutorials&quot;&gt;https://www.arrikto.com/tutorials&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><category term="release" /><category term="official" /><summary type="html">Special Message from Kubeflow Founders</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.kubeflow.org/images/logo.png" /><media:content medium="image" url="https://blog.kubeflow.org/images/logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Record metadata on Kubeflow from Notebooks</title><link href="https://blog.kubeflow.org/jupyter/2020/10/01/lineage.html" rel="alternate" type="text/html" title="Record metadata on Kubeflow from Notebooks" /><published>2020-10-01T00:00:00-05:00</published><updated>2020-10-01T00:00:00-05:00</updated><id>https://blog.kubeflow.org/jupyter/2020/10/01/lineage</id><content type="html" xml:base="https://blog.kubeflow.org/jupyter/2020/10/01/lineage.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-10-01-lineage.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Lineage-Tracking&quot;&gt;Lineage Tracking&lt;a class=&quot;anchor-link&quot; href=&quot;#Lineage-Tracking&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;This blog post will first guide you through the metadata SDK API, to create a notebook and log several actions to the metadata DB. Afterwards, you will be able to navigate to the Kubeflow UI and the resulting lineage graph, which gives you a graphical representation of the dependencies between the objects you logged using the SDK.&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Install-the-Kubeflow-metadata-library&quot;&gt;Install the &lt;em&gt;Kubeflow-metadata&lt;/em&gt; library&lt;a class=&quot;anchor-link&quot; href=&quot;#Install-the-Kubeflow-metadata-library&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;pip install kubeflow-metadata --user
&lt;span class=&quot;c1&quot;&gt;# Install other packages:&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;pip install pandas --user
&lt;span class=&quot;c1&quot;&gt;# Then restart the Notebook kernel.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;kubeflow.metadata&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metadata&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;datetime&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;uuid&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;uuid4&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered celltag_parameters&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;METADATA_STORE_HOST&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;metadata-grpc-service.kubeflow&amp;quot;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# default DNS of Kubeflow Metadata gRPC serivce.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;METADATA_STORE_PORT&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8080&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Create-a-new-Workspace-and-Run-in-a-workspace&quot;&gt;Create a new Workspace and Run in a workspace&lt;a class=&quot;anchor-link&quot; href=&quot;#Create-a-new-Workspace-and-Run-in-a-workspace&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;A &lt;a href=&quot;https://github.com/kubeflow/metadata/blob/25b44da29213968a2c438d24aad3656cc86d0499/sdk/python/kubeflow/metadata/metadata.py#L92&quot;&gt;Workspace&lt;/a&gt; groups a set of pipelines or notebooks runs, and their related artifacts and executions&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/kubeflow/metadata/blob/25b44da29213968a2c438d24aad3656cc86d0499/sdk/python/kubeflow/metadata/metadata.py#L59&quot;&gt;Store&lt;/a&gt; is an object that provides a connection to the Metadata gRPC service&lt;/li&gt;
&lt;li&gt;The &lt;a href=&quot;https://github.com/kubeflow/metadata/blob/25b44da29213968a2c438d24aad3656cc86d0499/sdk/python/kubeflow/metadata/metadata.py#L227&quot;&gt;Run&lt;/a&gt; object captures a pipeline or notebook run in a workspace&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ws1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Workspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Connect to metadata service in namespace kubeflow in k8s cluster.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;store&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Store&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grpc_host&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;METADATA_STORE_HOST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grpc_port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;METADATA_STORE_PORT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;xgboost-synthetic&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;description&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;workspace for xgboost-synthetic artifacts and executions&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;n1&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;v1&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;workspace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ws1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;xgboost-synthetic-faring-run&amp;quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utcnow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isoformat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;T&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;description&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;a notebook run&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Create-an-execution-in-a-run&quot;&gt;Create an execution in a run&lt;a class=&quot;anchor-link&quot; href=&quot;#Create-an-execution-in-a-run&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;An &lt;a href=&quot;https://github.com/kubeflow/metadata/blob/25b44da29213968a2c438d24aad3656cc86d0499/sdk/python/kubeflow/metadata/metadata.py#L251&quot;&gt;Execution&lt;/a&gt; is a specific instance of a run, and you can bind specific input/output artifacts to this instance. Execution also serves as object for logging artifacts as its input or output&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Execution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;execution&amp;quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utcnow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isoformat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;T&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;workspace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ws1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;description&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;execution for training xgboost-synthetic&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;An execution was created with id &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%s&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;exec&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;An execution was created with id 290
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Log-a-data-set-and-a-model&quot;&gt;Log a data set and a model&lt;a class=&quot;anchor-link&quot; href=&quot;#Log-a-data-set-and-a-model&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;A &lt;a href=&quot;https://github.com/kubeflow/metadata/blob/25b44da29213968a2c438d24aad3656cc86d0499/sdk/python/kubeflow/metadata/metadata.py#L319&quot;&gt;Log_input&lt;/a&gt; log an artifact as an input of this execution. Here exec.log_input accept an artifact class as an argument, a &lt;a href=&quot;https://github.com/kubeflow/metadata/blob/25b44da29213968a2c438d24aad3656cc86d0499/sdk/python/kubeflow/metadata/metadata.py#L412&quot;&gt;DataSet&lt;/a&gt; is an artifact. Every artifacts has different paramenters such as name, uri, query. The way to create DataSet artifact is calling ready-to-use APIs metadata.DataSet and provide arguments&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;date_set_version&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;data_set_version_&amp;quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uuid4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;data_set&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;exec&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataSet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;description&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;xgboost synthetic data&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;synthetic-data&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;owner&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;someone@kubeflow.org&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;uri&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;file://path/to/dataset&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;v1.0.0&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;SELECT * FROM mytable&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Data set id is &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{0.id}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; with version &amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{0.version}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;#39;&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Data set id is 171 with version &amp;#39;data_set_version_cbebc757-0d76-4e1e-bbd9-02b065e4c3ea&amp;#39;
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;a href=&quot;https://github.com/kubeflow/metadata/blob/25b44da29213968a2c438d24aad3656cc86d0499/sdk/python/kubeflow/metadata/metadata.py#L337&quot;&gt;Log_output&lt;/a&gt; log an artifact as a output of this execution. Here exec.log_output accept an artifact class as an argument, a &lt;a href=&quot;https://github.com/kubeflow/metadata/blob/25b44da29213968a2c438d24aad3656cc86d0499/sdk/python/kubeflow/metadata/metadata.py#L518&quot;&gt;Model&lt;/a&gt; is an artifact. Every artifacts has different paramenters such as name, uri, hyperparameters. The way to create Model artifact is calling ready-to-use APIs metadata.Model and provide arguments&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_version&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;model_version_&amp;quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uuid4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;exec&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;MNIST&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;description&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;model to recognize handwritten digits&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;owner&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;someone@kubeflow.org&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;uri&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;gcs://my-bucket/mnist&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;model_type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;neural network&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;training_framework&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;s2&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;tensorflow&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;s2&quot;&gt;&amp;quot;version&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;v1.0&amp;quot;&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;hyperparameters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;s2&quot;&gt;&amp;quot;learning_rate&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;s2&quot;&gt;&amp;quot;layers&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                &lt;span class=&quot;s2&quot;&gt;&amp;quot;early_stop&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_version&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;mylabel&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;l1&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}))&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;Model id is &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{0.id}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; and version is &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{0.version}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;kubeflow.metadata.metadata.Model(workspace=None, name=&amp;#39;MNIST&amp;#39;, description=&amp;#39;model to recognize handwritten digits&amp;#39;, owner=&amp;#39;someone@kubeflow.org&amp;#39;, uri=&amp;#39;gcs://my-bucket/mnist&amp;#39;, version=&amp;#39;model_version_50b419e2-af69-4c0e-a251-78246d4c0578&amp;#39;, model_type=&amp;#39;neural network&amp;#39;, training_framework={&amp;#39;name&amp;#39;: &amp;#39;tensorflow&amp;#39;, &amp;#39;version&amp;#39;: &amp;#39;v1.0&amp;#39;}, hyperparameters={&amp;#39;learning_rate&amp;#39;: 0.5, &amp;#39;layers&amp;#39;: [10, 3, 1], &amp;#39;early_stop&amp;#39;: True}, labels={&amp;#39;mylabel&amp;#39;: &amp;#39;l1&amp;#39;}, id=172, create_time=&amp;#39;2019-12-04T00:44:49.444411Z&amp;#39;, kwargs={})

Model id is 172 and version is model_version_50b419e2-af69-4c0e-a251-78246d4c0578
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Log-the-evaluation-of-a-model&quot;&gt;Log the evaluation of a model&lt;a class=&quot;anchor-link&quot; href=&quot;#Log-the-evaluation-of-a-model&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/kubeflow/metadata/blob/25b44da29213968a2c438d24aad3656cc86d0499/sdk/python/kubeflow/metadata/metadata.py#L639&quot;&gt;Metrics&lt;/a&gt; captures an evaluation metrics of a model on a data set&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;exec&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Metrics&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;MNIST-evaluation&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;description&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;validating the MNIST model to recognize handwritten digits&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;owner&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;someone@kubeflow.org&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;uri&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;gcs://my-bucket/mnist-eval.csv&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;data_set_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_set&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;model_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;metrics_type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VALIDATION&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;accuracy&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.95&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;mylabel&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;l1&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}))&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Metrics id is &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%s&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Metrics id is 173
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Add-Metadata-for-serving-the-model&quot;&gt;Add Metadata for serving the model&lt;a class=&quot;anchor-link&quot; href=&quot;#Add-Metadata-for-serving-the-model&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;serving_application&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Execution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;serving model&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;workspace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ws1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;description&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;an execution to represent model serving component&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Noticed we use model name, version, uri to uniquely identify existing model.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;served_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;MNIST&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;uri&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;gcs://my-bucket/mnist&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;serving_application&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;served_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Found the mode with id &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{0.id}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; and version &amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{0.version}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;#39;.&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Found the mode with id 172 and version &amp;#39;model_version_50b419e2-af69-4c0e-a251-78246d4c0578&amp;#39;.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Plot-the-lineage-graph&quot;&gt;Plot the lineage graph&lt;a class=&quot;anchor-link&quot; href=&quot;#Plot-the-lineage-graph&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/images-lineage/lineage.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The figure above shows an example of the lineage graph from our xgboost example. Follow below steps for you to try out:&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Follow the guide to &lt;a href=&quot;https://www.kubeflow.org/docs/notebooks/setup/&quot;&gt;setting up your Jupyter notebooks in Kubeflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Go back to your Jupyter notebook server in the Kubeflow UI. (If you’ve moved away from the notebooks section in Kubeflow, click Notebook Servers in the left-hand navigation panel to get back there.)&lt;/li&gt;
&lt;li&gt;In the Jupyter notebook UI, click Upload and follow the prompts to upload the &lt;a href=&quot;https://github.com/kubeflow/examples/blob/master/xgboost_synthetic/build-train-deploy.ipynb&quot;&gt;xgboost example&lt;/a&gt; notebook.&lt;/li&gt;
&lt;li&gt;Click the notebook name (build-train-deploy.ipynb.ipynb) to open the notebook in your Kubeflow cluster.&lt;/li&gt;
&lt;li&gt;Run the steps in the notebook to install and use the Metadata SDK.&lt;/li&gt;
&lt;li&gt;Click Artifact Store in the left-hand navigation panel on the Kubeflow UI.&lt;/li&gt;
&lt;li&gt;Select Pipelines -&amp;gt; Artifacts&lt;/li&gt;
&lt;li&gt;Navigate to xgboost-synthetic-traing-eval&lt;/li&gt;
&lt;li&gt;Click on Lineage explorer&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><category term="jupyter" /><summary type="html"></summary></entry><entry><title type="html">Data Science Meets Devops: MLOps with Jupyter, Git, &amp;amp; Kubernetes</title><link href="https://blog.kubeflow.org/mlops/" rel="alternate" type="text/html" title="Data Science Meets Devops: MLOps with Jupyter, Git, &amp;amp; Kubernetes" /><published>2020-08-01T00:00:00-05:00</published><updated>2020-08-01T00:00:00-05:00</updated><id>https://blog.kubeflow.org/data-science-meets-devops</id><content type="html" xml:base="https://blog.kubeflow.org/mlops/">&lt;h1 id=&quot;the-problem&quot;&gt;The Problem&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://www.kubeflow.org/&quot;&gt;Kubeflow&lt;/a&gt; is a fast-growing open source project that makes it easy to deploy and manage machine learning on Kubernetes.&lt;/p&gt;

&lt;p&gt;Due to Kubeflow’s explosive popularity, we receive a large influx of GitHub issues that must be triaged and routed to the appropriate subject matter expert.  The below chart illustrates the number of new issues opened for the past year:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-08-01-data-science-meets-devops/fig1.num-issues.png&quot; width=&quot;&quot; alt=&quot;Number of Kubeflow Issues&quot; title=&quot;&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Number of Kubeflow Issues&lt;/figcaption&gt;

&lt;p&gt;To keep up with this influx, we started investing in a Github App called &lt;a href=&quot;https://github.com/marketplace/issue-label-bot&quot;&gt;Issue Label Bot&lt;/a&gt; that used machine learning to auto label issues.  Our &lt;a href=&quot;https://github.com/marketplace/issue-label-bot&quot;&gt;first model&lt;/a&gt; was trained using a collection of popular public repositories on GitHub and only predicted generic labels.  Subsequently, we started using &lt;a href=&quot;https://cloud.google.com/automl/docs&quot;&gt;Google AutoML&lt;/a&gt; to train a Kubeflow specific model. The new model was able to predict Kubeflow specific labels with average precision of 72% and average recall of 50%. This significantly reduced the toil associated with issue management for Kubeflow maintainers. The table below contains evaluation metrics for Kubeflow specific labels on a holdout set.  The &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;&gt;precision and recall&lt;/a&gt; below coincide with prediction thresholds that we calibrated to suit our needs.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Label&lt;/th&gt;
      &lt;th&gt;Precision&lt;/th&gt;
      &lt;th&gt;Recall&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;area-backend&lt;/td&gt;
      &lt;td&gt;0.6&lt;/td&gt;
      &lt;td&gt;0.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;area-bootstrap&lt;/td&gt;
      &lt;td&gt;0.3&lt;/td&gt;
      &lt;td&gt;0.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;area-centraldashboard&lt;/td&gt;
      &lt;td&gt;0.6&lt;/td&gt;
      &lt;td&gt;0.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;area-components&lt;/td&gt;
      &lt;td&gt;0.5&lt;/td&gt;
      &lt;td&gt;0.3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;area-docs&lt;/td&gt;
      &lt;td&gt;0.8&lt;/td&gt;
      &lt;td&gt;0.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;area-engprod&lt;/td&gt;
      &lt;td&gt;0.8&lt;/td&gt;
      &lt;td&gt;0.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;area-front-end&lt;/td&gt;
      &lt;td&gt;0.7&lt;/td&gt;
      &lt;td&gt;0.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;area-frontend&lt;/td&gt;
      &lt;td&gt;0.7&lt;/td&gt;
      &lt;td&gt;0.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;area-inference&lt;/td&gt;
      &lt;td&gt;0.9&lt;/td&gt;
      &lt;td&gt;0.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;area-jupyter&lt;/td&gt;
      &lt;td&gt;0.9&lt;/td&gt;
      &lt;td&gt;0.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;area-katib&lt;/td&gt;
      &lt;td&gt;0.8&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;area-kfctl&lt;/td&gt;
      &lt;td&gt;0.8&lt;/td&gt;
      &lt;td&gt;0.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;area-kustomize&lt;/td&gt;
      &lt;td&gt;0.3&lt;/td&gt;
      &lt;td&gt;0.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;area-operator&lt;/td&gt;
      &lt;td&gt;0.8&lt;/td&gt;
      &lt;td&gt;0.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;area-pipelines&lt;/td&gt;
      &lt;td&gt;0.7&lt;/td&gt;
      &lt;td&gt;0.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;area-samples&lt;/td&gt;
      &lt;td&gt;0.5&lt;/td&gt;
      &lt;td&gt;0.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;area-sdk&lt;/td&gt;
      &lt;td&gt;0.7&lt;/td&gt;
      &lt;td&gt;0.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;area-sdk-dsl&lt;/td&gt;
      &lt;td&gt;0.6&lt;/td&gt;
      &lt;td&gt;0.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;area-sdk-dsl-compiler&lt;/td&gt;
      &lt;td&gt;0.6&lt;/td&gt;
      &lt;td&gt;0.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;area-testing&lt;/td&gt;
      &lt;td&gt;0.7&lt;/td&gt;
      &lt;td&gt;0.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;area-tfjob&lt;/td&gt;
      &lt;td&gt;0.4&lt;/td&gt;
      &lt;td&gt;0.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;platform-aws&lt;/td&gt;
      &lt;td&gt;0.8&lt;/td&gt;
      &lt;td&gt;0.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;platform-gcp&lt;/td&gt;
      &lt;td&gt;0.8&lt;/td&gt;
      &lt;td&gt;0.6&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;figcaption&gt;&lt;strong&gt;Table 1:&lt;/strong&gt; Evaluation metrics for various Kubeflow labels.&lt;/figcaption&gt;

&lt;p&gt;Given the rate at which new issues are arriving, retraining our model periodically became a priority. We believe continuously retraining and deploying our model to leverage this new data is critical to maintaining the efficacy of our models.&lt;/p&gt;

&lt;h1 id=&quot;our-solution&quot;&gt;Our Solution&lt;/h1&gt;

&lt;p&gt;Our CI/CD solution is illustrated in &lt;a href=&quot;#fig2&quot;&gt;Figure 2&lt;/a&gt;. We don’t explicitly create a directed acyclic graph (DAG)  to connect the steps in an ML workflow (e.g. preprocessing, training, validation, deployment, etc…). Rather, we use a set of independent controllers. Each controller declaratively describes the desired state of the world and takes  actions necessary to make the actual state of the world match. This independence makes it easy for us to use whatever tools make the most sense for each step. More specifically we use&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Jupyter notebooks for developing models.&lt;/li&gt;
  &lt;li&gt;GitOps for continuous integration and deployment.&lt;/li&gt;
  &lt;li&gt;Kubernetes and managed cloud services for underlying infrastructure.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img id=&quot;fig2&quot; src=&quot;/images/2020-08-01-data-science-meets-devops/fig2.ci-cd.png&quot; width=&quot;&quot; alt=&quot;alt_text&quot; title=&quot;&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;&lt;strong&gt;Figure 2:&lt;/strong&gt; illustrates how we do CI/CD. Our pipeline today consists of two independently operating controllers. We configure the Trainer (left hand side) by describing what models we want to exist; i.e. what it means for our models to be “fresh”.  The Trainer periodically checks whether the set of trained models are sufficiently fresh and if not trains a new model. We likewise configure the Deployer (right hand side) to define what it means for the deployed model to be in sync with the set of trained models. If the correct model is not deployed it will deploy a new model.&lt;/figcaption&gt;

&lt;p&gt;For more details on model training and deployment refer to the &lt;a href=&quot;#actuation&quot;&gt;Actuation section below&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;h2 id=&quot;building-resilient-systems-with-reconcilers&quot;&gt;Building Resilient Systems With Reconcilers&lt;/h2&gt;

&lt;p&gt;A reconciler is a control pattern that has proven to be immensely useful for building resilient systems. The reconcile pattern is &lt;a href=&quot;https://book.kubebuilder.io/cronjob-tutorial/controller-overview.html&quot;&gt;at the heart of how Kubernetes works&lt;/a&gt;. Figure 3 illustrates how a reconciler works. A reconciler works by first observing the state of the world; e.g. what model is currently deployed. The reconciler then compares this against the desired state of the world and computes the diff; e.g the model with label “version=20200724” should be deployed, but the model currently deployed has label “version=20200700”. The reconciler then takes the action necessary to drive the world to the desired state; e.g. open a pull request to change the deployed model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-08-01-data-science-meets-devops/fig3.reconciler.png&quot; width=&quot;&quot; alt=&quot;Figure 3&quot; title=&quot;&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;&lt;strong&gt;Figure 3.&lt;/strong&gt; Illustration of the reconciler pattern as applied by our deployer.&lt;/figcaption&gt;

&lt;p&gt;Reconcilers have proven immensely useful for building resilient systems because a well implemented reconciler provides a high degree of confidence that no matter how a system is perturbed it will eventually return to the desired state.&lt;/p&gt;

&lt;h2 id=&quot;there-is-no-dag&quot;&gt;There is no DAG&lt;/h2&gt;

&lt;p&gt;The declarative nature of controllers means data can flow through a series of controllers  without needing to explicitly create a DAG. In lieu of a DAG, a series of data processing steps can instead be expressed as a set of desired states, as illustrated in Figure 4 below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-08-01-data-science-meets-devops/fig4.data-pipeline.png&quot; width=&quot;&quot; alt=&quot;alt_text&quot; title=&quot;&quot; /&gt;&lt;/p&gt;

&lt;figcaption&gt;&lt;strong&gt;Figure 4:&lt;/strong&gt; illustrates how pipelines can emerge from independent controllers without explicitly encoding a DAG. Here we have two completely independent controllers. The first controller ensures that for every element a&lt;sub&gt;i&lt;/sub&gt; there should be an element b&lt;sub&gt;i&lt;/sub&gt;. The second controller ensures that for every element b&lt;sub&gt;i&lt;/sub&gt; there should be an element c&lt;sub&gt;i&lt;/sub&gt;.&lt;/figcaption&gt;

&lt;p&gt;This reconciler-based paradigm offers the following benefits over many traditional DAG-based workflows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Resilience against failures&lt;/strong&gt;:  the system continuously seeks to achieve and maintain the desired state.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Increased autonomy of engineering teams:&lt;/strong&gt; each team is free to choose the tools and infrastructure that suit their needs.  The reconciler framework only requires a minimal amount of coupling between controllers while still allowing one to write expressive workflows.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Battle tested patterns and tools&lt;/strong&gt;:  This reconciler based framework does not invent something new.  Kubernetes has a rich ecosystem of tools that aim to make it easy to build controllers. The popularity of Kubernetes means there is a large and growing community familiar with this pattern and supporting tools.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;gitops-operation-by-pull-request&quot;&gt;GitOps: Operation By Pull Request&lt;/h2&gt;

&lt;p&gt;GitOps, Figure 5, is a pattern for managing infrastructure. The core idea of GitOps is that source control (doesn’t have to be git) should be the source of truth for configuration files  describing your infrastructure. Controllers can then monitor source control and automatically update your infrastructure as your config changes. This means to make a change (or undo a change) you just open a pull request.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-08-01-data-science-meets-devops/fig5.gitops.png&quot; width=&quot;&quot; alt=&quot;alt_text&quot; title=&quot;&quot; /&gt;&lt;/p&gt;

&lt;figcaption&gt;&lt;strong&gt;Figure 5:&lt;/strong&gt; To push a new model for Label Bot we create a PR updating the config map storing the id of the Auto ML model we want to use. When the PR is merged, &lt;a href=&quot;https://cloud.google.com/anthos-config-management/docs&quot;&gt;Anthos Config Management(ACM&lt;/a&gt;) automatically rolls out those changes to our GKE cluster. As a result, subsequent predictions are made using the new model. (Image courtesy of &lt;a href=&quot;https://www.weave.works/blog/automate-kubernetes-with-gitops&quot;&gt;Weaveworks&lt;/a&gt;)&lt;/figcaption&gt;

&lt;h1 id=&quot;putting-it-together-reconciler--gitops--cicd-for-ml&quot;&gt;Putting It Together: Reconciler + GitOps = CI/CD for ML&lt;/h1&gt;

&lt;p&gt;With that background out of the way, let’s dive into how we built CI/CD for ML by combining the Reconciler and GitOps patterns.&lt;/p&gt;

&lt;p&gt;There were three problems we needed to solve:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;How do we compute the diff between the desired and actual state of the world?&lt;/li&gt;
  &lt;li&gt;How do we affect the changes needed to make the actual state match the desired state?&lt;/li&gt;
  &lt;li&gt;How do we build a control loop to continuously run 1 &amp;amp; 2?&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;computing-diffs&quot;&gt;Computing Diffs&lt;/h2&gt;

&lt;p&gt;To compute the diffs we just write lambdas that do exactly what we want. So in this case we wrote two lambdas:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The &lt;a href=&quot;https://github.com/kubeflow/code-intelligence/blob/faeb65757214ac93259f417b81e9e2fedafaebda/Label_Microservice/go/cmd/automl/pkg/server/server.go#L109&quot;&gt;first lambda&lt;/a&gt; determines whether we need to retrain based on the age of the most recent model.&lt;/li&gt;
  &lt;li&gt;The &lt;a href=&quot;https://github.com/kubeflow/code-intelligence/blob/faeb65757214ac93259f417b81e9e2fedafaebda/Label_Microservice/go/cmd/automl/pkg/server/server.go#L49&quot;&gt;second lambda&lt;/a&gt; determines whether the model needs to be updated by comparing the most recently trained model to the model listed in a config map checked into source control.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We wrap these lambdas in a simple web server and deploy on Kubernetes. One reason we chose this approach is because we wanted to rely on Kubernetes’ &lt;a href=&quot;https://github.com/kubernetes/git-sync&quot;&gt;git-sync&lt;/a&gt; to mirror our repository to a pod volume. This makes our lambdas super simple because all the git management is taken care of by a side-car running &lt;a href=&quot;https://github.com/kubernetes/git-sync&quot;&gt;git-sync&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;actuation&quot;&gt;Actuation&lt;/h2&gt;

&lt;p&gt;To apply the changes necessary, we use Tekton to glue together various CLIs that we use to perform the various steps.&lt;/p&gt;

&lt;h3 id=&quot;model-training&quot;&gt;Model Training&lt;/h3&gt;

&lt;p&gt;To train our model we have a &lt;a href=&quot;https://github.com/kubeflow/code-intelligence/blob/faeb65757214ac93259f417b81e9e2fedafaebda/tekton/tasks/run-notebook-task.yaml#L34&quot;&gt;Tekton task &lt;/a&gt; that:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Runs our notebook using &lt;a href=&quot;https://github.com/nteract/papermill&quot;&gt;papermill&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Converts the notebook to html using &lt;a href=&quot;https://nbconvert.readthedocs.io/en/latest/&quot;&gt;nbconvert&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Uploads the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.ipynb&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.html&lt;/code&gt; files to GCS using &lt;a href=&quot;https://cloud.google.com/storage/docs/gsutil&quot;&gt;gsutil&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This notebook fetches GitHub Issues data &lt;a href=&quot;https://medium.com/google-cloud/analyzing-github-issues-and-comments-with-bigquery-c41410d3308&quot;&gt;from BigQuery&lt;/a&gt; and generates CSV files on GCS suitable for import into &lt;a href=&quot;https://cloud.google.com/automl&quot;&gt;Google AutoML&lt;/a&gt;. The notebook then launches an &lt;a href=&quot;https://cloud.google.com/automl&quot;&gt;AutoML&lt;/a&gt; job to train a model.&lt;/p&gt;

&lt;p&gt;We chose AutoML because we wanted to focus on building a complete end to end solution rather than iterating on the model. AutoML provides a competitive baseline that we may try to improve upon in the future.&lt;/p&gt;

&lt;p&gt;To easily view the executed notebook we convert it to html and upload it to &lt;a href=&quot;https://cloud.google.com/storage/docs/hosting-static-website&quot;&gt;GCS which makes it easy to serve public, static content&lt;/a&gt;. This allows us to use notebooks to generate rich visualizations to evaluate our model.&lt;/p&gt;

&lt;h3 id=&quot;model-deployment&quot;&gt;Model Deployment&lt;/h3&gt;

&lt;p&gt;To deploy our model we have a &lt;a href=&quot;https://github.com/kubeflow/code-intelligence/blob/faeb65757214ac93259f417b81e9e2fedafaebda/tekton/tasks/update-model-pr-task.yaml#L68&quot;&gt;Tekton task&lt;/a&gt; that:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Uses kpt to update our configmap with the desired value.&lt;/li&gt;
  &lt;li&gt;Runs git to push our changes to a branch.&lt;/li&gt;
  &lt;li&gt;Uses a wrapper around the &lt;a href=&quot;https://github.com/cli/cli&quot;&gt;GitHub CLI&lt;/a&gt; (gh) to create a PR.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The controller ensures there is only one Tekton pipeline running at a time. We configure our pipelines to always push to the same branch. This ensures we only ever open one PR to update the model because GitHub doesn’t allow multiple PRs to be created from the same branch.&lt;/p&gt;

&lt;p&gt;Once the PR is merged &lt;a href=&quot;https://cloud.google.com/anthos/config-management&quot;&gt;Anthos Config Mesh&lt;/a&gt; automatically applies the Kubernetes manifests to our Kubernetes cluster.&lt;/p&gt;

&lt;h3 id=&quot;why-tekton&quot;&gt;Why Tekton&lt;/h3&gt;

&lt;p&gt;We picked Tekton because the primary challenge we faced was sequentially running a series of CLIs in various containers. Tekton is perfect for this. Importantly, all the steps in a Tekton task run on the same pod which allows data to be shared between steps using a pod volume.&lt;/p&gt;

&lt;p&gt;Furthermore, since Tekton resources are Kubernetes resources we can adopt the same GitOps pattern and tooling to 
update our pipeline definitions.&lt;/p&gt;

&lt;h2 id=&quot;the-control-loop&quot;&gt;The Control Loop&lt;/h2&gt;

&lt;p&gt;Finally, we needed to build a control loop that would periodically invoke our lambdas and launch our Tekton pipelines as needed. We used kubebuilder to create a &lt;a href=&quot;https://github.com/kubeflow/code-intelligence/tree/master/Label_Microservice/go&quot;&gt;simple custom controller&lt;/a&gt;. Our controller’s reconcile loop will call our lambda to determines whether a sync is needed and if so with what parameters. If a sync is needed the controller fires off a Tekton pipeline to perform the actual update. An example of our &lt;a href=&quot;https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/&quot;&gt;custom resource&lt;/a&gt; is illustrated below:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;automl.cloudai.kubeflow.org/v1alpha1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ModelSync&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;modelsync-sample&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;label-bot-prod&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;failedPipelineRunsHistoryLimit&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;needsSyncUrl&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;http://labelbot-diff.label-bot-prod/needsSync&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;needsSyncName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;pipelineName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;automl-model&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;pipelineRunTemplate&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;automl-model&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;notavlidmodel&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;branchName&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;auto-update&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;fork&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;git@github.com:kubeflow/code-intelligence.git&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;forkName&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;fork&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;pipelineRef&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;update-model-pr&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;repo&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;resourceSpec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;url&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;https://github.com/kubeflow/code-intelligence.git&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;revision&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;master&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;git&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;serviceAccountName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;auto-update&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;successfulPipelineRunsHistoryLimit&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The custom resource specifies the endpoint, &lt;strong&gt;needsSyncUrl&lt;/strong&gt;, for the lambda that computes whether a sync is needed and a Tekton PipelineRun, &lt;strong&gt;pipelineRunTemplate&lt;/strong&gt;, describing the pipeline run to create when a sync is needed. The controller takes care of the details; e.g. ensuring only 1 pipeline per resource is running at a time, garbage collecting old runs, etc… All of the heavy lifting is taken care of for us by Kubernetes and kubebuilder.&lt;/p&gt;

&lt;p&gt;Note, for historical reasons the kind, &lt;strong&gt;ModelSync&lt;/strong&gt;, and apiVersion &lt;strong&gt;automl.cloudai.kubeflow.org&lt;/strong&gt; are not reflective of what the controller actually does. We plan on fixing this in the future.&lt;/p&gt;

&lt;h1 id=&quot;build-your-own-cicd-pipelines&quot;&gt;Build Your Own CI/CD pipelines&lt;/h1&gt;

&lt;p&gt;Our code base is a long way from being polished, easily reusable tooling. Nonetheless it is all public  and could be a useful starting point for trying to build your own pipelines.&lt;/p&gt;

&lt;p&gt;Here are some pointers to get you started:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Use the Dockerfile to build your own &lt;a href=&quot;https://github.com/kubeflow/code-intelligence/blob/master/Label_Microservice/go/Dockerfile&quot;&gt;ModelSync controller&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/kubeflow/code-intelligence/tree/master/Label_Microservice/go/config/default&quot;&gt;Modify the kustomize package&lt;/a&gt; to use your image and deploy the controller&lt;/li&gt;
  &lt;li&gt;Define one or more lambdas as needed for your use cases
    &lt;ul&gt;
      &lt;li&gt;You can use our &lt;a href=&quot;https://github.com/kubeflow/code-intelligence/blob/master/Label_Microservice/go/cmd/automl/pkg/server/server.go&quot;&gt;Lambda server&lt;/a&gt; as an example&lt;/li&gt;
      &lt;li&gt;We wrote ours in go but you can use any language and web framework you like (e.g. flask)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Define Tekton pipelines suitable for your use cases; our pipelines(linked below) might be a useful starting point
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/kubeflow/code-intelligence/blob/master/tekton/tasks/run-notebook-task.yaml&quot;&gt;Notebook Tekton task &lt;/a&gt; - Run notebook with papermill and upload to GCS&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/kubeflow/code-intelligence/blob/master/tekton/tasks/update-model-pr-task.yaml&quot;&gt;PR Tekton Task&lt;/a&gt; - Tekton task to open GitHub PRs&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Define ModelSync resources for your use case; you can refer to ours as an example
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/kubeflow/code-intelligence/blob/master/Label_Microservice/auto-update/prod/modelsync.yaml&quot;&gt;ModelSync Deploy Spec&lt;/a&gt; - YAML to continuously deploy label bot&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/kubeflow/code-intelligence/blob/master/Label_Microservice/auto-update/prod/retrain-model.yaml&quot;&gt;ModelSync Train Spec&lt;/a&gt; - YAML to continuously train our model&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you’d like to see us clean it up and include it in a future Kubeflow release please chime in on issue &lt;a href=&quot;https://github.com/kubeflow/kubeflow/issues/5167&quot;&gt;kubeflow/kubeflow#5167&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;whats-next&quot;&gt;What’s Next&lt;/h1&gt;

&lt;h2 id=&quot;lineage-tracking&quot;&gt;Lineage Tracking&lt;/h2&gt;

&lt;p&gt;Since we do not have an explicit DAG representing the sequence of steps in our CI/CD pipeline understanding the lineage of our models can be challenging. Fortunately, Kubeflow Metadata solves this by making it easy for each step to record information about what outputs it produced using what code and inputs. Kubeflow metadata can easily recover and plot the lineage graph. The figure below shows an example of the lineage graph from our &lt;a href=&quot;https://github.com/kubeflow/examples/blob/master/xgboost_synthetic/build-train-deploy.ipynb&quot;&gt;xgboost example&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-08-01-data-science-meets-devops/fig6.lineage.png&quot; alt=&quot;alt_text&quot; title=&quot;&quot; /&gt;&lt;/p&gt;

&lt;figcaption&gt;&lt;strong&gt;Figure 6:&lt;/strong&gt; screenshot of the lineage tracking UI for our &lt;a href=&quot;https://github.com/kubeflow/examples/blob/master/xgboost_synthetic/build-train-deploy.ipynb&quot;&gt;xgboost example&lt;/a&gt;.&lt;/figcaption&gt;

&lt;p&gt;Our plan is to have our controller automatically write lineage tracking information to the metadata server so we can easily understand the lineage of what’s in production.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-08-01-data-science-meets-devops/meme.png&quot; width=&quot;&quot; alt=&quot;alt_text&quot; title=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Building ML products is a team effort. In order to move a model from a proof of concept to a shipped product, data scientists and devops engineers need to collaborate. To foster this collaboration, we believe it is important to allow data scientists and devops engineers to use their preferred tools.    Concretely, we wanted to support the following tools for Data Scientists, Devops Engineers, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Site_Reliability_Engineering&quot;&gt;SRE&lt;/a&gt;s:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Jupyter notebooks for developing models.&lt;/li&gt;
  &lt;li&gt;GitOps for continuous integration and deployment.&lt;/li&gt;
  &lt;li&gt;Kubernetes and managed cloud services for underlying infrastructure.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To maximize each team’s autonomy and reduce dependencies on tools, our  CI/CD process follows a decentralized approach. Rather than explicitly define a DAG that connects the steps, our approach relies on a series of controllers that can be defined and administered independently. We think this maps naturally to enterprises where responsibilities might be split across teams; a data engineering team might be responsible for turning weblogs into features, a modeling team might be responsible for producing models from the features, and a deployments team might be responsible for rolling those models into production.&lt;/p&gt;

&lt;h1 id=&quot;further-reading&quot;&gt;Further Reading&lt;/h1&gt;

&lt;p&gt;If you’d like to learn more about GitOps we suggest this &lt;a href=&quot;https://www.weave.works/technologies/gitops/&quot;&gt;guide&lt;/a&gt; from Weaveworks.&lt;/p&gt;

&lt;p&gt;To learn how to build your own Kubernetes controllers the &lt;a href=&quot;https://book.kubebuilder.io/&quot;&gt;kubebuilder book&lt;/a&gt; walks through an E2E example.&lt;/p&gt;</content><author><name>&lt;a href='https://www.linkedin.com/in/jeremy-lewi-600aaa8/'&gt;Jeremy Lewi&lt;/a&gt;, &lt;a href='https://hamel.dev/'&gt;Hamel Husain&lt;/a&gt;</name></author><category term="jupyter" /><category term="mlops" /><category term="tekton" /><category term="gitops" /><summary type="html">The Problem</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.kubeflow.org/images/2020-08-01-data-science-meets-devops/meme.png" /><media:content medium="image" url="https://blog.kubeflow.org/images/2020-08-01-data-science-meets-devops/meme.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>