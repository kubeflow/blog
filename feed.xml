<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://blog.kubeflow.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://blog.kubeflow.org/" rel="alternate" type="text/html" /><updated>2026-02-26T20:27:15-06:00</updated><id>https://blog.kubeflow.org/feed.xml</id><title type="html">Kubeflow</title><subtitle>The Machine Learning Toolkit for Kubernetes.</subtitle><author><name>{&quot;name&quot;=&gt;&quot;&quot;, &quot;email&quot;=&gt;&quot;&quot;}</name><email></email></author><entry><title type="html">Introducing the Metaflow-Kubeflow Integration</title><link href="https://blog.kubeflow.org/metaflow/" rel="alternate" type="text/html" title="Introducing the Metaflow-Kubeflow Integration" /><published>2026-02-04T00:00:00-06:00</published><updated>2026-02-04T00:00:00-06:00</updated><id>https://blog.kubeflow.org/introducing-metaflow-kubeflow-integration</id><content type="html" xml:base="https://blog.kubeflow.org/metaflow/"><![CDATA[<h1 id="a-tale-of-two-flows-metaflow-and-kubeflow">A tale of two flows: Metaflow and Kubeflow</h1>

<p>Metaflow is a Python framework for building and operating ML and AI projects, originally developed and open-sourced by Netflix in 2019. In many ways, Kubeflow and Metaflow are cousins: closely related in spirit, but designed with distinct goals and priorities.</p>

<p><a href="https://docs.metaflow.org/">Metaflow</a> emerged from Netflix‚Äôs need to empower data scientists and ML/AI developers with developer-friendly, Python-native tooling, so that they could easily iterate quickly on ideas, compare modeling approaches, and ship the best solutions to production without heavy engineering or DevOps involvement. On the infrastructure side, Metaflow started with AWS-native services like AWS Batch and Step Functions, later expanding to provide first-class support for the Kubernetes ecosystem and other hyperscaler clouds.</p>

<p>In contrast, Kubeflow began as a set of Kubernetes operators for distributed TensorFlow and Jupyter Notebook management. Over time, it has evolved into a comprehensive Cloud Native AI ecosystem, offering a broad set of tools out of the box. These include Trainer, Katib, Spark Operator for orchestrating distributed AI workloads, Workspaces for interactive development environments, Hub for AI catalog and artifacts management, KServe for model serving, and Pipelines to deploy end-to-end ML workflows and stitching Kubeflow components together.</p>

<p>Over the years, Metaflow has delighted end users with its intuitive APIs, while Kubeflow has delivered tons of value to infrastructure teams through its robust platform components. This complementary nature of the tools motivated us to build a bridge between the two: <a href="https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-kubeflow">you can now author projects in Metaflow and deploy them as Kubeflow Pipelines</a>, side by side with your existing Kubeflow workloads.</p>

<h1 id="why-metaflow--kubeflow">Why Metaflow ‚Üí Kubeflow</h1>

<p>In <a href="https://www.cncf.io/wp-content/uploads/2025/11/cncf_report_techradar_111025a.pdf">the most recent CNCF Technology Radar survey</a> from October 2025, Metaflow got the highest positive scores in the ‚Äú<em>likelihood to recommend</em>‚Äù and ‚Äú<em>usefulness</em>‚Äù categories, reflecting its success in providing a set of stable, productivity-boosting APIs for ML/AI developers.</p>

<p>Metaflow spans the entire development lifecycle‚Äîfrom early experimentation to production deployment and ongoing operations. To give you an idea, the core features below illustrate the breadth of its API surface, grouped by project stage:</p>

<h2 id="development">Development</h2>

<ul>
  <li>
    <p>Straightforward APIs for <a href="https://docs.metaflow.org/metaflow/basics">creating and composing workflows</a>.</p>
  </li>
  <li>
    <p>Automated state transfer and management through <a href="https://docs.metaflow.org/metaflow/basics#artifacts">artifacts</a>, allowing you to <a href="https://docs.metaflow.org/metaflow/authoring-flows/introduction">build flows incrementally</a> and resume them freely (see <a href="https://netflixtechblog.com/supercharging-the-ml-and-ai-development-experience-at-netflix-b2d5b95c63eb">a recent article by Netflix</a> about the topic)</p>
  </li>
  <li>
    <p>Interactive, <a href="https://docs.metaflow.org/metaflow/basics">real-time visual outputs</a> from tasks through cards - a perfect substrate for <a href="https://outerbounds.com/blog/visualize-everything-with-ai">custom observability solutions, created quickly with AI copilots</a>.</p>
  </li>
  <li>
    <p>Choose the right balance between code and configuration through <a href="https://docs.metaflow.org/metaflow/configuring-flows/introduction">built-in configuration management</a>.</p>
  </li>
  <li>
    <p>Create domain-specific abstractions and project-level policies through <a href="https://docs.metaflow.org/metaflow/composing-flows/introduction">custom decorators</a>.</p>
  </li>
</ul>

<h2 id="scaling">Scaling</h2>

<ul>
  <li>
    <p><a href="https://docs.metaflow.org/scaling/remote-tasks/introduction">Scale flows horizontally and vertically</a>: Both task and data parallelism are supported.</p>
  </li>
  <li>
    <p><a href="https://docs.metaflow.org/scaling/failures">Handle failures gracefully</a>.</p>
  </li>
  <li>
    <p><a href="https://docs.metaflow.org/scaling/dependencies">Package dependencies automatically</a> with support for Conda, PyPI, and uv.</p>
  </li>
  <li>
    <p>Leverage <a href="https://docs.metaflow.org/scaling/remote-tasks/distributed-computing">distributed computing paradigms</a> such as Ray, MPI, and Torch Distributed.</p>
  </li>
  <li>
    <p><a href="https://docs.metaflow.org/scaling/checkpoint/introduction">Checkpoint long-running tasks</a> and manage checkpoints consistently.</p>
  </li>
</ul>

<h2 id="deployment">Deployment</h2>

<ul>
  <li>
    <p>Maintain a clear separation between experimentation, production, and individual developers through <a href="https://docs.metaflow.org/scaling/tagging">namespaces</a>.</p>
  </li>
  <li>
    <p>Adopt CI/CD and GitOps best practices through <a href="https://docs.metaflow.org/production/coordinating-larger-metaflow-projects">branching</a>.</p>
  </li>
  <li>
    <p><a href="https://docs.metaflow.org/production/event-triggering">Compose large, reactive systems</a> through isolated sub-flows with event triggering.</p>
  </li>
</ul>

<p>These features provide a unified, user-facing API for the capabilities required by real-world ML and AI systems. Behind the scenes, Metaflow is built on integrations with production-quality infrastructure, effectively acting as a user-interface layer over platforms like Kubernetes - and now, Kubeflow. The diagram below illustrates the division of responsibilities:
<img style="max-width: 100%; height: auto; display: block;" alt="kubeflow-metaflow-arch" src="https://github.com/user-attachments/assets/88f4af4e-7e27-4287-b275-88e4b1b87449" /></p>

<p>The key benefit of the Metaflow‚ÄìKubeflow integration is that it allows organizations to <strong>keep their existing Kubernetes and Kubeflow infrastructure intact, while upgrading the developer experience with higher-level abstractions and additional functionality, provided by Metaflow.</strong></p>

<p>Currently, the integration supports deploying Metaflow flows as Kubeflow Pipelines. Once you have Metaflow tasks running on Kubernetes, you can access other components such as Katib and Trainer from Metaflow tasks through their Python clients as usual.</p>

<h1 id="metaflow--kubeflow-in-practice">Metaflow ‚Üí Kubeflow in practice</h1>

<p>As the integration requires no changes in your existing Kubeflow infrastructure, it is straightforward to get started. You can <a href="https://docs.metaflow.org/getting-started/infrastructure">deploy Metaflow in an existing cloud account</a> (GCP, Azure, or AWS) or you can <a href="https://docs.metaflow.org/getting-started/devstack">install the dev stack on your laptop</a> with a single command.</p>

<p>Once you have Metaflow and Kubeflow running independently, you can install the extension providing the integration (you can <a href="https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-kubeflow">follow instructions in the documentation</a>):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install metaflow-kubeflow
</code></pre></div></div>

<p>The only configuration needed is to point Metaflow at your Kubeflow Pipelines service, either by adding the following line in the Metaflow config or by setting it as an environment variable:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>METAFLOW_KUBEFLOW_PIPELINES_URL = "http://my-kubeflow"
</code></pre></div></div>

<p>After this, you can author a Metaflow flow as usual and test it locally:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python flow.py run
</code></pre></div></div>

<p>which runs the flow quickly as local processes. If everything looks good, you can deploy the flow as a Kubeflow pipeline:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python flow.py kubeflow-pipelines create
</code></pre></div></div>

<p>This will package all the source code and dependencies of the flow automatically, compile the Metaflow flow into a Kubeflow Pipelines YAML and deploy it to Kubeflow, which you can see alongside your existing pipelines in the Kubeflow UI. The following screencast shows the process in action:</p>

<p><a href="https://www.youtube.com/watch?v=ALg0A9SzRG8"><img src="https://i.ytimg.com/vi/ALg0A9SzRG8/maxresdefault.jpg" alt="" /></a></p>

<p>The integration doesn‚Äôt have 100% feature coverage yet: Some Metaflow features such as <a href="https://docs.metaflow.org/metaflow/basics#conditionals">conditional</a> and <a href="https://docs.metaflow.org/metaflow/basics#recursion">recursive</a> steps are not yet supported. In future versions, we may also provide additional convenience APIs for other Kubeflow components, such as KServe - or you can easily implement them by yourself as <a href="https://docs.metaflow.org/metaflow/composing-flows/custom-decorators">custom decorators</a> with the <a href="https://sdk.kubeflow.org/en/latest/">Kubeflow SDK</a>!</p>

<p>If you want to learn more about the integration, you can watch <a href="https://www.youtube.com/watch?v=YDKRIiQNMU0">an announcement webinar</a> on Youtube.</p>

<h1 id="feedback-welcome">Feedback welcome!</h1>

<p>Like Kubeflow, Metaflow is an open-source project actively developed by multiple organizations ‚Äî including Netflix, which maintains a dedicated team working on Metaflow, and <a href="https://outerbounds.com">Outerbounds, which provides a managed Metaflow platform</a> deployed in customers‚Äô own cloud environments.</p>

<p>The Metaflow community convenes at <a href="http://slack.outerbounds.co">the Metaflow Slack</a>. We welcome you to join, ask questions, and give feedback about the Kubeflow integration, and share your wishlist items for the roadmap. We are looking forward to a fruitful collaboration between the two communities!</p>]]></content><author><name>{&quot;name&quot;=&gt;&quot;&quot;, &quot;email&quot;=&gt;&quot;&quot;}</name></author><category term="community" /><summary type="html"><![CDATA[A tale of two flows: Metaflow and Kubeflow]]></summary></entry><entry><title type="html">Kubeflow AI Reference Platform 1.11 Release Announcement</title><link href="https://blog.kubeflow.org/kubeflow-1.11-release/" rel="alternate" type="text/html" title="Kubeflow AI Reference Platform 1.11 Release Announcement" /><published>2025-12-22T00:00:00-06:00</published><updated>2025-12-22T00:00:00-06:00</updated><id>https://blog.kubeflow.org/kubeflow-1.11-release</id><content type="html" xml:base="https://blog.kubeflow.org/kubeflow-1.11-release/"><![CDATA[<p>Kubeflow AI Reference Platform 1.11 delivers substantial platform improvements focused on scalability, security, and operational efficiency. The release reduces per namespace overhead, strengthens multi-tenant defaults, and improves overall reliability for running Kubeflow at scale on Kubernetes.</p>

<h2 id="highlight-features">Highlight features</h2>

<ul>
  <li>Trainer v2.1.0 with unified TrainJob API, Python-first workflows, and built-in LLM fine-tuning support</li>
  <li>Multi-tenant S3 storage with per-namespace credentials, with SeaweedFS replacing MinIO as the default backend</li>
  <li>Massive scalability improvements enabling Kubeflow deployments to scale to 1,000+ users, profiles, and namespaces</li>
  <li>Zero pod overhead by default for namespaces and profiles, significantly reducing baseline resource consumption</li>
  <li>Optimized Istio service mesh configuration to dramatically reduce sidecar memory usage and network traffic in large clusters</li>
  <li>Stronger security defaults with Pod Security Standards (restricted for system namespaces, baseline for user namespaces)</li>
  <li>Improved authentication and exposure patterns for KServe inference services, with automated tests and documentation</li>
  <li>Expanded Helm chart support (experimental) to improve modularity and deployment flexibility</li>
  <li>Updates across core components, including Kubeflow Pipelines, Katib, KServe, Model Registry, Istio, and Spark Operator</li>
</ul>

<h2 id="kubeflow-platform-manifests--security">Kubeflow Platform (Manifests &amp; Security)</h2>

<p>The Kubeflow Platform Working Group focuses on simplifying Kubeflow installation, operations, and security. See details below.</p>

<h3 id="manifests">Manifests:</h3>

<ul>
  <li><a href="https://github.com/kubeflow/manifests/blob/master/README.md">Documentation updates</a> that make it easier to install,
extend and upgrade Kubeflow</li>
  <li>For more details and future plans please check <a href="https://github.com/kubeflow/manifests/issues/3038">1.12.0</a> roadmap.</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Notebooks</th>
      <th style="text-align: center">Dashboard</th>
      <th style="text-align: center">Pipelines</th>
      <th style="text-align: center">Katib</th>
      <th style="text-align: center">Trainer</th>
      <th style="text-align: center">KServe</th>
      <th style="text-align: center">Model Registry</th>
      <th style="text-align: center">Spark</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="https://github.com/kubeflow/kubeflow/issues/7459">1.10</a></td>
      <td style="text-align: center"><a href="https://github.com/kubeflow/kubeflow/releases/tag/v1.10.0">1.10</a></td>
      <td style="text-align: center"><a href="https://github.com/kubeflow/pipelines/releases/tag/2.15.2">2.15.2</a></td>
      <td style="text-align: center"><a href="https://github.com/kubeflow/katib/releases/tag/v0.19.0">0.19.0</a></td>
      <td style="text-align: center"><a href="https://github.com/kubeflow/trainer/releases/tag/v2.1.0">2.1.0</a></td>
      <td style="text-align: center"><a href="https://github.com/kserve/kserve/releases/tag/v0.15.2">0.15.2</a></td>
      <td style="text-align: center"><a href="https://github.com/kubeflow/model-registry/releases/tag/v0.3.4">0.3.4</a></td>
      <td style="text-align: center"><a href="https://github.com/kubeflow/spark-operator/releases/tag/v2.4.0">2.4.0</a></td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Kubernetes</th>
      <th style="text-align: center">Kind</th>
      <th style="text-align: center">Kustomize</th>
      <th style="text-align: center">Cert Manager</th>
      <th style="text-align: center">Knative</th>
      <th style="text-align: center">Istio</th>
      <th style="text-align: center">Dex</th>
      <th style="text-align: center">OAuth2-proxy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">1.33+</td>
      <td style="text-align: center">0.30.0</td>
      <td style="text-align: center">5.7.1</td>
      <td style="text-align: center">1.16.1</td>
      <td style="text-align: center">1.20</td>
      <td style="text-align: center">1.28</td>
      <td style="text-align: center">2.43</td>
      <td style="text-align: center">7.10</td>
    </tr>
  </tbody>
</table>

<h3 id="security">Security:</h3>

<ul>
  <li><strong>Pod Security Standards enforced by default</strong>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">restricted</code> for all Kubeflow system namespaces<br />
(<a href="https://github.com/kubeflow/manifests/pull/3190">#3190</a>, <a href="https://github.com/kubeflow/manifests/pull/3050">#3050</a>)</li>
      <li><code class="language-plaintext highlighter-rouge">baseline</code> for user namespaces<br />
(<a href="https://github.com/kubeflow/manifests/pull/3204">#3204</a>, <a href="https://github.com/kubeflow/manifests/pull/3220">#3220</a>)</li>
    </ul>
  </li>
  <li><strong>Network policies enabled by default</strong> for critical system namespaces<br />
(<code class="language-plaintext highlighter-rouge">knative-serving</code>, <code class="language-plaintext highlighter-rouge">oauth2-proxy</code>, <code class="language-plaintext highlighter-rouge">cert-manager</code>, <code class="language-plaintext highlighter-rouge">istio-system</code>, <code class="language-plaintext highlighter-rouge">auth</code>)<br />
(<a href="https://github.com/kubeflow/manifests/pull/3228">#3228</a>)</li>
  <li><strong>Improved multi-tenant isolation for object storage</strong>, with per-namespace S3 credentials<br />
(<a href="https://github.com/kubeflow/manifests/pull/3240">#3240</a>)</li>
  <li><strong>Authentication enforcement for KServe inference services</strong><br />
(<a href="https://github.com/kubeflow/manifests/pull/3180">#3180</a>)</li>
</ul>

<p>Trivy CVE scans December 15 2025:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Working Group</th>
      <th style="text-align: center">Images</th>
      <th style="text-align: center">Critical CVE</th>
      <th style="text-align: center">High CVE</th>
      <th style="text-align: center">Medium CVE</th>
      <th style="text-align: center">Low CVE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Katib</td>
      <td style="text-align: center">18</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">35</td>
      <td style="text-align: center">158</td>
      <td style="text-align: center">562</td>
    </tr>
    <tr>
      <td style="text-align: center">Pipelines</td>
      <td style="text-align: center">15</td>
      <td style="text-align: center">12</td>
      <td style="text-align: center">432</td>
      <td style="text-align: center">1051</td>
      <td style="text-align: center">1558</td>
    </tr>
    <tr>
      <td style="text-align: center">Workbenches(Notebooks)</td>
      <td style="text-align: center">12</td>
      <td style="text-align: center">39</td>
      <td style="text-align: center">312</td>
      <td style="text-align: center">525</td>
      <td style="text-align: center">267</td>
    </tr>
    <tr>
      <td style="text-align: center">Kserve</td>
      <td style="text-align: center">16</td>
      <td style="text-align: center">35</td>
      <td style="text-align: center">535</td>
      <td style="text-align: center">11929</td>
      <td style="text-align: center">1745</td>
    </tr>
    <tr>
      <td style="text-align: center">Manifests</td>
      <td style="text-align: center">15</td>
      <td style="text-align: center">6</td>
      <td style="text-align: center">105</td>
      <td style="text-align: center">256</td>
      <td style="text-align: center">55</td>
    </tr>
    <tr>
      <td style="text-align: center">Trainer</td>
      <td style="text-align: center">9</td>
      <td style="text-align: center">4</td>
      <td style="text-align: center">157</td>
      <td style="text-align: center">9012</td>
      <td style="text-align: center">728</td>
    </tr>
    <tr>
      <td style="text-align: center">Model Registry</td>
      <td style="text-align: center">3</td>
      <td style="text-align: center">3</td>
      <td style="text-align: center">75</td>
      <td style="text-align: center">132</td>
      <td style="text-align: center">36</td>
    </tr>
    <tr>
      <td style="text-align: center">Spark</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">4</td>
      <td style="text-align: center">22</td>
      <td style="text-align: center">1688</td>
      <td style="text-align: center">151</td>
    </tr>
    <tr>
      <td style="text-align: center">All Images</td>
      <td style="text-align: center">89</td>
      <td style="text-align: center">104</td>
      <td style="text-align: center">1673</td>
      <td style="text-align: center">24751</td>
      <td style="text-align: center">5102</td>
    </tr>
  </tbody>
</table>

<h2 id="pipelines">Pipelines</h2>

<p>This release of KFP introduces several notable changes that users should consider prior to upgrading. Comprehensive upgrade and documentation notes will follow shortly. In the interim, please note the following key modifications</p>

<h3 id="default-object-store-update">Default object store update</h3>

<p>Kubeflow Pipelines now defaults to SeaweedFS for the object store deployment, replacing the previous default of MinIO.
MinIO remains fully supported, as does any S3-compatible object storage backend, only the default deployment configuration has changed.</p>

<p>Existing MinIO manifests are still available for users who wish to continue using MinIO, though these legacy manifests may be removed in future releases. Users with existing data are advised to back up and restore as needed when switching object store backends.</p>

<h3 id="database-backend-upgrade">Database backend upgrade</h3>

<p>This release includes a major upgrade to the Gorm database backend, which introduces an automated database index migration for users upgrading from versions prior to 2.15.0.
Because this migration does not support rollback, it is strongly recommended that production databases be backed up before performing the upgrade.</p>

<h2 id="model-registry">Model Registry</h2>

<p>Model Registry continues to mature with new capabilities for model discovery, governance, and deeper integration with the Kubeflow ecosystem.</p>

<h3 id="model-registry-ui">Model Registry UI</h3>

<p>The user-friendly web interface for centralized model metadata, version tracking, and artifact management now supports filtering, sorting, archiving, custom metadata, and metadata editing making it easier for teams to organize and govern their model lifecycle.</p>

<h3 id="model-catalog">Model Catalog</h3>

<p>A new Model Catalog feature enables model discovery and sharing with governance controls.
A <a href="https://github.com/kubeflow/community/blob/master/proposals/907-model-registry-renaming/README.md#model-catalog-cluster-scoped-company-scoped">Model Catalog</a> is a pattern where an organisation can define their validated and approved models, enabling discovery and sharing across teams, while at the same time ensuring model governance and compliance.
Admin can define a number of catalog sources, filtering and enable model visibility, including Hugging Face.
Teams can discover and use approved models from the organisation‚Äôs catalog.
The catalog UI and backend are under active development.</p>

<h3 id="kserve-integration">KServe Integration</h3>

<ul>
  <li><strong>Custom Storage Initializer (CSI)</strong>  Enables model download and deployment using model metadata directly from the Registry.</li>
  <li><strong>Reconciliation loop</strong>  A deployable Kubernetes controller which observes KServe InferenceServices to automatically populate Model Registry logical-model records, keeping registry audit records of live deployments.</li>
</ul>

<h3 id="storage-integrations">Storage Integrations</h3>

<ul>
  <li><strong>Python client workflows</strong>  Data scientists can leverage convenience functions in the Python client to <a href="https://model-registry.readthedocs.io/en/latest/#uploading-local-models-to-external-storage-and-registering-them">package, store, and register models and their metadata</a> in a single playbook.</li>
  <li><strong>Async Upload Job</strong>  A Kubernetes Job for transferring and packaging models (including KServe ModelCar OCI Image format), simplifying model storage operations in production environments, leveraging scaling and orchestration capabilities of Kubernetes without additional dependencies.</li>
</ul>

<h3 id="additional-improvements">Additional Improvements</h3>

<ul>
  <li>Removal of the legacy Google MLMD dependency.</li>
  <li>PostgreSQL support alongside MySQL.</li>
  <li>Multi-architecture container builds (amd64/arm64).</li>
  <li>SBOM generation for container builds and OpenSSF Scorecard CI integration.</li>
</ul>

<h2 id="training-operator-trainer--katib">Training Operator (Trainer) &amp; Katib</h2>

<p>Kubeflow 1.11 includes Trainer v2.1.0, a major architectural evolution that simplifies distributed training on Kubernetes with a unified API, Python-first workflows, and enhanced LLM fine-tuning capabilities.</p>

<h3 id="new-api-architecture">New API Architecture</h3>

<p>Kubeflow Trainer v2 introduces <strong>TrainJob</strong>  a unified training job API that replaces framework-specific CRDs (PyTorchJob, TFJob, etc.). Infrastructure configuration is now separated into <strong>TrainingRuntime</strong> and <strong>ClusterTrainingRuntime</strong> resources, creating a clean boundary between platform engineering (runtime setup) and data science (job submission).</p>

<h3 id="python-first-experience">Python-First Experience</h3>

<ul>
  <li><strong>No YAML required</strong>  Install with <code class="language-plaintext highlighter-rouge">pip install kubeflow</code> and submit jobs directly from Python notebooks or scripts.</li>
  <li><strong>Local execution mode</strong>  Develop and test training code locally without a Kubernetes cluster before scaling to production.</li>
  <li><strong>Helm Charts</strong>  Deploy with <code class="language-plaintext highlighter-rouge">helm install kubeflow-trainer oci://ghcr.io/kubeflow/charts/kubeflow-trainer --version 2.1.0</code>.</li>
</ul>

<h3 id="llm-fine-tuning">LLM Fine-Tuning</h3>

<p>Built-in support for large language model fine-tuning workflows:</p>
<ul>
  <li>TorchTune trainer with pre-configured runtimes for Llama 3.2, Qwen 2.5, and more.</li>
  <li>LoRA, QLoRA, and DoRA for parameter-efficient fine-tuning.</li>
  <li>Dataset and model initializers for HuggingFace and S3 storage.</li>
</ul>

<h3 id="distributed-ai-data-cache">Distributed AI Data Cache</h3>

<p>Optional in-memory cache cluster (powered by <a href="https://arrow.apache.org/">Apache Arrow</a> and <a href="https://datafusion.apache.org/">Apache DataFusion</a> ) streams datasets directly to GPU nodes with zero-copy transfers, maximizing GPU utilization and minimizing I/O wait times for large-scale training workloads. More details can be found <a href="https://www.kubeflow.org/docs/components/trainer/user-guides/data-cache/">here</a>.</p>

<h3 id="scheduler-integrations">Scheduler Integrations</h3>

<ul>
  <li><strong>Kueue</strong>  Topology-aware scheduling and multi-cluster job dispatching for TrainJobs, enabling optimal placement for distributed training across node groups.</li>
  <li><strong>Volcano</strong>  Gang-scheduling support with PodGroup integration.</li>
  <li><strong>MPI</strong>  First-class support for MPI-based distributed training workloads on Kubernetes.</li>
</ul>

<h3 id="katib">Katib</h3>

<p>Katib hyperparameter tuning remains compatible with Trainer v2, allowing users to optimize model hyperparameters alongside the new training workflow.</p>

<p>A major addition is the integration with Kubeflow SDK (<a href="https://github.com/kubeflow/sdk/tree/main/docs/proposals/46-hyperparameter-optimization">KEP-46</a>, <a href="https://github.com/kubeflow/sdk/pull/124">PR #124</a>). The new <code class="language-plaintext highlighter-rouge">OptimizerClient</code> allows users to define and run hyperparameter experiments directly from Python notebooks without writing YAML. You can configure search spaces, objectives, and algorithms using <code class="language-plaintext highlighter-rouge">OptimizerClient().optimize()</code>. Each trial runs as a TrainJob with different hyperparameter values, and training code can report metrics using simple Python functions. The client includes standard methods for managing jobs: <code class="language-plaintext highlighter-rouge">create_job()</code>, <code class="language-plaintext highlighter-rouge">get_job()</code>, <code class="language-plaintext highlighter-rouge">list_jobs()</code>, and <code class="language-plaintext highlighter-rouge">delete_job()</code>.</p>

<h2 id="spark-operator">Spark Operator</h2>

<p>The Spark Operator has received broad improvements in Kubeflow 1.11, spanning Spark version support, workload management, scheduling, and operational simplicity.</p>

<h3 id="broader-spark-support">Broader Spark Support</h3>

<p>The operator now supports Apache Spark 4 and introduces Spark Connect, enabling modern client‚Äìserver Spark interactions. This allows users to connect to Spark sessions remotely and improves compatibility with the evolving Spark ecosystem.</p>

<h3 id="workload-management--scheduling">Workload Management &amp; Scheduling</h3>

<ul>
  <li><strong>Suspend / Resume SparkApplications</strong>  Users can now suspend and resume jobs, giving greater control over workload lifecycle.</li>
  <li><strong>Kueue integration</strong>  Integration with <a href="https://kueue.sigs.k8s.io/">Kueue</a> enables queue-based workload management and fair sharing of cluster resources across teams.</li>
  <li><strong>Enhanced dynamic allocation</strong>  Improved shuffle tracking and dynamic allocation controls for more efficient resource usage.</li>
</ul>

<h3 id="operations--security">Operations &amp; Security</h3>

<ul>
  <li><strong>Automatic CRD upgrades</strong>  Helm hooks now handle CRD upgrades automatically, reducing manual steps during upgrades.</li>
  <li><strong>Deprecation of sparkctl</strong>  Legacy <code class="language-plaintext highlighter-rouge">sparkctl</code> has been deprecated in favor of kubectl-native workflows.</li>
  <li><strong>Flexible Ingress &amp; cert-manager support</strong>  More configurable Ingress (TLS, annotations, URL patterns) and simplified certificate handling via cert-manager.</li>
</ul>

<h3 id="observability">Observability</h3>

<ul>
  <li><strong>Structured logging</strong>  Configurable JSON and console log output formats.</li>
  <li><strong>Better validation</strong>  Stricter validation of SparkApplication names and specs, catching misconfigurations earlier.</li>
</ul>

<h2 id="kserve">KServe</h2>

<p>KServe in Kubeflow 1.11 delivers major improvements across model serving, inference capabilities, and operational maturity.</p>

<h3 id="multi-node-inference">Multi-Node Inference</h3>

<p>KServe now supports multi-node inference, enabling large models to be distributed across multiple nodes using Ray-based serving runtimes. This is critical for deploying very large language models that exceed single-node GPU capacity.</p>

<h3 id="model-cache-improvements">Model Cache Improvements</h3>

<p>The Model Cache feature, introduced in v0.14, has been significantly hardened. Fixes include correct URI matching, protection against cache mismatches, support for multiple node groups, and PVC/PV retention after InferenceService deletion making model caching more reliable for production use.</p>

<h3 id="keda-autoscaling-integration">KEDA Autoscaling Integration</h3>

<p>KServe introduces integration with <a href="https://keda.sh/">KEDA</a> for event-driven autoscaling, including an external scaler implementation. This gives users more flexible scaling options beyond the built-in Knative and HPA-based autoscalers.</p>

<h3 id="gateway-api-support">Gateway API Support</h3>

<p>Raw deployment mode now supports the Kubernetes Gateway API, providing a modern, standardized alternative to Ingress for routing inference traffic.</p>

<h3 id="vllm--hugging-face-runtime-updates">vLLM &amp; Hugging Face Runtime Updates</h3>

<ul>
  <li>Upgraded vLLM to v0.8.1+ with support for reasoning models, tool calling, embeddings, reranking, and Llama 4 / Qwen 3.</li>
  <li>vLLM V1 engine support and CPU inference via Intel Extension for PyTorch.</li>
  <li>LMCache integration with vLLM for improved KV cache reuse.</li>
  <li>Hugging Face runtime updates include 4-bit quantization support (bitsandbytes), speculative decoding, and deprecation of OpenVINO support.</li>
</ul>

<h3 id="inference-graph-enhancements">Inference Graph Enhancements</h3>

<ul>
  <li>InferenceGraphs now support pod spec fields (affinity, tolerations, resources) and well-known labels.</li>
  <li>Improved Istio mesh compatibility and fixed response codes for conditional routing steps.</li>
</ul>

<h3 id="operational--security-improvements">Operational &amp; Security Improvements</h3>

<ul>
  <li>ModelCar (OCI-based model loading) enabled by default.</li>
  <li>Collocation of transformer and predictor containers in a single pod.</li>
  <li>Stop-and-resume model serving via annotations (serverless mode).</li>
  <li>Configurable label and annotation propagation to serving pods.</li>
  <li>SBOM generation and third-party license inclusion for all images.</li>
  <li>Multiple CVE fixes including <code class="language-plaintext highlighter-rouge">CVE-2025-43859</code> and <code class="language-plaintext highlighter-rouge">CVE-2025-24357</code>.</li>
</ul>

<h2 id="kubeflow-sdk">Kubeflow SDK</h2>

<p>Kubeflow 1.11 is the first AI Reference Platform release where users can simply <code class="language-plaintext highlighter-rouge">pip install kubeflow</code> to start working with AI workloads, no Kubernetes expertise required. The <a href="https://sdk.kubeflow.org/en/latest/">Kubeflow SDK</a> provides a unified Python interface to train models, run hyperparameter tuning, and manage model artifacts across the Kubeflow ecosystem. It also enables local development without a Kubernetes cluster, so users can iterate on their training code locally before scaling to production. For documentation and examples, visit <a href="https://sdk.kubeflow.org/en/latest/">sdk.kubeflow.org</a>.</p>

<h2 id="dashboard-and-notebooks">Dashboard and Notebooks</h2>

<p>The Kubeflow Central Dashboard and Notebooks remain at version 1.10 in this release, providing stable and reliable experiences. Stay tuned for interesting updates in upcoming Kubeflow AI Reference Platform releases.</p>

<h2 id="how-to-get-started-with-111">How to get started with 1.11</h2>

<p>Visit the Kubeflow AI Reference Platform 1.11 <a href="https://github.com/kubeflow/manifests/releases">release page</a> or head over to the Getting Started and Support pages.</p>

<h2 id="join-the-community">Join the Community</h2>

<p>We would like to thank everyone who contributed to Kubeflow 1.11, and especially Valentina Rodriguez Sosa for her work as the v1.11 Release Manager. We also extend our thanks to the entire release team and the working group leads, who continuously and generously dedicate their time and expertise to Kubeflow.</p>

<p>Release team members : Valentina Rodriguez Sosa, Anya Kramar, Tarek Abouzeid, Andy Stoneberg, Humair Khan, Matteo Mortari, Adysen Rothman, Jon Burdo, Milos Grubjesic, Vraj Bhatt, Dhanisha Phadate, Alok Dangre</p>

<p>Working Group leads : Andrey Velichkevich, Julius von Kohout,  Mathew Wicks, Matteo Mortari</p>

<p>Kubeflow Steering Committee : Andrey Velichkevich, Julius von Kohout, Yuan Tang, Johnu George, Francisco Javier Araceo</p>

<p>You can find more details about Kubeflow distributions
<a href="https://www.kubeflow.org/docs/started/installing-kubeflow/#packaged-distributions">here</a>.</p>

<h2 id="want-to-help">Want to help?</h2>

<p>The Kubeflow community Working Groups hold open meetings and are always looking for more volunteers and users to unlock
the potential of machine learning. If you‚Äôre interested in becoming a Kubeflow contributor, please feel free to check
out the resources below. We look forward to working with you!</p>

<ul>
  <li>Visit our <a href="https://www.kubeflow.org/docs/about/community/">Kubeflow website</a> or Kubeflow GitHub Page.</li>
  <li>Join the <a href="https://www.kubeflow.org/docs/about/community/">Kubeflow Slack channel</a>.</li>
  <li>Join the <a href="https://groups.google.com/g/kubeflow-discuss">kubeflow-discuss</a> mailing list.</li>
  <li>Attend our weekly <a href="https://www.kubeflow.org/docs/about/community/#kubeflow-community-call">community meeting</a>.</li>
</ul>]]></content><author><name>Kubeflow 1.11 Release Team</name></author><category term="release" /><summary type="html"><![CDATA[Kubeflow AI Reference Platform 1.11 delivers substantial platform improvements focused on scalability, security, and operational efficiency. The release reduces per namespace overhead, strengthens multi-tenant defaults, and improves overall reliability for running Kubeflow at scale on Kubernetes.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.kubeflow.org/images/logo.png" /><media:content medium="image" url="https://blog.kubeflow.org/images/logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Introducing the Kubeflow SDK: A Pythonic API to Run AI Workloads at Scale</title><link href="https://blog.kubeflow.org/sdk/intro/" rel="alternate" type="text/html" title="Introducing the Kubeflow SDK: A Pythonic API to Run AI Workloads at Scale" /><published>2025-11-07T00:00:00-06:00</published><updated>2025-11-07T00:00:00-06:00</updated><id>https://blog.kubeflow.org/sdk/introducing-kubeflow-sdk</id><content type="html" xml:base="https://blog.kubeflow.org/sdk/intro/"><![CDATA[<blockquote>
  <p><strong>‚ö° We want your feedback!</strong> Help shape the future of Kubeflow SDK by taking our <a href="https://docs.google.com/forms/d/e/1FAIpQLSet_IAFQzMMDWolzFt5LI9lhzqOOStjIGHxgYqKBnVcRtDfrw/viewform?usp=dialog">quick survey</a>.</p>
</blockquote>

<h1 id="unified-sdk-concept">Unified SDK Concept</h1>

<p>Scaling AI workloads shouldn‚Äôt require deep expertise in distributed systems and container orchestration. Whether you are prototyping on local hardware or deploying to a production Kubernetes cluster, you need a unified API that abstracts infrastructure complexity while preserving flexibility. That‚Äôs exactly what the Kubeflow Python SDK delivers.</p>

<p>As an AI Practitioner, you‚Äôve probably experienced this frustrating journey: you start by prototyping locally, training your model on your laptop. When you need more compute power, you have to rewrite everything for distributed training. You containerize your code, rebuild images for every small change, write Kubernetes YAMLs, wrestle with kubectl, and juggle multiple SDKs ‚Äî one for training, another for hyperparameter tuning, and yet another for pipelines. Each step demands different tools, APIs, and mental models.</p>

<p>All this complexity slows down productivity, drains focus, and ultimately holds back AI innovation. What if there was a better way?</p>

<p>The Kubeflow community started the <strong>Kubeflow SDK &amp; ML Experience Working Group</strong> (WG) in order to address these challenges. You can find more information about this WG on our <a href="https://youtu.be/VkbVVk2OGUI?list=PLmzRWLV1CK_wSO2IMPnzChxESmaoXNfrY">YouTube playlist</a>.</p>

<h1 id="introducing-kubeflow-sdk">Introducing Kubeflow SDK</h1>

<p>The SDK sits on top of the Kubeflow ecosystem as a unified interface layer. When you write Python code, the SDK translates it into the appropriate Kubernetes resources ‚Äî generating CRs, handling orchestration, and managing distributed communication. You get all the power of Kubeflow and distributed AI compute without needing to understand Kubernetes.</p>

<p><img src="/images/2025-11-07-introducing-kubeflow-sdk/kubeflow-sdk.drawio.svg" alt="kubeflow ecosystem" /></p>

<p>Getting started is simple:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pip</span> <span class="n">install</span> <span class="n">kubeflow</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">kubeflow.trainer</span> <span class="kn">import</span> <span class="n">TrainerClient</span>

<span class="k">def</span> <span class="nf">train_model</span><span class="p">():</span>
    <span class="kn">import</span> <span class="nn">torch</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">())</span>

    <span class="c1"># Training loop
</span>    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="c1"># Your training logic
</span>        <span class="k">pass</span>

    <span class="n">torch</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s">"model.pt"</span><span class="p">)</span>

<span class="c1"># Create a client and train
</span><span class="n">client</span> <span class="o">=</span> <span class="n">TrainerClient</span><span class="p">()</span>
<span class="n">client</span><span class="p">.</span><span class="n">train</span><span class="p">(</span><span class="n">train_func</span><span class="o">=</span><span class="n">train_model</span><span class="p">)</span>
</code></pre></div></div>

<p>The following principles are the foundation that guide the design and implementation of the SDK:</p>

<ul>
  <li><strong>Unified Experience</strong>: Single SDK to interact with multiple Kubeflow projects through consistent Python APIs</li>
  <li><strong>Simplified AI Workloads</strong>: Abstract away Kubernetes complexity and work effortlessly across all Kubeflow projects using familiar Python APIs</li>
  <li><strong>Built for Scale</strong>: Seamlessly scale any AI workload ‚Äî from local laptop to large-scale production cluster with thousands of GPUs using the same APIs.</li>
  <li><strong>Rapid Iteration</strong>: Reduced friction between development and production environments</li>
  <li><strong>Local Development</strong>: First-class support for local development without a Kubernetes cluster requiring only pip installation</li>
</ul>

<h2 id="role-in-the-kubeflow-ecosystem">Role in the Kubeflow Ecosystem</h2>

<p>The SDK doesn‚Äôt replace any Kubeflow projects ‚Äî it provides a unified way to use them. Kubeflow Trainer, Katib, Spark Operator, Pipelines, etc still handle the actual workload execution. The SDK makes them easier to interact with through consistent Python APIs, letting you work entirely in the language you already use for ML development.</p>

<p>This creates a clear separation:</p>
<ul>
  <li><strong>AI Practitioners</strong> use the SDK to submit jobs and manage workflows through Python, without touching YAML or Kubernetes directly</li>
  <li><strong>Platform Administrators</strong> continue managing infrastructure ‚Äî installing components, configuring runtimes, setting resource quotas. Nothing changes on the infrastructure side.</li>
</ul>

<p><img src="/images/2025-11-07-introducing-kubeflow-sdk/user-personas.drawio.svg" alt="kubeflow user personas" /></p>

<p>The Kubeflow SDK works with your existing Kubeflow deployment. If you already have Kubeflow Trainer and Katib installed, just <code class="language-plaintext highlighter-rouge">pip install kubeflow</code> and start using them through the unified interface. As Kubeflow evolves with new components and features, the SDK provides a stable Python layer that adapts alongside the ecosystem.</p>

<table>
  <thead>
    <tr>
      <th>Project</th>
      <th>Status</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Kubeflow Trainer</td>
      <td>Available ‚úÖ</td>
      <td>Train and fine-tune AI models with various frameworks</td>
    </tr>
    <tr>
      <td>Kubeflow Optimizer</td>
      <td>Available ‚úÖ</td>
      <td>Hyperparameter optimization</td>
    </tr>
    <tr>
      <td>Kubeflow Pipelines</td>
      <td>Planned üöß</td>
      <td>Build, run, and track AI workflows</td>
    </tr>
    <tr>
      <td>Kubeflow Model Registry</td>
      <td>Planned üöß</td>
      <td>Manage model artifacts, versions and ML artifacts metadata</td>
    </tr>
    <tr>
      <td>Kubeflow Spark Operator</td>
      <td>Planned üöß</td>
      <td>Manage Spark applications for data processing and feature engineering</td>
    </tr>
  </tbody>
</table>

<h1 id="key-features">Key Features</h1>

<h2 id="unified-python-interface">Unified Python Interface</h2>

<p>The SDK provides a consistent experience across all Kubeflow components. Whether you‚Äôre training models or optimizing hyperparameters, the APIs follow the same patterns:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">kubeflow.trainer</span> <span class="kn">import</span> <span class="n">TrainerClient</span>
<span class="kn">from</span> <span class="nn">kubeflow.optimizer</span> <span class="kn">import</span> <span class="n">OptimizerClient</span>

<span class="c1"># Initialize clients
</span><span class="n">trainer</span> <span class="o">=</span> <span class="n">TrainerClient</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">OptimizerClient</span><span class="p">()</span>

<span class="c1"># List jobs
</span><span class="n">TrainerClient</span><span class="p">().</span><span class="n">list_jobs</span><span class="p">()</span>
<span class="n">OptimizerClient</span><span class="p">().</span><span class="n">list_jobs</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="trainer-client">Trainer Client</h2>

<p>The TrainerClient provides the easiest way to run distributed training on Kubernetes, built on top of <a href="https://blog.kubeflow.org/trainer/intro/">Kubeflow Trainer v2</a>. Whether you‚Äôre training custom models with PyTorch, or fine-tuning LLMs, the client provides a Python API for submitting and monitoring training jobs at scale.</p>

<p>The client works with pre-configured runtimes that Platform Administrators set up. These runtimes define the container images, resource policies, and infrastructure settings. As an AI Practitioner, you reference these runtimes and focus on your training code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">kubeflow.trainer</span> <span class="kn">import</span> <span class="n">TrainerClient</span><span class="p">,</span> <span class="n">CustomTrainer</span>

<span class="k">def</span> <span class="nf">get_torch_dist</span><span class="p">():</span>
    <span class="s">"""Your PyTorch training code runs on each node."""</span>
    <span class="kn">import</span> <span class="nn">os</span>
    <span class="kn">import</span> <span class="nn">torch</span>
    <span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="n">dist</span>

    <span class="n">dist</span><span class="p">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s">"gloo"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"PyTorch Distributed Environment"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"WORLD_SIZE: </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="n">get_world_size</span><span class="p">()</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"RANK: </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="n">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"LOCAL_RANK: </span><span class="si">{</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'LOCAL_RANK'</span><span class="p">]</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="c1"># Create the TrainJob
</span><span class="n">job_id</span> <span class="o">=</span> <span class="n">TrainerClient</span><span class="p">().</span><span class="n">train</span><span class="p">(</span>
    <span class="n">runtime</span><span class="o">=</span><span class="n">TrainerClient</span><span class="p">().</span><span class="n">get_runtime</span><span class="p">(</span><span class="s">"torch-distributed"</span><span class="p">),</span>
    <span class="n">trainer</span><span class="o">=</span><span class="n">CustomTrainer</span><span class="p">(</span>
        <span class="n">func</span><span class="o">=</span><span class="n">get_torch_dist</span><span class="p">,</span>
        <span class="n">num_nodes</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">resources_per_node</span><span class="o">=</span><span class="p">{</span>
            <span class="s">"cpu"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Wait for TrainJob to complete
</span><span class="n">TrainerClient</span><span class="p">().</span><span class="n">wait_for_job_status</span><span class="p">(</span><span class="n">job_id</span><span class="p">)</span>

<span class="c1"># Print TrainJob logs
</span><span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">TrainerClient</span><span class="p">().</span><span class="n">get_job_logs</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">job_id</span><span class="p">)))</span>
</code></pre></div></div>

<p>The TrainerClient supports <code class="language-plaintext highlighter-rouge">CustomTrainer</code> for your own training logic and <a href="https://www.kubeflow.org/docs/components/trainer/user-guides/builtin-trainer/torchtune/"><code class="language-plaintext highlighter-rouge">BuiltinTrainer</code></a> for pre-packaged training patterns like LLM fine-tuning.</p>

<p>Getting started with LLM fine-tuning is as simple as a single line. The default model, dataset, and training configurations are pre-baked into the runtime:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">TrainerClient</span><span class="p">().</span><span class="n">train</span><span class="p">(</span>
    <span class="n">runtime</span><span class="o">=</span><span class="n">TrainerClient</span><span class="p">().</span><span class="n">get_runtime</span><span class="p">(</span><span class="s">"torchtune-qwen2.5-1.5b"</span><span class="p">),</span>
<span class="p">)</span>
</code></pre></div></div>

<p>You can also customize every aspect of the fine-tuning process ‚Äî specify your own dataset, model, LoRA configuration, and training hyperparameters:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">kubeflow.trainer</span> <span class="kn">import</span> <span class="n">TrainerClient</span><span class="p">,</span> <span class="n">BuiltinTrainer</span><span class="p">,</span> <span class="n">TorchTuneConfig</span>
<span class="kn">from</span> <span class="nn">kubeflow.trainer</span> <span class="kn">import</span> <span class="n">Initializer</span><span class="p">,</span> <span class="n">HuggingFaceDatasetInitializer</span><span class="p">,</span> <span class="n">HuggingFaceModelInitializer</span>
<span class="kn">from</span> <span class="nn">kubeflow.trainer</span> <span class="kn">import</span> <span class="n">TorchTuneInstructDataset</span><span class="p">,</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">DataFormat</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">TrainerClient</span><span class="p">()</span>

<span class="n">client</span><span class="p">.</span><span class="n">train</span><span class="p">(</span>
    <span class="n">runtime</span><span class="o">=</span><span class="n">client</span><span class="p">.</span><span class="n">get_runtime</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">"torchtune-llama3.2-1b"</span><span class="p">),</span>
    <span class="n">initializer</span><span class="o">=</span><span class="n">Initializer</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">HuggingFaceDatasetInitializer</span><span class="p">(</span>
            <span class="n">storage_uri</span><span class="o">=</span><span class="s">"hf://tatsu-lab/alpaca/data"</span>
        <span class="p">),</span>
        <span class="n">model</span><span class="o">=</span><span class="n">HuggingFaceModelInitializer</span><span class="p">(</span>
            <span class="n">storage_uri</span><span class="o">=</span><span class="s">"hf://meta-llama/Llama-3.2-1B-Instruct"</span><span class="p">,</span>
            <span class="n">access_token</span><span class="o">=</span><span class="s">"hf_..."</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="p">),</span>
    <span class="n">trainer</span><span class="o">=</span><span class="n">BuiltinTrainer</span><span class="p">(</span>
        <span class="n">config</span><span class="o">=</span><span class="n">TorchTuneConfig</span><span class="p">(</span>
            <span class="n">dataset_preprocess_config</span><span class="o">=</span><span class="n">TorchTuneInstructDataset</span><span class="p">(</span>
                <span class="n">source</span><span class="o">=</span><span class="n">DataFormat</span><span class="p">.</span><span class="n">PARQUET</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">peft_config</span><span class="o">=</span><span class="n">LoraConfig</span><span class="p">(</span>
                <span class="n">apply_lora_to_mlp</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                <span class="n">lora_attn_modules</span><span class="o">=</span><span class="p">[</span><span class="s">"q_proj"</span><span class="p">,</span> <span class="s">"k_proj"</span><span class="p">,</span> <span class="s">"v_proj"</span><span class="p">,</span> <span class="s">"output_proj"</span><span class="p">],</span>
                <span class="n">quantize_base</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">resources_per_node</span><span class="o">=</span><span class="p">{</span>
                <span class="s">"gpu"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="p">)</span>
    <span class="p">)</span>
<span class="p">)</span>
</code></pre></div></div>

<p>You can mix and match ‚Äî use the runtime‚Äôs default model but specify your own dataset, or keep the default dataset but customize the LoRA parameters. The Initializers download datasets and models once to shared storage, then all training pods access the data from there ‚Äî reducing startup time and network usage.</p>

<p>For more details about Kubeflow Trainer capabilities, including gang-scheduling, fault tolerance, and MPI support, check out the <a href="https://blog.kubeflow.org/trainer/intro/">Kubeflow Trainer v2 blog post</a>.</p>

<h2 id="optimizer-client">Optimizer Client</h2>

<p>The OptimizerClient manages hyperparameter optimization for large models of any size on Kubernetes. With consistent APIs across TrainerClient and OptimizerClient, you can easily transition from training to optimization ‚Äî define your training job template once, specify which parameters to optimize, and the client orchestrates multiple trials to find the best hyperparameter configuration. This consistent API design significantly enhances the user experience during AI development.</p>

<p>The client launches trials in parallel according to your resource constraints, tracks metrics across experiments, and identifies optimal parameters.</p>

<p>First, define your training job template:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">kubeflow.trainer</span> <span class="kn">import</span> <span class="n">TrainerClient</span><span class="p">,</span> <span class="n">CustomTrainer</span>
<span class="kn">from</span> <span class="nn">kubeflow.optimizer</span> <span class="kn">import</span> <span class="n">OptimizerClient</span><span class="p">,</span> <span class="n">TrainJobTemplate</span><span class="p">,</span> <span class="n">Search</span><span class="p">,</span> <span class="n">Objective</span><span class="p">,</span> <span class="n">TrialConfig</span>

<span class="k">def</span> <span class="nf">train_func</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="s">"""Training function with hyperparameters."""</span>
    <span class="c1"># Your training code here
</span>    <span class="kn">import</span> <span class="nn">time</span>
    <span class="kn">import</span> <span class="nn">random</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">time</span><span class="p">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Training </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">, lr: </span><span class="si">{</span><span class="n">learning_rate</span><span class="si">}</span><span class="s">, batch_size: </span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"loss=</span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.77</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>


<span class="c1"># Create a reusable template
</span><span class="n">template</span> <span class="o">=</span> <span class="n">TrainJobTemplate</span><span class="p">(</span>
    <span class="n">trainer</span><span class="o">=</span><span class="n">CustomTrainer</span><span class="p">(</span>
        <span class="n">func</span><span class="o">=</span><span class="n">train_func</span><span class="p">,</span>
        <span class="n">func_args</span><span class="o">=</span><span class="p">{</span><span class="s">"learning_rate"</span><span class="p">:</span> <span class="s">"0.01"</span><span class="p">,</span> <span class="s">"batch_size"</span><span class="p">:</span> <span class="s">"16"</span><span class="p">},</span>
        <span class="n">num_nodes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">resources_per_node</span><span class="o">=</span><span class="p">{</span><span class="s">"gpu"</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span>
    <span class="p">),</span>
    <span class="n">runtime</span><span class="o">=</span><span class="n">TrainerClient</span><span class="p">().</span><span class="n">get_runtime</span><span class="p">(</span><span class="s">"torch-distributed"</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Verify that your TrainJob is working with test hyperparameters.
</span><span class="n">TrainerClient</span><span class="p">().</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">template</span><span class="p">)</span>
</code></pre></div></div>

<p>Then optimize hyperparameters with a single call:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span> <span class="o">=</span> <span class="n">OptimizerClient</span><span class="p">()</span>

<span class="n">job_name</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">.</span><span class="n">optimize</span><span class="p">(</span>
    <span class="c1"># The same template can be used for Hyperparameter Optimisation
</span>    <span class="n">trial_template</span><span class="o">=</span><span class="n">template</span><span class="p">,</span>
    <span class="n">search_space</span><span class="o">=</span><span class="p">{</span>
        <span class="s">"learning_rate"</span><span class="p">:</span> <span class="n">Search</span><span class="p">.</span><span class="n">loguniform</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">),</span>
        <span class="s">"batch_size"</span><span class="p">:</span> <span class="n">Search</span><span class="p">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">]),</span>
    <span class="p">},</span>
    <span class="n">trial_config</span><span class="o">=</span><span class="n">TrialConfig</span><span class="p">(</span>
        <span class="n">num_trials</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
        <span class="n">parallel_trials</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">max_failed_trials</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Verify OptimizationJob was created
</span><span class="n">optimizer</span><span class="p">.</span><span class="n">get_job</span><span class="p">(</span><span class="n">job_name</span><span class="p">)</span>

<span class="c1"># Wait for OptimizationJob to complete
</span><span class="n">optimizer</span><span class="p">.</span><span class="n">wait_for_job_status</span><span class="p">(</span><span class="n">job_name</span><span class="p">)</span>

<span class="c1"># Get the best hyperparameters and metrics from an OptimizationJob
</span><span class="n">best_results</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">.</span><span class="n">get_best_results</span><span class="p">(</span><span class="n">job_name</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">best_results</span><span class="p">)</span>
<span class="c1"># Output:
# Result(
#     parameters={'learning_rate': '0.0234', 'batch_size': '64'},
#     metrics=[Metric(name='loss', min='0.78', max='0.78', latest='0.78')]
# )
</span>
<span class="c1"># See all the trials (TrainJobs) created during optimization
</span><span class="n">job</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">.</span><span class="n">get_job</span><span class="p">(</span><span class="n">job_name</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">job</span><span class="p">.</span><span class="n">trials</span><span class="p">)</span>
</code></pre></div></div>

<p>This creates multiple TrainJob instances (trials) with different hyperparameter combinations, executes them in parallel based on available resources, and tracks which parameters produce the best results. Each trial is a full training job managed by Kubeflow Trainer. Using <a href="https://www.kubeflow.org/docs/components/katib/user-guides/katib-ui/">Katib UI</a>, you can visualize your optimization with an interactive graph that shows metric performance against hyperparameter values across all trials.</p>

<p><img src="/images/2025-11-07-introducing-kubeflow-sdk/katib-ui.png" alt="Katib UI example" /></p>

<p>For more details about hyperparameter optimization, check out the <a href="https://github.com/kubeflow/sdk/tree/main/docs/proposals/46-hyperparameter-optimization">OptimizerClient KEP</a>.</p>

<h2 id="local-execution-mode">Local Execution Mode</h2>

<p>Local Execution Mode provides backend flexibility while maintaining full API compatibility with the Kubernetes backend, substantially reducing friction for AI practitioners when developing and iterating.</p>

<p>Choose the right execution environment for your stage of development:</p>

<h3 id="local-process-backend-fastest-iteration">Local Process Backend: Fastest Iteration</h3>

<p>The Local Process Backend is your starting point for ML development - offering the fastest possible iteration cycle with zero infrastructure overhead. This backend executes your training code directly as a Python subprocess on your local machine, bypassing containers, orchestration, and network complexity entirely.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">kubeflow.trainer.backends.localprocess</span> <span class="kn">import</span> <span class="n">LocalProcessBackendConfig</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">LocalProcessBackendConfig</span><span class="p">()</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">TrainerClient</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

<span class="c1"># Runs directly on your machine - no containers, no cluster
</span><span class="n">client</span><span class="p">.</span><span class="n">train</span><span class="p">(</span><span class="n">train_func</span><span class="o">=</span><span class="n">train_model</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="container-backend-production-like-environment">Container Backend: Production-Like Environment</h3>

<p>The Container Backend bridges the gap between local development and production deployment by bringing production parity to your laptop. This backend executes your training code inside containers (using Docker or Podman), ensuring that your development environment matches your production environment byte-for-byte - same dependencies, same Python version, same system libraries, same everything.</p>

<p>Docker Example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">kubeflow.trainer.backends.container</span> <span class="kn">import</span> <span class="n">ContainerBackendConfig</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">ContainerBackendConfig</span><span class="p">(</span>
    <span class="n">container_runtime</span><span class="o">=</span><span class="s">"docker"</span><span class="p">,</span>
    <span class="n">auto_remove</span><span class="o">=</span><span class="bp">True</span>  <span class="c1"># Clean up containers after completion
</span><span class="p">)</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">TrainerClient</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

<span class="c1"># Launch 2-node distributed training locally
</span><span class="n">client</span><span class="p">.</span><span class="n">train</span><span class="p">(</span><span class="n">train_func</span><span class="o">=</span><span class="n">train_model</span><span class="p">,</span> <span class="n">num_nodes</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<p>Podman Example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">kubeflow.trainer.backends.container</span> <span class="kn">import</span> <span class="n">ContainerBackendConfig</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">ContainerBackendConfig</span><span class="p">(</span>
    <span class="n">container_runtime</span><span class="o">=</span><span class="s">"podman"</span><span class="p">,</span>
    <span class="n">auto_remove</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">TrainerClient</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">client</span><span class="p">.</span><span class="n">train</span><span class="p">(</span><span class="n">train_func</span><span class="o">=</span><span class="n">train_model</span><span class="p">,</span> <span class="n">num_nodes</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="kubernetes-backend-production-scale">Kubernetes Backend: Production Scale</h3>

<p>The Kubernetes Backend enables Kubeflow SDK to perform reliably at production scale - enabling you to deploy the exact same training code you developed locally to a production Kubernetes cluster with massive computational resources. This backend transforms your simple <code class="language-plaintext highlighter-rouge">client.train()</code> call into a full-fledged distributed training job managed by Kubeflow‚Äôs Trainer, complete with fault tolerance, resource scheduling, and cluster-wide orchestration.</p>

<p>Kubernetes Example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">kubeflow.trainer.backends.kubernetes</span> <span class="kn">import</span> <span class="n">KubernetesBackendConfig</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">KubernetesBackendConfig</span><span class="p">(</span>
    <span class="n">namespace</span><span class="o">=</span><span class="s">"ml-training"</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">TrainerClient</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

<span class="c1"># Scales to hundreds of nodes - the same code you tested locally
</span><span class="n">client</span><span class="p">.</span><span class="n">train</span><span class="p">(</span>
    <span class="n">train_func</span><span class="o">=</span><span class="n">train_model</span><span class="p">,</span>
    <span class="n">num_nodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">packages_to_install</span><span class="o">=</span><span class="p">[</span><span class="s">"torch"</span><span class="p">,</span> <span class="s">"transformers"</span><span class="p">]</span>
<span class="p">)</span>
</code></pre></div></div>

<h1 id="whats-next">What‚Äôs Next?</h1>

<p>We‚Äôre just getting started. The Kubeflow SDK currently supports Trainer and Optimizer, but the vision is much bigger ‚Äî a unified Python interface for the entire <a href="https://www.kubeflow.org/docs/started/architecture/#kubeflow-projects-in-the-ai-lifecycle">Cloud Native AI Lifecycle</a>.</p>

<p>Here‚Äôs what‚Äôs on the horizon:</p>

<ul>
  <li><a href="https://github.com/kubeflow/sdk/issues/125"><strong>Pipelines Integration</strong></a>: A PipelinesClient to build end-to-end ML workflows. Pipelines will reuse the core Kubeflow SDK primitives for training, optimization, and deployment in a single pipeline. The Kubeflow SDK will also power <a href="https://github.com/kubeflow/pipelines-components">KFP core components</a></li>
  <li><a href="https://github.com/kubeflow/sdk/issues/59"><strong>Model Registry Integration</strong></a>: Seamlessly manage model artifacts and versions across the training and serving lifecycle</li>
  <li><a href="https://github.com/kubeflow/sdk/issues/107"><strong>Spark Operator Integration</strong></a>: Data processing and feature engineering through a SparkClient interface</li>
  <li><a href="https://github.com/kubeflow/sdk/issues/50"><strong>Documentation</strong></a>: Full Kubeflow SDK documentation with guides, examples, and API references</li>
  <li><a href="https://github.com/kubeflow/sdk/issues/153"><strong>Local Execution for Optimizer</strong></a>: Run hyperparameter optimization experiments locally before scaling to Kubernetes</li>
  <li><a href="https://github.com/kubeflow/sdk/issues/48"><strong>Workspace Snapshots</strong></a>: Capture your entire development environment and reproduce it in distributed training jobs</li>
  <li><a href="https://github.com/kubeflow/sdk/issues/23"><strong>Multi-Cluster Support</strong></a>: Manage training jobs across multiple Kubernetes clusters from a single SDK interface</li>
  <li><a href="https://github.com/kubeflow/trainer/issues/2655"><strong>Distributed Data Cache</strong></a>: In-memory caching for large datasets via initializer SDK configuration</li>
  <li><a href="https://github.com/kubeflow/trainer/issues/2752"><strong>Additional Built-in Trainers</strong></a>: Support for more fine-tuning frameworks beyond TorchTune ‚Äî <a href="https://github.com/unslothai/unsloth">Unsloth</a>, <a href="https://github.com/meta-pytorch/torchforge">torchforge</a>, <a href="https://github.com/axolotl-ai-cloud/axolotl">Axolotl</a>, <a href="https://github.com/hiyouga/LLaMA-Factory">LLaMA-Factory</a>, and others</li>
</ul>

<p>The community is driving these features forward. If you have ideas, feedback, or want to contribute, we‚Äôd love to hear from you!</p>

<h1 id="get-involved">Get Involved</h1>

<p>The Kubeflow SDK is built by and for the community. We welcome contributions, feedback, and participation from everyone!</p>

<p><strong>üîî Help Shape the Future of Kubeflow SDK</strong></p>

<p>We want to hear from you! Take our <a href="https://docs.google.com/forms/d/e/1FAIpQLSet_IAFQzMMDWolzFt5LI9lhzqOOStjIGHxgYqKBnVcRtDfrw/viewform?usp=dialog">Kubeflow Unified SDK Survey</a> 
to help us understand your biggest pain points and identify which new features will provide the most value to you and 
your team. Your feedback directly influences our roadmap and priorities.</p>

<p><strong>Resources</strong>:</p>
<ul>
  <li><a href="https://github.com/kubeflow/sdk">GitHub Repo</a></li>
  <li><a href="https://docs.google.com/document/d/1rX7ELAHRb_lvh0Y7BK1HBYAbA0zi9enB0F_358ZC58w/edit?tab=t.0#heading=h.e0573r7wwkgl">Kubeflow SDK design document</a></li>
</ul>

<p><strong>Connect with the Community</strong>:</p>
<ul>
  <li>Join <a href="https://cloud-native.slack.com/archives/C08KJBVDH5H">#kubeflow-ml-experience</a> on <a href="https://www.kubeflow.org/docs/about/community/#kubeflow-slack-channels">CNCF Slack</a></li>
  <li>Attend the <a href="https://bit.ly/kf-ml-experience">Kubeflow SDK and ML Experience WG</a> meetings</li>
  <li>Check out <a href="https://github.com/kubeflow/sdk/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22">good first issues</a> to get started</li>
</ul>]]></content><author><name>Kubeflow SDK Team</name></author><category term="sdk" /><category term="trainer" /><category term="optimizer" /><summary type="html"><![CDATA[‚ö° We want your feedback! Help shape the future of Kubeflow SDK by taking our quick survey.]]></summary></entry><entry><title type="html">GSoC 2025: Meet Our Projects and Contributors üöÄ</title><link href="https://blog.kubeflow.org/gsoc/community/kubeflow/2025/09/06/kubeflow-and-gsoc2025.html" rel="alternate" type="text/html" title="GSoC 2025: Meet Our Projects and Contributors üöÄ" /><published>2025-09-06T00:00:00-05:00</published><updated>2025-09-06T00:00:00-05:00</updated><id>https://blog.kubeflow.org/gsoc/community/kubeflow/2025/09/06/kubeflow-and-gsoc2025</id><content type="html" xml:base="https://blog.kubeflow.org/gsoc/community/kubeflow/2025/09/06/kubeflow-and-gsoc2025.html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>Google Summer of Code (GSoC) 2025 has been an exciting journey for the Kubeflow community! We are very grateful for Google and the open source community members dedication and effort.üéâ<br />
This year, 9 contributors from around the world collaborated with mentors to improve different parts of the Kubeflow ecosystem ‚Äî from infrastructure and CI/CD, to notebooks, ML workflows, and beyond.</p>

<p>In this blog, we are highlighting all the projects that were part of <strong>GSoC 2025</strong>, their goals, the impact they‚Äôve created, and the amazing contributors behind them.</p>

<p>üëâ You can explore the full list on our <a href="https://www.kubeflow.org/events/gsoc-2025/">GSoC 2025 page</a>.</p>

<hr />

<h2 id="-project-highlights">üìö Project Highlights</h2>

<p>Below are the projects from this year‚Äôs GSoC. Each section includes a short summary, contributor details, and links to project resources.</p>

<hr />

<h3 id="project-1-kubeflow-platform-enhancements">Project 1: Kubeflow Platform Enhancements</h3>
<p><strong>Contributor:</strong> Harshvir Potpose (<a href="https://github.com/akagami-harsh">@akagami-harsh</a>)
<strong>Mentors:</strong> Julius von Kohout (<a href="https://github.com/juliusvonkohout">@juliusvonkohout</a>)</p>

<p><strong>Overview:</strong><br />
We need an up to date S3 storage with hard multi-tenancy and run our containers with PodSecurityStandards restricted. MinIO transitioned to the AGPLv3 license in 2021, creating significant compliance challenges for the project.</p>

<p>This project addressed this critical blocker by implementing SeaweedFS as a production-ready replacement for MinIO. SeaweedFS offers a more permissive Apache 2.0 license while providing superior performance characteristics and enterprise-grade security and reliability.</p>

<p><strong>Key Outcomes:</strong></p>
<ul>
  <li>Provided S3 storage with hard multi-tenancy</li>
  <li>Successfully migrated to SeaweedFS as a secure replacement for MinIO and integrated it into Kubeflow Pipelines</li>
  <li>Eliminated MinIO‚Äôs licensing constraints by adopting SeaweedFS‚Äôs more permissive license model</li>
  <li>Implemented comprehensive CI tests for SeaweedFS deployment and namespace isolation functionality</li>
  <li>Strengthened the manifests repository‚Äôs CI pipeline and contributed to the dashboard migration efforts</li>
  <li>Enforcing PodSecurityStandards baseline/restricted</li>
</ul>

<p><strong>Resources:</strong></p>
<ul>
  <li>üìÑ <a href="https://summerofcode.withgoogle.com/programs/2025/projects/PWDq4Zvt">Project Page</a></li>
  <li>‚úçÔ∏è <a href="https://medium.com/@hpotpose26/kubeflow-pipelines-embraces-seaweedfs-9a7e022d5571">Personal Blog: Kubeflow Pipelines Embraces SeaweedFS</a></li>
</ul>

<hr />

<h3 id="project-2-kserve-models-web-application-modernization">Project 2: KServe Models Web Application Modernization</h3>
<p><strong>Contributor:</strong> (GitHub: <a href="https://github.com/LogicalGuy77">@LogicalGuy77</a>)<br />
<strong>Mentors:</strong> Griffin Sullivan (<a href="https://github.com/Griffin-Sullivan">@Griffin-Sullivan</a>), Julius von Kohout (<a href="https://github.com/juliusvonkohout">@juliusvonkohout</a>)</p>

<p><strong>Overview:</strong><br />
This project revived and modernized the KServe Models Web Application (Angular + Flask), the UI used to manage machine learning inference services in Kubeflow via KServe. What began as a small Node.js update evolved into a comprehensive upgrade of the frontend stack, CI/CD, testing, and feature set‚Äîbringing the app up to modern standards and making it easier for both users and contributors to work with.</p>

<p><strong>Key Outcomes:</strong></p>
<ul>
  <li>Modernized core stack: upgraded Node.js (v16 ‚Üí v23) and Angular (v12 ‚Üí v14), resolving security issues and improving performance</li>
  <li>Migrated container images from Docker Hub to GitHub Container Registry (GHCR) to avoid rate limits and improve reliability</li>
  <li>Overhauled CI/CD with GitHub Actions: updated actions, added intelligent caching for pip, Docker layers, and node_modules for significantly faster builds</li>
  <li>Introduced Jest unit tests for core utilities (e.g., parsing Kubernetes object statuses and KServe predictor configs)</li>
  <li>Added Cypress end-to-end tests for critical user journeys (deploy, edit, delete) including failure handling and input validation</li>
  <li>Wrote comprehensive documentation to help contributors run and extend the test suites</li>
  <li>Shipped ‚ÄúEdit InferenceService YAML‚Äù directly in the UI via an integrated Monaco editor‚Äîno kubectl required</li>
  <li>Fixed RawDeployment-mode crash and added ModelMesh support so resources and statuses render correctly</li>
  <li>Added support for the latest KServe predictor runtimes, including HuggingFace</li>
  <li>Simplified contributor onboarding with a Makefile that automates full frontend setup in a single command</li>
  <li>Implemented runtime-configurable settings via a new <code class="language-plaintext highlighter-rouge">/api/config</code> endpoint (e.g., Grafana DB names, URL prefixes)</li>
  <li>Cut the v0.15.0 release of the Models Web App, consolidating months of modernization and feature work</li>
</ul>

<p><strong>By the Numbers:</strong></p>
<ul>
  <li>PRs merged: 19</li>
  <li>Issues closed: 8</li>
  <li>Lines of code changed: +22,309 / ‚àí11,628</li>
  <li>Frontend: Angular, TypeScript, SCSS</li>
  <li>Backend: Flask (Python)</li>
  <li>CI/CD: GitHub Actions, Docker</li>
  <li>Local cluster: Kubernetes (Kind) + Istio + Kubeflow</li>
</ul>

<p><strong>Resources:</strong></p>
<ul>
  <li><a href="https://github.com/kserve/models-web-app">Project Repo: kserve/models-web-app</a></li>
  <li><a href="https://github.com/kserve/models-web-app/commits?author=LogicalGuy77">All commits by @LogicalGuy77</a></li>
  <li><a href="https://medium.com/@harshitweb3/my-gsoc-2025-journey-reviving-kserves-models-web-application-2f18ef16fb51">Blog Post</a></li>
</ul>

<hr />

<h3 id="project-3-istio-cni-and-ambient-mesh">Project 3: Istio CNI and Ambient Mesh</h3>
<p><strong>Contributor:</strong> Ayush Gupta (GitHub: <a href="https://github.com/madmecodes">@madmecodes</a>)<br />
<strong>Mentors:</strong> Julius von Kohout (<a href="https://github.com/juliusvonkohout">@juliusvonkohout</a>), Kimonas Sotirchos (<a href="https://github.com/kimwnasptd">@kimwnasptd</a>)</p>

<p><strong>Overview:</strong><br />
This GSoC 2025 project modernized Kubeflow‚Äôs service mesh infrastructure by implementing Istio CNI as the default configuration and pioneering Istio Ambient Mesh support. The 175-hour medium-difficulty project involved 25+ pull requests across multiple Kubeflow repositories, transitioning from traditional sidecar-based architecture to ambient mesh with ztunnel and waypoint proxies, pioneering the migration to Gateway API (HTTPRoute), implementing path-based routing for KServe model serving endpoints, and utilizing Kustomize overlay method for easy installation and configuration management.</p>

<p><strong>Key Outcomes:</strong></p>
<ul>
  <li>Implemented Istio CNI by default with Kustomize overlay method enabling easy switching between traditional Istio and CNI configurations</li>
  <li>Created path-based routing for KServe multi-model serving and Gateway API (HTTPRoute) migration</li>
  <li>Pioneered Ambient Mesh support with ztunnel/waypoint proxies and coordinating cross-repository compatibility</li>
</ul>

<p><strong>Resources:</strong></p>
<ul>
  <li>üìÑ <a href="https://summerofcode.withgoogle.com/programs/2025/projects/WAHCCi8V">Project Page</a></li>
  <li>‚úçÔ∏è <a href="https://medium.com/@ayushguptadev1/gsoc25-kubeflow-securing-and-optimizing-ml-infrastructure-with-istio-31f535c77fd6">Blog Post</a></li>
</ul>

<hr />

<h3 id="project-4-deploying-kubeflow-with-helm-charts">Project 4: Deploying Kubeflow with Helm Charts</h3>

<p><strong>Contributor:</strong> Kunal Dugar (<a href="https://github.com/kunal-511">@kunal-511</a>)<br />
<strong>Mentors:</strong> Julius von Kohout (<a href="https://github.com/juliusvonkohout">@juliusvonkohout</a>), Valentina Rodriguez Sosa (<a href="https://github.com/varodrig">@varodrig</a>), Chase Cadet (<a href="https://github.com/Chasecadet">@Chasecadet</a>)</p>

<p><strong>Overview:</strong><br />
This project focused on creating component-based Helm charts for Kubeflow, enabling flexible and incremental deployment of ML infrastructure. Instead of requiring a full platform installation, users can now deploy specific components like Katib, Pipelines, Model Registry, and others independently with customized configurations.</p>

<p><strong>Key Outcomes:</strong></p>
<ul>
  <li>Kubeflow AI reference platform end to end testing</li>
  <li>Created production-ready Helm charts for Katib, Model Registry, KServe Web App, Notebook Controller, and Kubeflow Pipelines‚Äîenabling one-command deployment of individual components</li>
  <li>Built automated testing infrastructure with diff tools to validate Helm charts against Kustomize manifests, ensuring accuracy and catching regressions quickly</li>
  <li>Enabled incremental Kubeflow adoption, reducing deployment complexity from days to hours for organizations building production ML platforms</li>
</ul>

<p><strong>Resources:</strong></p>
<ul>
  <li>üìÑ <a href="https://summerofcode.withgoogle.com/programs/2025/projects/">Project Page</a></li>
  <li>üß© <a href="https://github.com/kubeflow/community/pull/832">Kubeflow Enhancement Proposal (KEP)-831-Kubeflow-Helm-Support: Support Helm as an Alternative for Kustomize</a></li>
  <li>‚úçÔ∏è <a href="https://medium.com/@kunalD02/my-gsoc-journey-deploying-kubeflow-with-helm-charts-e7f9dea7b56e">Blog: My GSoC Journey: Deploying Kubeflow with Helm Charts</a></li>
</ul>

<hr />

<h3 id="project-5-jupyterlab-plugin-for-kubeflow">Project 5: JupyterLab Plugin for Kubeflow</h3>

<p><strong>Contributor:</strong> Amrit Kumar (<a href="https://github.com/Amrit27k">@Amrit27k</a>)<br />
<strong>Mentors:</strong> Eder Ignatowicz (<a href="https://github.com/ederign">@ederign</a>), Stefano Fioravanzo (<a href="https://github.com/StefanoFioravanzo">@StefanoFioravanzo</a>)</p>

<p><strong>Overview:</strong>
The project fully modernized Kubeflow Kale‚Äôs architecture, migrating the backend from KFPv1 to KFPv2 with a new Jinja2 templating system for notebook-to-pipeline conversion. The initiative also featured a complete overhaul of the JupyterLab frontend (Typescriptv5.9.2, MUIv7) and comprehensive updates to GitHub workflows, documentation, and dependencies to meet modern community standards.</p>

<p><strong>Key Outcomes:</strong></p>
<ul>
  <li>Rebuilt the Kale backend to support the modern, future-proof Kubeflow Pipelines v2 (KFPv2) architecture, moving away from the deprecated KFPv1.</li>
  <li>Implemented a new Jinja2 templating system that intelligently converts annotated Jupyter notebook cells into valid KFPv2 Python DSL scripts.</li>
  <li>Updated the JupyterLab frontend extension using current standards (Typescript v5.9.2, Jupyterlab v4, and MUI v7), resolving hundreds of legacy compatibility issues.</li>
  <li>Integrated KFPv2‚Äôs robust system for better type-safe artifact handling and automated ML Metadata registration, ensuring rich lineage tracking for pipeline steps.</li>
  <li>Standardized the project structure, updated GitHub workflows, and implemented UI test scripts to align with community standards and ensure maintainability for future contributors.</li>
</ul>

<p><strong>Resources:</strong></p>
<ul>
  <li>üìÑ <a href="https://github.com/kubeflow-kale/kale">Project Repo - Kubeflow Kale</a></li>
  <li>üß© <a href="https://github.com/kubeflow-kale/kale/issues/457">Kubeflow Kale 2.0- Project Roadmap</a></li>
  <li>‚úçÔ∏è <a href="https://medium.com/@amritkmr4272/from-notebooks-to-pipelines-my-gsoc25-journey-modernizing-kubeflow-kale-with-kfpv2-and-e098f194208c">Blog: From Notebooks to Pipelines: My GSoC‚Äô25 Journey Modernizing Kubeflow Kale with KFPv2 and Jupyterlabv4</a></li>
</ul>

<hr />

<h3 id="project-6-spark-operator-with-kubeflow-notebooks">Project 6: Spark Operator with Kubeflow Notebooks</h3>

<p><strong>Contributor:</strong> Fellipe Resende (<a href="https://github.com/fresende">@fresende</a>)<br />
<strong>Mentors:</strong> Shekhar Rajak (<a href="https://github.com/Shekharrajak">@Shekharrajak</a>),
Luciano Resende (<a href="https://github.com/lresende">@lresende</a>),
Chaoran Yu (<a href="https://github.com/yuchaoran2011">@yuchaoran2011</a>),
Andrey Velichkevich (<a href="https://github.com/andreyvelich">@andreyvelich</a>)</p>

<p><img src="/images/2025-09-06-kubeflow-and-gsoc2025/project6.png" alt="Diagram" /></p>

<p><strong>Overview:</strong>
This project enables seamless PySpark execution within Kubeflow Notebooks by integrating the Spark Operator and Jupyter Enterprise Gateway. It allows data scientists to run distributed machine learning and big data workloads directly from their notebooks on Kubernetes, simplifying workflows and eliminating Spark infrastructure overhead, improving both usability and scalability within the Kubeflow ecosystem.</p>

<p><strong>Key Outcomes:</strong></p>

<ul>
  <li>
    <p>Extended Kubeflow Notebooks to enable seamless integration with Spark via Spark Operator leveraging Jupyter Enterprise Gateway to manage the spark application lifecycle.</p>
  </li>
  <li>
    <p>Enable data scientists and ML engineer to run distributed big-data workloads directly in Spark, from inside Kubeflow Notebooks, without manual cluster setup.</p>
  </li>
  <li>
    <p>Provided documentation and guidance for setting up, configuring, and customizing Kubeflow Notebook environments integrated with the Spark Operator, enabling users to run scalable distributed Spark workloads directly from Jupyter-based workflows.</p>
  </li>
</ul>

<p><strong>Resources:</strong></p>

<ul>
  <li>üìò <a href="https://www.kubeflow.org/docs/components/spark-operator/user-guide/notebooks-spark-operator/">Main Documentation Page</a></li>
  <li>üé• <a href="https://youtu.be/g7tctdeitvc">Setup Demo Video</a></li>
  <li>üêû <a href="https://www.youtube.com/watch?v=p6K6PdlkmeU">Debugging Demo Video</a></li>
  <li>üìÑ <a href="https://summerofcode.withgoogle.com/programs/2025/projects/zRPtxGBI">Project Page</a></li>
  <li>üíª <a href="https://github.com/kubeflow/website/pull/4141">Implementation Pull Request</a></li>
</ul>

<hr />

<h3 id="project-7-gpu-testing-for-llm-blueprints">Project 7: GPU Testing for LLM Blueprints</h3>

<p><strong>Contributor:</strong> Akash Jaiswal (<a href="https://github.com/jaiakash">@jaiakash</a>)<br />
<strong>Mentors:</strong> Andrey Velichkevich (<a href="https://github.com/andreyvelich">@andreyvelich</a>), Valentina Rodriguez Sosa(<a href="https://github.com/varodrig">@varodrig</a>)</p>

<p><img src="/images/2025-09-06-kubeflow-and-gsoc2025/project7.png" alt="Diagram" /></p>

<p><strong>Overview:</strong><br />
We had a few examples in the repository that we wanted to include in our end-to-end (E2E) tests, but all of them were CPU-based. Projects like Torchtune and Qwen 2.5, for instance, require GPU resources to run ‚Äî yet our existing CI setup couldn‚Äôt validate them at all because it was entirely CPU-focused.</p>

<p>This created a major gap: whenever someone contributed a new LLM example or modified the trainer logic, we had no automated way to verify if those changes would work in a GPU environment ‚Äî the same environment where these workloads are actually deployed in production.</p>

<p>The goal of this project was to add CI with GPU support directly into our CI/CD workflow.</p>

<p><strong>Key Outcomes:</strong></p>

<ul>
  <li>
    <p>Integrating GPU runners into GitHub Actions so that any pull request could automatically trigger GPU-backed E2E tests.</p>
  </li>
  <li>
    <p>Making the setup scalable and cost-efficient ‚Äî instead of maintaining expensive GPU machines 24/7, we needed an on-demand system that provisions GPU resources only when a test is triggered.</p>
  </li>
</ul>

<p><strong>Resources:</strong></p>

<ul>
  <li>üìÑ <a href="https://summerofcode.withgoogle.com/programs/2025/projects/fwZkvPr0">Project Page</a></li>
  <li>üß© <a href="https://github.com/kubeflow/trainer/pull/2689">Kubeflow Enhancement Proposal (KEP)</a></li>
  <li>‚úçÔ∏è <a href="https://my-experience-with-kubeflow-for-gsoc.hashnode.dev/gsoc-2025-with-kubeflow-scaling-gpu-testing-for-llm-blueprints">Personal Blog: Scaling GPU Testing for LLM Blueprints</a></li>
</ul>

<hr />

<h3 id="project-10-support-volcano-scheduler-in-kubeflow-trainer">Project 10: Support Volcano Scheduler in Kubeflow Trainer</h3>
<p><strong>Contributor:</strong> Xinmin Du (GitHub: <a href="https://github.com/Doris-xm">@Doris-xm</a>)<br />
<strong>Mentors:</strong> Shao Wang (<a href="https://github.com/Electronic-Waste">@Electronic-Waste</a>), Yuchen Cheng(<a href="https://github.com/rudeigerc">@rudeigerc</a>)</p>

<p><strong>Overview:</strong><br />
The project aims to integrate the <strong>Volcano scheduler</strong> into Kubeflow Trainer as a <strong>runtime plugin</strong>.
This will allow users to take advantage of advanced AI-specific scheduling features, such as Gang Scheduling and priority scheduling, supported by Volcano.</p>

<p><strong>Key Outcomes:</strong></p>
<ul>
  <li>Integrate the <strong>Volcano</strong> scheduler into Trainer as a runtime plugin to support Gang Scheduling and resource management for distributed training jobs.</li>
  <li>Enabled AI-specific features such as priority scheduling, queue-based management, and network topology‚Äìaware scheduling.</li>
</ul>

<p><strong>Resources:</strong></p>

<ul>
  <li>üìÑ <a href="https://summerofcode.withgoogle.com/programs/2025/projects/ZWbY1Rfj">Project Page</a></li>
  <li>üß© <a href="https://github.com/kubeflow/trainer/pull/2672">Kubeflow Enhancement Proposal (KEP)</a></li>
</ul>

<hr />

<h3 id="project-12-empowering-kubeflow-documentation-with-llms-">Project 12: Empowering Kubeflow Documentation with LLMs ü§ñ</h3>
<p><strong>Contributor:</strong> Santhosh Toorpu (GitHub: <a href="https://github.com/SanthoshToorpu">@SanthoshToorpu</a>)<br />
<strong>Mentors:</strong> Francisco Javier Arceo (<a href="https://github.com/franciscojavierarceo">@franciscojavierarceo</a>), Chase Cadet (<a href="https://github.com/Chasecadet">@Chasecadet</a>)</p>

<p><strong>Overview:</strong><br />
This project introduced an intelligent documentation assistant that uses <strong>Retrieval-Augmented Generation (RAG)</strong> and <strong>KServe-hosted LLMs</strong> to enhance the Kubeflow documentation experience. The goal was to help users find relevant, accurate answers drawn from Kubeflow docs, GitHub issues, and community discussions ‚Äî all through a conversational interface on the Kubeflow website.</p>

<p>The system leverages <strong>Kubeflow Pipelines</strong> to automate documentation ingestion and indexing, <strong>Milvus</strong> for semantic vector search, and <strong>FastAPI with WebSockets</strong> for real-time interactions. Built on Kubernetes, the architecture follows Kubeflow‚Äôs MLOps principles end-to-end ‚Äî from automated retraining and indexing to monitored LLM inference served via KServe.</p>

<p><strong>Key Outcomes:</strong></p>
<ul>
  <li>Designed and deployed an <strong>LLM-powered Documentation Assistant</strong> using Kubeflow-native tools (KFP, KServe, Feast, Milvus).</li>
  <li>Implemented <strong>automated documentation indexing pipelines</strong> triggered by GitHub Actions to keep vector embeddings up-to-date.</li>
  <li>Developed an <strong>interactive chat interface</strong> integrated into the Kubeflow website for natural-language documentation search.</li>
  <li>Introduced a <strong>RAG agentic workflow</strong> with tool-calling to decide when to retrieve external documentation or use model knowledge.</li>
  <li>Implemented <strong>RBAC-based access control</strong> for pipelines and KServe endpoints to align with Kubeflow‚Äôs multi-user isolation standards.</li>
  <li>Developed a <strong>feedback loop system</strong> (‚Äúüëç / üëé‚Äù) to improve the model‚Äôs performance and documentation quality.</li>
  <li>Delivered a functional prototype hosted on Kubernetes, showcasing real-time semantic search across Kubeflow repositories.</li>
</ul>

<p><strong>Resources:</strong></p>
<ul>
  <li>üìÑ <a href="https://summerofcode.withgoogle.com/programs/2025/projects/a9JPxfEh">Project Page</a></li>
  <li>üß† <a href="https://github.com/kubeflow/docs-agent">Demo Repo</a></li>
  <li>‚úçÔ∏è <a href="https://medium.com/@toorpusanthosh/empowering-kubeflow-documentation-with-llms-my-gsoc-journey-58eb946ba2af">Blog Post: Empowering Kubeflow Documentation with LLMs</a> <!-- Add blog link here when published --></li>
</ul>

<hr />

<h2 id="-wrapping-up">üéâ Wrapping Up</h2>

<p>We are proud of what our GSoC 2025 contributors achieved and the impact they have made on the Kubeflow ecosystem. Their work not only strengthens existing components but also lays the foundation for future innovation in MLOps and AI infrastructure.</p>

<p>A huge <strong>thank you</strong> üôè to all contributors, mentors, and community members who made this program a success.</p>

<hr />

<h2 id="-want-to-get-involved">üë©‚Äçüíª Want to Get Involved?</h2>

<p>The Kubeflow community is open to contributors of all backgrounds and skill levels. Whether you are passionate about ML infrastructure, frontend, DevOps, or documentation ‚Äî there‚Äôs a place for you here.</p>

<ul>
  <li>üíª Visit our <a href="https://www.kubeflow.org/docs/about/community/">website</a> and <a href="https://github.com/kubeflow">GitHub</a></li>
  <li>üí¨ Join our <a href="https://www.kubeflow.org/docs/about/community/">Slack</a></li>
  <li>üóìÔ∏è Attend the <a href="https://www.kubeflow.org/docs/about/community/#kubeflow-community-call">community calls</a></li>
  <li>üì© Subscribe to the <a href="https://groups.google.com/g/kubeflow-discuss">kubeflow-discuss</a> mailing list</li>
</ul>

<p>Let‚Äôs continue building the future of MLOps together üöÄ</p>]]></content><author><name>Kubeflow Outreach Team</name></author><category term="gsoc" /><category term="community" /><category term="kubeflow" /><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">KubeCon India 2025 with Kubeflow: Our Community Experience</title><link href="https://blog.kubeflow.org/kubecon/community/2025/08/23/kubecon-2025-india-kubeflow.html" rel="alternate" type="text/html" title="KubeCon India 2025 with Kubeflow: Our Community Experience" /><published>2025-08-23T00:00:00-05:00</published><updated>2025-08-23T00:00:00-05:00</updated><id>https://blog.kubeflow.org/kubecon/community/2025/08/23/kubecon-2025-india-kubeflow</id><content type="html" xml:base="https://blog.kubeflow.org/kubecon/community/2025/08/23/kubecon-2025-india-kubeflow.html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p><img src="/images/2025-08-23-kubecon-2025-india-kubeflow/KubeConIndiaKeynote.png" alt="KubeCon India 2025" /></p>

<p>KubeCon + CloudNativeCon India 2025 in Hyderabad was an absolute blast! As a second-time attendee (<a href="https://github.com/jaiakash">Akash Jaiswal</a>) and a first-time attendee (<a href="https://github.com/yashpal2104">Yash Pal</a>), we couldn‚Äôt help but be blown away by the incredible energy at one of world‚Äôs biggest cloud native gatherings. We were super excited seeing Kubeflow get a special shoutout during the opening keynote for its role in cloud native AI/ML and MLOps - definitely made us proud to be part of the community! (Above image shows the keynote moment)</p>

<p>We also got super lucky with the chance to volunteer at the Kubeflow booth this year. We also met <a href="https://github.com/johnugeorge">Johnu George</a> in person, who delivered two amazing talks on Kubeflow‚Äôs latest capabilities. It was really exciting to finally meet community members face-to-face whom we‚Äôve only seen in community calls and Slack!</p>

<p>This blog shares all the exciting bits from our packed 2 days at KubeCon - from awesome booth conversations to technical deep-dives. We hope this motivates more community members to not just contribute but also attend and help Kubeflow at events like KubeCon. Trust me, you won‚Äôt want to miss the next one! üòä</p>

<h2 id="featured-talks">Featured Talks</h2>

<ul>
  <li><strong>Cloud Native GenAI using KServe and OPEA</strong>
<strong>Speakers:</strong> <a href="https://github.com/johnugeorge">Johnu George</a>, Gavrish Prabhu (Nutanix)
<strong>Sched Link:</strong> <a href="https://kccncind2025.sched.com/event/23EtS/cloud-native-genai-using-kserve-and-opea-johnu-george-gavrish-prabhu-nutanix">View on Sched</a></li>
</ul>

<iframe width="100%" height="400" src="https://www.youtube.com/embed/0o8Ng0E1rrA?list=PLj6h78yzYM2MEQTMX_LIOK1hrePHxLD6U" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<ul>
  <li><strong>Bridging Big Data and Machine Learning Ecosystems</strong>
<strong>Speakers:</strong> <a href="https://github.com/johnugeorge">Johnu George</a>, Shiv Jha (Nutanix)
<strong>Sched Link:</strong> <a href="https://kccncind2025.sched.com/event/23Eur/bridging-big-data-and-machine-learning-ecosystems-a-cloud-native-approach-using-kubeflow-johnu-george-shiv-jha-nutanix">View on Sched</a></li>
</ul>

<iframe width="100%" height="400" src="https://www.youtube.com/embed/3NWFCKUhB3A?list=PLj6h78yzYM2MEQTMX_LIOK1hrePHxLD6U" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h2 id="kubeflow-booth-highlights">Kubeflow Booth Highlights</h2>

<p><img src="/images/2025-08-23-kubecon-2025-india-kubeflow/KubeflowBoothPic.png" alt="Kubeflow Booth" /></p>

<p>Here‚Äôs a picture of our Kubeflow booth volunteer team. It was really great to meet and interact with audiences who had dozens of questions about Kubeflow, contributors who wanted to help, and developers who were already using it and shared their experiences.</p>

<p>Here are some key highlights from our booth conversations:</p>

<ul>
  <li><strong>Community Engagement:</strong>
    <ul>
      <li>Discussions on real-world use cases and deployment strategies. Few users shared their experience of using Kubeflow in their companies and how its benefiting them.</li>
      <li>Many of the audience wants to learn more about how to explore and contribute to Kubeflow. (Answers: Join community calls, and check out GitHub for open issues)</li>
      <li>Several companies expressed interest in adopting projects like Kubeflow. Few senior engineers were already using it for some of their workloads, now they want to use it for production workload.</li>
    </ul>
  </li>
  <li><strong>Popular Questions from Audience:</strong>
    <ul>
      <li>How does Kubeflow simplify ML workflows using Kubernetes? Can you clarify why Kubeflow is not multicluster agnostic?
Answer: You can just send your jobs to 5 different and independent Kubeflow clusters if you want to. So We do not think that this is needed at all. We offer APIs for external access (KFP, everything you can also do in the UI) so we do not need the Kubeflow deployment to span multiple clusters directly. If you want to span multiple regions then either use the API of multiple independent Kubeflow clusters in different regions and just submit your jobs or use a Kubernetes layer that transparently handles clusters spanning multiple regions. But nevertheless adding this complexity burden on Kubeflow does not offer much benefit.</li>
      <li>How does Kubeflow integrate with other cloud-native tools? How is Kubeflow different from other tools in the industry?</li>
      <li>What are the security considerations for running ML pipelines? How can Kubeflow help optimize costs when working with LLMs, especially in terms of minimizing GPU usage to stay within quota limits while still delivering performance?</li>
      <li>How mature is Kubeflow today, and how well does it align with the workflows of different MLOps? What is the timeline of graduation for Kubeflow? What does the roadmap for Kubeflow look like?</li>
      <li>Why has Kubeflow chosen to integrate with ArgoCD rather than Tekton CD ‚Äî the question that came up from a maintainer of the Tekton project.</li>
    </ul>
  </li>
</ul>

<h2 id="our-experience">Our experience</h2>

<p>What an incredible journey these past two days have been! Beyond the technical talks and booth duties, what really stood out was the genuine excitement around Kubeflow in the community. Seeing users‚Äô faces light up when sharing their success stories, or watching newcomers get that ‚Äúaha!‚Äù moment during demos - these are the moments that make community events special.</p>

<p>The technical discussions were mind-blowing too. From hearing how startups are using Kubeflow to train their LLMs, to learning how enterprises are scaling it across thousands of models - each conversation taught us something new. We even got into some heated (but friendly!) debates about MLOps architectures and the future of AI on Kubernetes.</p>

<p>But the best part? The people. Meeting community members we‚Äôve only known through Slack emojis and GitHub comments to sharing chai/biryani with fellow contributors. These personal connections are what make the open source community truly special. Can‚Äôt wait for the next one! üöÄ</p>

<h2 id="want-to-help">Want to help?</h2>

<p>The Kubeflow community holds open meetings and is always looking for more volunteers and users to unlock the potential of machine learning. If you‚Äôre interested in becoming a Kubeflow contributor, please feel free to check out the resources below. We look forward to working with you!</p>

<ul>
  <li>Visit our <a href="https://www.kubeflow.org/docs/about/community/">website</a> or <a href="https://github.com/kubeflow">GitHub</a> page.</li>
  <li>Join the <a href="https://www.kubeflow.org/docs/about/community/">Kubeflow Slack channels</a>.</li>
  <li>Join the <a href="https://groups.google.com/g/kubeflow-discuss">kubeflow-discuss</a> mailing list.</li>
  <li>Want to volunteer for such events, Join the <a href="https://cloud-native.slack.com/archives/C078ZMRQPB6">kubeflow-outreach</a> channel on CNCF Slack.</li>
  <li>Attend our weekly <a href="https://www.kubeflow.org/docs/about/community/#kubeflow-community-call">community meeting</a>.</li>
</ul>

<p>Feel free to share your thoughts or questions in the comments!</p>]]></content><author><name>Akash Jaiswal, Yash Pal</name></author><category term="kubecon" /><category term="community" /><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Democratizing AI Model Training on Kubernetes: Introducing Kubeflow Trainer V2</title><link href="https://blog.kubeflow.org/trainer/intro/" rel="alternate" type="text/html" title="Democratizing AI Model Training on Kubernetes: Introducing Kubeflow Trainer V2" /><published>2025-07-21T00:00:00-05:00</published><updated>2025-07-21T00:00:00-05:00</updated><id>https://blog.kubeflow.org/trainer/introducing-trainer-v2</id><content type="html" xml:base="https://blog.kubeflow.org/trainer/intro/"><![CDATA[<p>Running machine learning workloads on Kubernetes can be challenging.
Distributed training and LLMs fine-tuning, in particular, involves managing multiple nodes, GPUs, large datasets, and fault tolerance, which often requires deep Kubernetes knowledge.
The <strong>Kubeflow Trainer v2 (KF Trainer)</strong> was created to hide this complexity, by abstracting Kubernetes from AI Practitioners and providing the easiest, most scalable way to run distributed PyTorch jobs.</p>

<p><strong>The main goals of Kubeflow Trainer v2 include:</strong></p>
<ul>
  <li>Make AI/ML workloads easier to manage at scale</li>
  <li>Provide a Pythonic interface to train models</li>
  <li>Deliver the easiest and most scalable PyTorch distributed training on Kubernetes</li>
  <li>Add built-in support for fine-tuning large language models</li>
  <li>Abstract Kubernetes complexity from AI Practitioners</li>
  <li>Consolidate efforts between Kubernetes Batch WG and Kubeflow community</li>
</ul>

<p>We‚Äôre deeply grateful to all contributors and community members who made the <strong>Trainer v2</strong> possible with their hard work and valuable feedback.
We‚Äôd like to give special recognition to <a href="https://github.com/andreyvelich">andreyvelich</a>, <a href="https://github.com/tenzen-y">tenzen-y</a>, <a href="https://github.com/electronic-waste">electronic-waste</a>, <a href="https://github.com/astefanutti">astefanutti</a>, <a href="https://github.com/ironicbo">ironicbo</a>, <a href="https://github.com/mahdikhashan">mahdikhashan</a>, <a href="https://github.com/kramaranya">kramaranya</a>, <a href="https://github.com/harshal292004">harshal292004</a>, <a href="https://github.com/akshaychitneni">akshaychitneni</a>, <a href="https://github.com/chenyi015">chenyi015</a> and the rest of the contributors.
We would also like to highlight <a href="https://github.com/ahg-g">ahg-g</a>, <a href="https://github.com/kannon92">kannon92</a>, and <a href="https://github.com/vsoch">vsoch</a> whose feedback was essential while we designed the Kubeflow Trainer architecture together with the Batch WG.
See the full <a href="https://kubeflow.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;var-period_name=Last%206%20months&amp;var-metric=commits&amp;var-repogroup_name=kubeflow%2Ftrainer&amp;var-country_name=All&amp;var-companies=All">contributor list</a> for everyone who helped make this release possible.</p>

<h1 id="background-and-evolution">Background and Evolution</h1>

<p><strong>Kubeflow Trainer v2</strong> represents the next evolution of the <strong>Kubeflow Training Operator</strong>, building on over seven years of experience running ML workloads on Kubernetes.
The journey began in 2017 when the <strong>Kubeflow</strong> project introduced <strong>TFJob</strong> to orchestrate TensorFlow training on Kubernetes.
At that time, Kubernetes lacked many of the advanced batch processing features needed for distributed ML training, so the community had to implement these capabilities from scratch.</p>

<p>Over the years, the project expanded to support multiple ML frameworks including <strong>PyTorch</strong>, <strong>MXNet</strong>, <strong>MPI</strong>, and <strong>XGBoost</strong> through various specialized operators.
In 2021, these were consolidated into the unified <strong><a href="https://docs.google.com/document/d/1x1JPDQfDMIbnoQRftDH1IzGU0qvHGSU4W6Jl4rJLPhI/edit?tab=t.0#heading=h.e33ufidnl8z6">Training Operator v1</a></strong>.
Meanwhile, the Kubernetes community introduced the <strong>Batch Working Group</strong>, developing important APIs like JobSet, Kueue, Indexed Jobs, and PodFailurePolicy that improved HPC and AI workload management.</p>

<p><strong>Trainer v2</strong> leverages these Kubernetes-native improvements to make use of existing functionality and not reinvent the wheel.
This collaboration between the Kubernetes and Kubeflow communities delivers a more standardized approach to ML training on Kubernetes.</p>

<h1 id="user-personas">User Personas</h1>

<p>One of the main challenges with ML training on Kubernetes is that it often requires <strong>AI Practitioners</strong> to have an understanding of <strong>Kubernetes concepts</strong> and the <strong>infrastructure</strong> being used for training. This distracts AI Practitioners from their primary focus.</p>

<p><strong>The KF Trainer v2</strong> addresses this by <strong>separating the infrastructure configuration from the training job definition</strong>.
This separation is built around three new custom resources definitions (CRDs):</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">TrainingRuntime</code> - a namespace-scoped resource that contains the infrastructure details that are required for a training job, such as the training image to use, failure policy, and gang-scheduling configuration.</li>
  <li><code class="language-plaintext highlighter-rouge">ClusterTrainingRuntime</code> - similar to <code class="language-plaintext highlighter-rouge">TrainingRuntime</code>, but cluster scoped.</li>
  <li><code class="language-plaintext highlighter-rouge">TrainJob</code> - specifies the training job configuration, including the training code to run, config for pulling the training dataset &amp; model, and a reference to the training runtime.</li>
</ul>

<p>The diagram below shows how different personas interact with these custom resources:</p>

<p><img src="/images/2025-07-21-introducing-trainer-v2/user-personas.drawio.svg" alt="user_personas" /></p>

<ul>
  <li><strong>Platform Administrators</strong> define and manage <strong>the infrastructure configurations</strong> required for training jobs using <code class="language-plaintext highlighter-rouge">TrainingRuntimes</code> or <code class="language-plaintext highlighter-rouge">ClusterTrainingRuntimes</code>.</li>
  <li><strong>AI Practitioners</strong> focus on model development using the simplified <code class="language-plaintext highlighter-rouge">TrainJob</code> resource or <strong>Python SDK</strong> wrapper, providing a reference to <strong>the training runtime</strong> created by <strong>Platform Administrators</strong>.</li>
</ul>

<h1 id="python-sdk">Python SDK</h1>

<p><strong>The KF Trainer v2</strong> introduces a <strong>redesigned Python SDK</strong>, which is intended to be the <strong>primary interface for AI Practitioners</strong>.
The SDK provides a unified interface across multiple ML frameworks and cloud environments, abstracting away the underlying Kubernetes complexity.</p>

<p>The diagram below illustrates how Kubeflow Trainer provides a consistent experience for running ML jobs across different ML frameworks, Kubernetes infrastructures, and cloud providers:</p>

<p><img src="/images/2025-07-21-introducing-trainer-v2/trainerv2.png" alt="trainerv2" /></p>

<p><strong>Kubeflow Trainer v2</strong> supports multiple ML frameworks through <strong>pre-configured runtimes</strong>. The table below shows the current framework support:</p>

<p><img src="/images/2025-07-21-introducing-trainer-v2/runtimes.png" alt="runtimes" /></p>

<p>The SDK makes it easier for users familiar with Python to <strong>create, manage, and monitor training jobs</strong>, without requiring them to deal with any YAML definitions:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from kubeflow.trainer import TrainerClient

client = TrainerClient()

def my_train_func():
    """User defined function that runs on each distributed node process"""
    import os
    import torch
    import torch.distributed as dist
    from torch.utils.data import DataLoader, DistributedSampler
    
    # Setup PyTorch distributed
    backend = "nccl" if torch.cuda.is_available() else "gloo"
    local_rank = int(os.getenv("LOCAL_RANK", 0))
    dist.init_process_group(backend=backend)
    
    # Define your model, dataset, and training loop
    model = YourModel()
    dataset = YourDataset()
    train_loader = DataLoader(dataset, sampler=DistributedSampler(dataset))
    
    # Your training logic here
    for epoch in range(num_epochs):
        for batch in train_loader:
            # Forward pass, backward pass, optimizer step
            ...
            
    # Wait for the distributed training to complete
    dist.barrier()
    if dist.get_rank() == 0:
        print("Training is finished")

    # Clean up PyTorch distributed
    dist.destroy_process_group()

job_name = client.train(
  runtime=client.get_runtime("torch-distributed"),
  trainer=CustomTrainer(
    func=my_train_func,
    num_nodes=5,
    resources_per_node={
      "gpu": 2,
     },
  ),
)

job = client.get_job(name=job_name)

for step in job.steps:
   print(f"Step: {step.name}, Status: {step.status}")

client.get_job_logs(job_name, follow=True)
</code></pre></div></div>
<p>The SDK handles all Kubernetes API interactions. This eliminates the need for AI Practitioners to directly interact with the Kubernetes API.</p>

<h1 id="simplified-api">Simplified API</h1>

<p>Previously, in the <strong>Kubeflow Training Operator</strong> users worked with different custom resources for each ML framework, each with their own framework-specific configurations.
The <strong>KF Trainer v2</strong> replaces these multiple CRDs with a <strong>unified TrainJob API</strong> that works with <strong>multiple ML frameworks</strong>.</p>

<p>For example, here‚Äôs how a <strong>PyTorch training job</strong> looks like using <strong>KF Trainer v1</strong>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>apiVersion: kubeflow.org/v1
kind: PyTorchJob
metadata:
  name: pytorch-simple
  namespace: kubeflow
spec:
  pytorchReplicaSpecs:
    Master:
      replicas: 1
      restartPolicy: OnFailure
      template:
        spec:
          containers:
            - name: pytorch
              image: docker.io/kubeflowkatib/pytorch-mnist:v1beta1-45c5727
              imagePullPolicy: Always
              command:
                - "python3"
                - "/opt/pytorch-mnist/mnist.py"
                - "--epochs=1"
    Worker:
      replicas: 1
      restartPolicy: OnFailure
      template:
        spec:
          containers:
            - name: pytorch
              image: docker.io/kubeflowkatib/pytorch-mnist:v1beta1-45c5727
              imagePullPolicy: Always
              command:
                - "python3"
                - "/opt/pytorch-mnist/mnist.py"
                - "--epochs=1"
</code></pre></div></div>

<p>In the <strong>KF Trainer v2</strong>, creating an equivalent job becomes much simpler:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>apiVersion: trainer.kubeflow.org/v1alpha1
kind: TrainJob
metadata:
  name: pytorch-simple
  namespace: kubeflow
spec:
  trainer:
    numNodes: 2
    image: docker.io/kubeflowkatib/pytorch-mnist:v1beta1-45c5727
    command:
      - "python3"
      - "/opt/pytorch-mnist/mnist.py"
      - "--epochs=1"
  runtimeRef:
    name: torch-distributed
    apiGroup: trainer.kubeflow.org
    kind: ClusterTrainingRuntime
</code></pre></div></div>

<p>Additional <strong>infrastructure</strong> and <strong>Kubernetes-specific</strong> details are provided in the referenced <strong>runtime</strong> definition, and managed separately by <strong>Platform Administrators</strong>.
In the future, we might support other runtimes in addition to <code class="language-plaintext highlighter-rouge">TrainingRuntime</code> and <code class="language-plaintext highlighter-rouge">ClusterTrainingRuntime</code>, for example <a href="https://github.com/kubeflow/trainer/issues/2249">SlurmRuntime</a>.</p>

<h1 id="extensibility-and-pipeline-framework">Extensibility and Pipeline Framework</h1>

<p>One of the challenges in <strong>KF Trainer v1</strong> was supporting additional ML frameworks, especially for closed-sourced frameworks.
The v2 architecture addresses this by introducing a <strong>Pipeline Framework</strong> that allows Platform Administrators  to <strong>extend the Plugins</strong> and <strong>support orchestration</strong> for their custom in-house ML frameworks.</p>

<p>The diagram below shows Kubeflow Trainer Pipeline Framework overview:</p>

<p><img src="/images/2025-07-21-introducing-trainer-v2/trainer-pipeline-framework.drawio.svg" alt="trainer_pipeline_framework" /></p>

<p>The framework works through a series of phases - <strong>Startup</strong>, <strong>PreExecution</strong>, <strong>Build</strong>, and <strong>PostExecution</strong> - each with <strong>extension points</strong> where custom Plugins can hook in.
This approach allows adding support for new frameworks, custom validation logic, or specialized training orchestration without changing the underlying system.</p>

<h1 id="llms-fine-tuning-support">LLMs Fine-Tuning Support</h1>

<p>Another improvement of <strong>Trainer v2</strong> is its <strong>built-in support for fine-tuning large language models</strong>, where we provide two types of trainers:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">BuiltinTrainer</code> - already includes the fine-tuning logic and allows AI Practitioners to quickly start fine-tuning requiring only parameter adjustments.</li>
  <li><code class="language-plaintext highlighter-rouge">CustomTrainer</code> - allows users to provide their own training function that encapsulates the entire LLMs fine-tuning.</li>
</ul>

<p>In the first release, we support <strong>TorchTune LLM Trainer</strong> as the initial option for <code class="language-plaintext highlighter-rouge">BuiltinTrainer</code>.
For TorchTune, we provide pre-configured runtimes (<code class="language-plaintext highlighter-rouge">ClusterTrainingRuntime</code>) that currently support <code class="language-plaintext highlighter-rouge">Llama-3.2-1B-Instruct</code> and <code class="language-plaintext highlighter-rouge">Llama-3.2-3B-Instruct</code> in the <a href="https://github.com/kubeflow/trainer/tree/master/manifests/base/runtimes/torchtune/llama3_2">manifest</a>.
This approach means that in the future, we can add more frameworks, such as <a href="https://github.com/unslothai/unsloth">unsloth</a>, as additional <code class="language-plaintext highlighter-rouge">BuiltinTrainer</code> options.
Here‚Äôs an example using the <code class="language-plaintext highlighter-rouge">BuiltinTrainer</code> with <strong>TorchTune</strong>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>job_name = client.train(
    runtime=Runtime(
        name="torchtune-llama3.2-1b"
    ),
    initializer=Initializer(
        dataset=HuggingFaceDatasetInitializer(
            storage_uri="hf://tatsu-lab/alpaca/data"
        ),
        model=HuggingFaceModelInitializer(
            storage_uri="hf://meta-llama/Llama-3.2-1B-Instruct",
            access_token="&lt;YOUR_HF_TOKEN&gt;"  # Replace with your Hugging Face token,
        )
    ),
    trainer=BuiltinTrainer(
        config=TorchTuneConfig(
            dataset_preprocess_config=TorchTuneInstructDataset(
                source=DataFormat.PARQUET,
            ),
            resources_per_node={
                "gpu": 1,
            }
        )
    )
)
</code></pre></div></div>

<p>This example uses a <strong>builtin runtime image</strong> that uses a foundation Llama model, and fine-tunes it using a dataset pulled from Hugging Face, with the TorchTune configuration provided by the AI Practitioner.
For more details, please refer to <a href="https://github.com/kubeflow/trainer/blob/master/examples/torchtune/llama3_2/alpaca-trainjob-yaml.ipynb">this example</a>.</p>

<h1 id="dataset-and-model-initializers">Dataset and Model Initializers</h1>

<p><strong>Trainer v2</strong> provides <strong>dedicated initializers</strong> for datasets and models, which significantly simplify the setup process.
Instead of each training pod independently downloading large models and datasets, <strong>initializers handle this once</strong> and <strong>share the data</strong> across all training nodes through a <strong>shared volume</strong>.</p>

<p>This approach saves both <strong>time and resources</strong> by preventing network slowdowns, and <strong>reducing GPU waiting time</strong> during setup by offloading data loading tasks to CPU-based initializers, which preserves expensive GPU resources for the actual training.</p>

<h1 id="use-of-jobset-api">Use of JobSet API</h1>

<p>Under the hood, the <strong>KF Trainer v2</strong> uses <strong><a href="https://jobset.sigs.k8s.io/docs/overview/">JobSet</a></strong>, a <strong>Kubernetes-native API</strong> for managing groups of jobs.
This integration allows the KF Trainer v2 to better utilize standard Kubernetes features instead of trying to recreate them.</p>

<h1 id="kueue-integration">Kueue Integration</h1>

<p>Resource management is improved through integration with <strong><a href="https://kueue.sigs.k8s.io/">Kueue</a></strong>, a <strong>Kubernetes-native queueing system</strong>.
The KF Trainer v2 includes initial support for Kueue through Pod Integration, which allows individual training pods to be queued when resources are busy.
We are working on <strong><a href="https://github.com/kubernetes-sigs/kueue/issues/3884">native Kueue support</a></strong> for <code class="language-plaintext highlighter-rouge">TrainJob</code> to provide richer queueing features in future releases.</p>

<h1 id="mpi-support">MPI Support</h1>

<p>The <strong>KF Trainer v2</strong> also provides <strong>MPI v2 support</strong>, which includes <strong>automatic generation of SSH keys</strong> for secure inter-node communication and boosting performance MPI on Kubernetes.</p>

<p><img src="/images/2025-07-21-introducing-trainer-v2/MPI-support.drawio.svg" alt="MPI_support" /></p>

<p>The diagram above shows how this works in practice - the <strong>KF Trainer</strong> automatically <strong>handles the SSH key generation</strong> and <strong>MPI communication</strong> between training pods, which allows frameworks like <a href="https://www.deepspeed.ai/">DeepSpeed</a> to coordinate training across multiple GPU nodes without requiring manual configuration of inter-node communication.</p>

<h1 id="gang-scheduling">Gang-Scheduling</h1>

<p><strong>Gang-scheduling</strong> is an important feature for distributed training that ensures <strong>all pods in a training job are scheduled together</strong> or not at all.
This prevents scenarios where only some pods are scheduled while others remain pending due to resource constraints, which would waste GPU resources and prevent training from starting.</p>

<p><strong>The KF Trainer v2</strong> provides <strong>built-in gang-scheduling support</strong> through <strong>PodGroupPolicy API</strong>.
This creates <strong>PodGroup resources</strong> that ensure all required pods can be scheduled simultaneously before the training job starts.</p>

<p><strong>Platform Administrators</strong> can configure gang-scheduling in their <strong>TrainingRuntime</strong> or <strong>ClusterTrainingRuntime</strong> definitions. Here‚Äôs an example:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">trainer.kubeflow.org/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ClusterTrainingRuntime</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">torch-distributed-gang-scheduling</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">mlPolicy</span><span class="pi">:</span>
    <span class="na">numNodes</span><span class="pi">:</span> <span class="m">3</span>
    <span class="na">torch</span><span class="pi">:</span>
      <span class="na">numProcPerNode</span><span class="pi">:</span> <span class="m">2</span>
  <span class="na">podGroupPolicy</span><span class="pi">:</span>
    <span class="na">coscheduling</span><span class="pi">:</span>
      <span class="na">scheduleTimeoutSeconds</span><span class="pi">:</span> <span class="m">120</span>
  <span class="c1"># ... rest of runtime configuration</span>
</code></pre></div></div>

<p>Currently, <strong>KF Trainer v2</strong> supports the <strong>Co-Scheduling plugin</strong> from <a href="https://github.com/kubernetes-sigs/scheduler-plugins">Kubernetes scheduler-plugins</a> project.
<strong><a href="https://github.com/kubeflow/trainer/pull/2672">Volcano</a></strong> and <strong><a href="https://github.com/kubeflow/trainer/pull/2663">KAI</a></strong> scheduler support is coming in future releases to provide more advanced scheduling capabilities.</p>

<h1 id="fault-tolerance-improvements">Fault Tolerance Improvements</h1>

<p>Training jobs can sometimes fail due to node issues or other problems. The <strong>KF Trainer v2</strong> improves handling these faults by supporting <strong>Kubernetes PodFailurePolicy</strong>, which allows users to <strong>define specific rules</strong> for handling different types of failures, such as restarting the job after temporary node issues or terminating the job after critical errors.</p>

<h1 id="whats-next">What‚Äôs Next?</h1>

<p>Future enhancements will continue to improve the user experience, integrate deeper with other Kubeflow components, and support more training frameworks.
<strong>Upcoming features</strong> include:</p>
<ul>
  <li><strong><a href="https://github.com/kubeflow/sdk/issues/22">Local Execution</a></strong> - run training jobs locally without Kubernetes</li>
  <li><strong><a href="https://docs.google.com/document/d/1rX7ELAHRb_lvh0Y7BK1HBYAbA0zi9enB0F_358ZC58w/edit?tab=t.0#heading=h.e0573r7wwkgl">Unified Kubeflow SDK</a></strong> - a single SDK for all Kubeflow projects</li>
  <li><strong><a href="https://github.com/kubeflow/trainer/issues/2648">Trainer UI</a></strong> - a user interface to expose high level metrics for training jobs and monitor training logs</li>
  <li><strong><a href="https://github.com/kubernetes-sigs/kueue/issues/3884">Native Kueue integration</a></strong> - improve resource management and scheduling capabilities for TrainJob resources</li>
  <li><strong><a href="https://github.com/kubeflow/trainer/issues/2245">Model Registry integrations</a></strong> - export trained models directly to Model Registry</li>
  <li><strong><a href="https://github.com/kubeflow/community/pull/864">Distributed Data Cache</a></strong> - in-memory Apache Arrow caching for tabular datasets</li>
  <li><strong><a href="https://github.com/kubeflow/trainer/pull/2672">Volcano support</a></strong> - advanced AI-specific scheduling with gang scheduling, priority queues, and resource management capabilities</li>
  <li><strong><a href="https://github.com/kubeflow/trainer/pull/2643">JAX runtime support</a></strong> - ClusterTrainingRuntime for JAX distributed training</li>
  <li><strong><a href="https://github.com/kubeflow/trainer/pull/2663">KAI Scheduler support</a></strong> - NVIDIA‚Äôs GPU-optimized scheduler for AI workloads</li>
</ul>

<h1 id="migration-from-training-operator-v1">Migration from Training Operator v1</h1>

<p>For users migrating from <strong>Kubeflow Training Operator v1</strong>, check out a <a href="https://www.kubeflow.org/docs/components/trainer/operator-guides/migration/"><strong>Migration Guide</strong></a>.</p>

<h1 id="resources-and-community">Resources and Community</h1>

<p>For more information about <strong>Trainer V2</strong>, check out the <a href="https://www.kubeflow.org/docs/components/trainer/">Kubeflow Trainer documentation</a> and the <a href="https://github.com/kubeflow/trainer/tree/master/docs/proposals/2170-kubeflow-trainer-v2">design proposal</a> for technical implementation details.</p>

<p>For more details about Kubeflow Trainer, you can also watch our KubeCon presentations:</p>
<ul>
  <li><a href="https://youtu.be/Lgy4ir1AhYw">Democratizing AI Model Training on Kubernetes with Kubeflow TrainJob and JobSet</a></li>
  <li><a href="https://youtu.be/Fnb1a5Kaxgo">From High Performance Computing To AI Workloads on Kubernetes: MPI Runtime in Kubeflow TrainJob</a></li>
</ul>

<p>Join the community via the <a href="https://www.kubeflow.org/docs/about/community/#kubeflow-slack-channels">#kubeflow-trainer</a> channel on CNCF Slack, or attend the <a href="https://docs.google.com/document/d/1MChKfzrKAeFRtYqypFbMXL6ZIc_OgijjkvbqmwRV-64/edit?tab=t.0#heading=h.o8oe6e5kry87">AutoML and Training Working Group</a> meetings to contribute or ask questions.
Your feedback, contributions, and questions are always welcome!</p>]]></content><author><name>Kubeflow Trainer Team</name></author><category term="trainer" /><summary type="html"><![CDATA[Running machine learning workloads on Kubernetes can be challenging. Distributed training and LLMs fine-tuning, in particular, involves managing multiple nodes, GPUs, large datasets, and fault tolerance, which often requires deep Kubernetes knowledge. The Kubeflow Trainer v2 (KF Trainer) was created to hide this complexity, by abstracting Kubernetes from AI Practitioners and providing the easiest, most scalable way to run distributed PyTorch jobs.]]></summary></entry><entry><title type="html">From Raw Data to Model Serving: A Blueprint for the AI/ML Lifecycle with Kubeflow</title><link href="https://blog.kubeflow.org/fraud-detection-e2e/" rel="alternate" type="text/html" title="From Raw Data to Model Serving: A Blueprint for the AI/ML Lifecycle with Kubeflow" /><published>2025-07-15T00:00:00-05:00</published><updated>2025-07-15T00:00:00-05:00</updated><id>https://blog.kubeflow.org/fraud-detection-e2e</id><content type="html" xml:base="https://blog.kubeflow.org/fraud-detection-e2e/"><![CDATA[<p>Are you looking for a practical, reproducible way to take a machine learning project from raw data all the way to a deployed, production-ready model? This post is your blueprint for the AI/ML lifecycle: you‚Äôll learn how to use <a href="https://www.kubeflow.org">Kubeflow</a> and open source tools such as <a href="https://github.com/feast-dev/feast">Feast</a> to build a workflow you can run on your laptop and adapt to your own projects.</p>

<p>We‚Äôll walk through the entire ML lifecycle‚Äîfrom data preparation to live inference‚Äîleveraging the Kubeflow platform to create a cohesive, production-grade MLOps workflow.</p>

<h2 id="project-overview">Project Overview</h2>

<p>The project implements a complete MLOps workflow for a fraud detection use case. Fraud detection is a critical application in financial services, where organizations need to identify potentially fraudulent transactions in real-time while minimizing false positives that could disrupt legitimate customer activity.</p>

<p>Our fraud detection system leverages machine learning to analyze large volumes of transaction data, learn patterns from historical behavior, and flag suspicious transactions that deviate from normal patterns. The model considers various features such as transaction amounts, location data, merchant information, and user behavior patterns to make predictions. This makes fraud detection an ideal use case for demonstrating MLOps concepts because it requires:</p>

<ul>
  <li><strong>Real-time inference</strong>: Fraud detection decisions must be made instantly as transactions occur</li>
  <li><strong>Feature consistency</strong>: The same features used in training must be available during inference to ensure model accuracy</li>
  <li><strong>Scalability</strong>: The system must handle high transaction volumes</li>
  <li><strong>Continuous learning</strong>: Models need regular retraining as fraud patterns evolve</li>
  <li><strong>Compliance and auditability</strong>: Financial services require comprehensive model tracking and governance</li>
</ul>

<p>The workflow ingests raw transaction data, proceeds through data preparation and feature engineering, then model training and registration, and finally deploys the model as a production-ready inference service that can evaluate transactions in real-time.</p>

<p>The entire workflow is orchestrated as a Kubeflow Pipeline, which provides a powerful framework for defining, deploying, and managing complex machine learning pipelines on Kubernetes.</p>

<p>Here is a high-level overview of the pipeline:</p>

<p><img src="../images/2025-07-15-fraud-detection-e2e/pipeline.png" alt="pipeline.png" /></p>

<h2 id="a-note-on-the-data">A Note on the Data</h2>

<p>The pipeline assumes that the initial datasets (<code class="language-plaintext highlighter-rouge">train.csv</code>, <code class="language-plaintext highlighter-rouge">test.csv</code>, etc.) are already available. For readers who wish to follow along or generate their own sample data, a script is provided in the <code class="language-plaintext highlighter-rouge">synthetic_data_generation</code> directory. This script was used to create the initial data for this project but is not part of the automated Kubeflow pipeline itself.</p>

<h2 id="why-kubeflow">Why Kubeflow?</h2>

<p>This project demonstrates the power of using Kubeflow to abstract away the complexity of Kubernetes infrastructure, allowing AI Engineers, Data Scientists, and ML engineers to focus on what matters most: the data and model performance.</p>

<h3 id="key-benefits">Key Benefits</h3>

<p><strong>Infrastructure Abstraction</strong>: Instead of manually managing Kubernetes deployments, service accounts, networking, and storage configurations, the pipeline handles all the infrastructure complexity behind the scenes. You define your ML workflow as code, and Kubeflow takes care of orchestrating the execution across your Kubernetes cluster.</p>

<p><strong>Focus on AI, Not DevOps</strong>: With the infrastructure automated, you can spend your time on the activities that directly impact model performance:</p>

<ul>
  <li>Experimenting with different feature engineering approaches</li>
  <li>Tuning hyperparameters and model architectures</li>
  <li>Analyzing prediction results and model behavior</li>
  <li>Iterating on data preparation and validation strategies</li>
</ul>

<p><strong>Reproducible and Scalable</strong>: The pipeline ensures that every run follows the same steps with the same environment configurations, making your experiments reproducible. When you‚Äôre ready to scale up, the same pipeline can run on larger Kubernetes clusters without code changes.</p>

<p><strong>Production-Ready from Day One</strong>: By using production-grade tools like KServe for model serving, Feast for feature management, and the Model Registry for governance, your development pipeline is already structured for production deployment.</p>

<p><strong>Portable and Cloud-Agnostic</strong>: The entire workflow runs on standard Kubernetes, making it portable across different cloud providers or on-premises environments. What works on your laptop will work in production.</p>

<p>This approach shifts the cognitive load from infrastructure management to data science innovation, enabling faster experimentation and more reliable production deployments.</p>

<hr />

<h2 id="getting-started-prerequisites-and-cluster-setup">Getting Started: Prerequisites and Cluster Setup</h2>

<p>Before diving into the pipeline, you need to set up your local environment. This project is designed to run on a local Kubernetes cluster using <code class="language-plaintext highlighter-rouge">kind</code>.</p>

<h3 id="prerequisites">Prerequisites</h3>

<ul>
  <li>A container engine, like <a href="https://podman.io/">Podman</a> or <a href="https://www.docker.com/get-started">Docker</a>.</li>
  <li><a href="https://www.python.org/downloads/">Python</a> (3.11 or newer).</li>
  <li><a href="https://github.com/astral-sh/uv">uv</a>: A fast Python package installer.</li>
  <li><a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">kubectl</a></li>
  <li><a href="https://kind.sigs.k8s.io/docs/user/quick-start/#installation">kind</a></li>
  <li><a href="https://min.io/docs/minio/linux/reference/minio-mc.html">mc (MinIO Client)</a></li>
</ul>

<blockquote>
  <p><strong>Note:</strong> This setup was tested on a VM with 12GB RAM, 8 CPUs, and 150GB of disk space.</p>
</blockquote>

<h3 id="1-create-a-local-kubernetes-cluster">1. Create a Local Kubernetes Cluster</h3>

<p>First, create a <code class="language-plaintext highlighter-rouge">kind</code> cluster. The following command will set up a new cluster with a specific node image compatible with the required components:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kind create cluster <span class="nt">-n</span> fraud-detection-e2e-demo <span class="nt">--image</span> kindest/node:v1.31.6
</code></pre></div></div>

<h3 id="2-deploy-kubeflow-pipelines">2. Deploy Kubeflow Pipelines</h3>

<p>With your cluster running, the next step is to deploy Kubeflow Pipelines. For this project, the <a href="https://www.kubeflow.org/docs/components/pipelines/operator-guides/installation/#deploying-kubeflow-pipelines">standalone installation</a> is recommended, as it‚Äôs lighter and faster to set up than a full Kubeflow deployment.</p>

<p>Follow the official <a href="https://www.kubeflow.org/docs/components/pipelines/operator-guides/installation/#deploying-kubeflow-pipelines">Kubeflow Pipelines standalone installation guide</a> for the latest instructions.</p>

<h3 id="3-upload-the-raw-data-to-minio">3. Upload the Raw Data to MinIO</h3>

<p><a href="https://min.io/">MinIO</a> is an open source, S3-compatible object storage system. In this project, MinIO is used to store raw datasets, intermediate artifacts, and model files, making them accessible to all pipeline components running in Kubernetes.</p>

<p>Before uploading, you need to port-forward the MinIO service so it‚Äôs accessible locally. <strong>Run the following command in a separate terminal window:</strong></p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl port-forward <span class="nt">--namespace</span> kubeflow svc/minio-service 9000:9000
</code></pre></div></div>

<p>Next, generate the synthetic data and copy it to <code class="language-plaintext highlighter-rouge">feature_engineering/feature_repo/data/input/</code> if you haven‚Äôt done yet. The synthetic data generation script creates the <code class="language-plaintext highlighter-rouge">raw_transaction_datasource.csv</code> file that serves as the primary input for the pipeline.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>synthetic_data_generation
uv <span class="nb">sync
source</span> .venv/bin/activate
python synthetic_data_generation.py
<span class="nb">cp </span>raw_transaction_datasource.csv ../feature_engineering/feature_repo/data/input
<span class="nb">cd</span> ..
</code></pre></div></div>

<p>You should see an output similar to the following. The generation may take a few minutes depending on your hardware.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Using CPython 3.11.11
Creating virtual environment at: .venv
Resolved 7 packages <span class="k">in </span>14ms
Installed 6 packages <span class="k">in </span>84ms
 + <span class="nv">numpy</span><span class="o">==</span>2.3.0
 + <span class="nv">pandas</span><span class="o">==</span>2.3.0
 + python-dateutil<span class="o">==</span>2.9.0.post0
 + <span class="nv">pytz</span><span class="o">==</span>2025.2
 + <span class="nv">six</span><span class="o">==</span>1.17.0
 + <span class="nv">tzdata</span><span class="o">==</span>2025.2
loading data...
generating transaction level data...
        0 of 1,000,000 <span class="o">(</span>0%<span class="o">)</span> <span class="nb">complete
  </span>100,000 of 1,000,000 <span class="o">(</span>10%<span class="o">)</span> <span class="nb">complete
  </span>200,000 of 1,000,000 <span class="o">(</span>20%<span class="o">)</span> <span class="nb">complete
  </span>300,000 of 1,000,000 <span class="o">(</span>30%<span class="o">)</span> <span class="nb">complete
  </span>400,000 of 1,000,000 <span class="o">(</span>40%<span class="o">)</span> <span class="nb">complete
  </span>500,000 of 1,000,000 <span class="o">(</span>50%<span class="o">)</span> <span class="nb">complete
  </span>600,000 of 1,000,000 <span class="o">(</span>60%<span class="o">)</span> <span class="nb">complete
  </span>700,000 of 1,000,000 <span class="o">(</span>70%<span class="o">)</span> <span class="nb">complete
  </span>800,000 of 1,000,000 <span class="o">(</span>80%<span class="o">)</span> <span class="nb">complete
  </span>900,000 of 1,000,000 <span class="o">(</span>90%<span class="o">)</span> <span class="nb">complete</span>
</code></pre></div></div>

<p>Next, install and configure the <a href="https://min.io/docs/minio/linux/reference/minio-mc.html">MinIO Client (<code class="language-plaintext highlighter-rouge">mc</code>)</a> if you haven‚Äôt already. Then, set up the alias and upload the datasets:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mc <span class="nb">alias set </span>minio-local http://localhost:9000 minio minio123
mc mb minio-local/mlpipeline
mc <span class="nb">cp</span> <span class="nt">-r</span> feature_engineering/feature_repo/data/input/ minio-local/mlpipeline/artifacts/feature_repo/data/
mc <span class="nb">cp </span>feature_engineering/feature_repo/feature_store.yaml minio-local/mlpipeline/artifacts/feature_repo/
</code></pre></div></div>

<p>This will create the required bucket and directory structure in MinIO and upload your raw datasets, making them available for the pipeline.</p>

<blockquote>
  <p>Once the upload is complete, you can stop the port-forward process.</p>
</blockquote>

<h3 id="4-install-model-registry-kserve-spark-operator-and-set-policies">4. Install Model Registry, KServe, Spark Operator, and Set Policies</h3>

<p>While the datasets are uploading to MinIO, you can proceed to install the remaining Kubeflow components and set up the required Kubernetes policies. The following steps summarize what‚Äôs in <code class="language-plaintext highlighter-rouge">setup.sh</code>:</p>

<h4 id="install-model-registry">Install Model Registry</h4>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl apply <span class="nt">-k</span> <span class="s2">"https://github.com/kubeflow/model-registry/manifests/kustomize/overlays/db?ref=v0.2.16"</span>
</code></pre></div></div>

<h4 id="install-kserve">Install KServe</h4>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl create namespace kserve
kubectl config set-context <span class="nt">--current</span> <span class="nt">--namespace</span><span class="o">=</span>kserve
curl <span class="nt">-s</span> <span class="s2">"https://raw.githubusercontent.com/kserve/kserve/release-0.15/hack/quick_install.sh"</span> | bash
kubectl config set-context <span class="nt">--current</span> <span class="nt">--namespace</span><span class="o">=</span>kubeflow
</code></pre></div></div>

<h4 id="install-kubeflow-spark-operator">Install Kubeflow Spark Operator</h4>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>helm repo add <span class="nt">--force-update</span> spark-operator https://kubeflow.github.io/spark-operator
helm <span class="nb">install </span>spark-operator spark-operator/spark-operator <span class="se">\</span>
    <span class="nt">--namespace</span> spark-operator <span class="se">\</span>
    <span class="nt">--create-namespace</span>

<span class="c"># Make sure the Spark Operator is watching all namespaces:</span>
helm upgrade spark-operator spark-operator/spark-operator <span class="nt">--set</span> spark.jobNamespaces<span class="o">={}</span> <span class="nt">--namespace</span> spark-operator
</code></pre></div></div>

<h4 id="apply-service-accounts-roles-secrets-and-serving-runtime">Apply Service Accounts, Roles, Secrets, and Serving Runtime</h4>

<p>The <code class="language-plaintext highlighter-rouge">manifests/</code> directory contains several YAML files that set up the necessary service accounts, permissions, secrets, and runtime configuration for both KServe and Spark jobs. Here‚Äôs what each file does:</p>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">kserve-sa.yaml</code>: Creates a service account for KServe, referencing the MinIO secret.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">kserve-minio-secret.yaml</code>: Creates a secret with MinIO credentials and endpoint info, so KServe can access models and artifacts in MinIO.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">kserve-role.yaml</code>: Defines a ClusterRole allowing management of KServe InferenceService resources.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">kserve-role-binding.yaml</code>: Binds the above ClusterRole to the <code class="language-plaintext highlighter-rouge">pipeline-runner</code> service account in the <code class="language-plaintext highlighter-rouge">kubeflow</code> namespace, so pipeline steps can create/manage inference services.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">serving-runtime.yaml</code>: Registers a custom ServingRuntime for ONNX models, specifying the container image and runtime configuration for model serving.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">spark-sa.yaml</code>: Creates a service account for Spark jobs in the <code class="language-plaintext highlighter-rouge">kubeflow</code> namespace.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">spark-role.yaml</code>: Defines a Role granting Spark jobs permissions to manage pods, configmaps, services, secrets, PVCs, and SparkApplication resources in the namespace.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">spark-role-binding.yaml</code>: Binds the above Role to both the <code class="language-plaintext highlighter-rouge">spark</code> and <code class="language-plaintext highlighter-rouge">pipeline-runner</code> service accounts in the <code class="language-plaintext highlighter-rouge">kubeflow</code> namespace.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">kustomization.yaml</code>: A Kustomize manifest that groups all the above resources for easy application.</p>
  </li>
</ul>

<p>Apply all of these with:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl apply <span class="nt">-k</span> ./manifests <span class="nt">-n</span> kubeflow
</code></pre></div></div>

<blockquote>
  <p>These resources ensure that KServe and Spark jobs have the right permissions and configuration to run in your Kubeflow environment.</p>
</blockquote>

<h2 id="building-and-understanding-the-pipeline-images">Building and Understanding the Pipeline Images</h2>

<p>In Kubeflow Pipelines, each step of a pipeline runs inside a container. This containerized approach provides several key benefits: isolation between steps, reproducible environments, and the ability to use different runtime requirements for different stages of your pipeline.</p>

<p>While Kubeflow Pipelines provides default images for common tasks, most real-world ML projects require custom images tailored to their specific needs. Each pipeline component in this project uses a specialized container image that includes the necessary dependencies, libraries, and code to execute that particular step of the ML workflow.</p>

<p>This section covers how to build these custom images. For detailed information about what each image does and how the code inside each container works, refer to the individual pipeline step sections that follow.</p>

<blockquote>
  <p><strong>Note:</strong> You only need to build and push these images if you want to modify the code for any of the pipeline components. If you‚Äôre using the project as-is, you can use the prebuilt images referenced in the pipeline.</p>
</blockquote>

<p>The pipeline uses custom container images for the following components:</p>

<h3 id="image-locations">Image Locations</h3>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">data_preparation/Containerfile</code></p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">feature_engineering/Containerfile</code></p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">pipeline/Containerfile</code></p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">rest_predictor/Containerfile</code></p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">train/Containerfile</code></p>
  </li>
</ul>

<h3 id="how-to-build">How to Build</h3>

<p>You can build each image using Podman or Docker. For example, to build the data preparation image:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>data_preparation
podman build <span class="nt">-t</span> fraud-detection-e2e-demo-data-preparation:latest <span class="nb">.</span>
<span class="c"># or</span>
<span class="c"># docker build -t fraud-detection-e2e-demo-data-preparation:latest .</span>
</code></pre></div></div>

<p>You can also refer to the <code class="language-plaintext highlighter-rouge">build_images.sh</code> script in the project root to see how to build all images in sequence.</p>

<p>Repeat this process for each component, adjusting the tag and directory as needed.</p>

<h3 id="entry-points">Entry points</h3>

<ul>
  <li><strong>data_preparation:</strong> <code class="language-plaintext highlighter-rouge">python main.py</code></li>
  <li><strong>feature_engineering:</strong> <code class="language-plaintext highlighter-rouge">python feast_feature_engineering.py</code></li>
  <li><strong>pipeline:</strong> Used for orchestrating the pipeline steps (see <code class="language-plaintext highlighter-rouge">fraud-detection-e2e.py</code>)</li>
  <li><strong>rest_predictor:</strong> <code class="language-plaintext highlighter-rouge">python predictor.py</code></li>
  <li><strong>train:</strong> <code class="language-plaintext highlighter-rouge">python train.py</code></li>
</ul>

<h3 id="pushing-images">Pushing Images</h3>

<p>After building, push the images to a container registry accessible by your Kubernetes cluster. Update the image references in your pipeline as needed.</p>

<h2 id="the-kubeflow-pipeline">The Kubeflow Pipeline</h2>

<p>The main pipeline definition is in <code class="language-plaintext highlighter-rouge">pipeline/fraud-detection-e2e.py</code>. This file is the entrypoint for the Kubeflow pipeline and orchestrates all the steps described below.</p>

<p>With your environment and permissions set up, you‚Äôre ready to run the end-to-end pipeline. Let‚Äôs walk through each stage of the workflow and see how Kubeflow orchestrates the entire machine learning lifecycle‚Äîfrom data preparation to real-time inference.</p>

<h3 id="1-data-preparation-with-spark">1. Data Preparation with Spark</h3>

<p><a href="https://spark.apache.org/">Apache Spark</a> is a powerful open source engine for large-scale data processing and analytics. In this project, we use Spark to efficiently process and transform raw transaction data before it enters the ML pipeline.</p>

<p>To run Spark jobs on Kubernetes, we use the <a href="https://www.kubeflow.org/docs/components/spark-operator/">Kubeflow Spark Operator</a>. The Spark Operator makes it easy to submit and manage Spark applications as native Kubernetes resources, enabling scalable, distributed data processing as part of your MLOps workflow.</p>

<h4 id="container-image-for-data-preparation">Container Image for Data Preparation</h4>

<p>This pipeline step uses a custom container image built from <code class="language-plaintext highlighter-rouge">data_preparation/Containerfile</code>. The image includes:</p>

<ul>
  <li><strong>PySpark and dependencies:</strong> Required libraries for distributed data processing</li>
  <li><strong>MinIO client libraries:</strong> For reading from and writing to object storage</li>
  <li><strong>Custom data processing code:</strong> The <code class="language-plaintext highlighter-rouge">main.py</code> script that implements the data transformation logic</li>
</ul>

<p>The container runs with the entry point <code class="language-plaintext highlighter-rouge">python main.py</code>, which orchestrates all the data preparation tasks within the Spark job.</p>

<p>The pipeline begins by launching a Spark job that performs several key data preparation steps, implemented in <code class="language-plaintext highlighter-rouge">data_preparation/main.py</code>:</p>

<h4 id="combining-datasets">Combining Datasets</h4>

<p>The job reads the raw <code class="language-plaintext highlighter-rouge">train.csv</code>, <code class="language-plaintext highlighter-rouge">test.csv</code>, and <code class="language-plaintext highlighter-rouge">validate.csv</code> datasets, adds a <code class="language-plaintext highlighter-rouge">set</code> column to each, and combines them:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_set</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">csv</span><span class="p">(</span><span class="n">INPUT_DIR</span> <span class="o">+</span> <span class="s">"train.csv"</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">inferSchema</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">test_set</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">csv</span><span class="p">(</span><span class="n">INPUT_DIR</span> <span class="o">+</span> <span class="s">"test.csv"</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">inferSchema</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">validate_set</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">csv</span><span class="p">(</span><span class="n">INPUT_DIR</span> <span class="o">+</span> <span class="s">"validate.csv"</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">inferSchema</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">train_set</span> <span class="o">=</span> <span class="n">train_set</span><span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">"set"</span><span class="p">,</span> <span class="n">lit</span><span class="p">(</span><span class="s">"train"</span><span class="p">))</span>
<span class="n">test_set</span> <span class="o">=</span> <span class="n">test_set</span><span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">"set"</span><span class="p">,</span> <span class="n">lit</span><span class="p">(</span><span class="s">"test"</span><span class="p">))</span>
<span class="n">validate_set</span> <span class="o">=</span> <span class="n">validate_set</span><span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">"set"</span><span class="p">,</span> <span class="n">lit</span><span class="p">(</span><span class="s">"valid"</span><span class="p">))</span>

<span class="n">all_sets</span> <span class="o">=</span> <span class="n">train_set</span><span class="p">.</span><span class="n">unionByName</span><span class="p">(</span><span class="n">test_set</span><span class="p">).</span><span class="n">unionByName</span><span class="p">(</span><span class="n">validate_set</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="type-conversion-and-feature-engineering">Type Conversion and Feature Engineering</h4>

<p>It converts certain columns to boolean types and generates unique IDs:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">all_sets</span> <span class="o">=</span> <span class="n">all_sets</span><span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">"fraud"</span><span class="p">,</span> <span class="n">col</span><span class="p">(</span><span class="s">"fraud"</span><span class="p">)</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="n">all_sets</span> <span class="o">=</span> <span class="n">all_sets</span><span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">"repeat_retailer"</span><span class="p">,</span> <span class="n">col</span><span class="p">(</span><span class="s">"repeat_retailer"</span><span class="p">)</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="n">all_sets</span> <span class="o">=</span> <span class="n">all_sets</span><span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">"used_chip"</span><span class="p">,</span> <span class="n">col</span><span class="p">(</span><span class="s">"used_chip"</span><span class="p">)</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="n">all_sets</span> <span class="o">=</span> <span class="n">all_sets</span><span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">"used_pin_number"</span><span class="p">,</span> <span class="n">col</span><span class="p">(</span><span class="s">"used_pin_number"</span><span class="p">)</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="n">all_sets</span> <span class="o">=</span> <span class="n">all_sets</span><span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">"online_order"</span><span class="p">,</span> <span class="n">col</span><span class="p">(</span><span class="s">"online_order"</span><span class="p">)</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">)</span>

<span class="n">w</span> <span class="o">=</span> <span class="n">Window</span><span class="p">.</span><span class="n">orderBy</span><span class="p">(</span><span class="n">lit</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="n">all_sets</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">all_sets</span>
    <span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">"idx"</span><span class="p">,</span> <span class="n">row_number</span><span class="p">().</span><span class="n">over</span><span class="p">(</span><span class="n">w</span><span class="p">))</span>
    <span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">"user_id"</span><span class="p">,</span> <span class="n">concat</span><span class="p">(</span><span class="n">lit</span><span class="p">(</span><span class="s">"user_"</span><span class="p">),</span> <span class="n">col</span><span class="p">(</span><span class="s">"idx"</span><span class="p">)</span> <span class="o">-</span> <span class="n">lit</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span>
    <span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">"transaction_id"</span><span class="p">,</span> <span class="n">concat</span><span class="p">(</span><span class="n">lit</span><span class="p">(</span><span class="s">"txn_"</span><span class="p">),</span> <span class="n">col</span><span class="p">(</span><span class="s">"idx"</span><span class="p">)</span> <span class="o">-</span> <span class="n">lit</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span>
    <span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="s">"idx"</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div></div>

<h4 id="timestamping">Timestamping</h4>

<p>The job adds <code class="language-plaintext highlighter-rouge">created</code> and <code class="language-plaintext highlighter-rouge">updated</code> timestamp columns:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">date_col</span> <span class="ow">in</span> <span class="p">[</span><span class="s">"created"</span><span class="p">,</span> <span class="s">"updated"</span><span class="p">]:</span>
    <span class="n">all_sets</span> <span class="o">=</span> <span class="n">all_sets</span><span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="n">date_col</span><span class="p">,</span> <span class="n">current_timestamp</span><span class="p">())</span>
</code></pre></div></div>

<h4 id="point-in-time-feature-calculation">Point-in-Time Feature Calculation</h4>

<p>Using the raw transaction history, the Spark job calculates features such as the number of previous transactions, average/max/stddev of previous transaction amounts, and days since the last/first transaction:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calculate_point_in_time_features</span><span class="p">(</span><span class="n">label_dataset</span><span class="p">:</span> <span class="n">DataFrame</span><span class="p">,</span> <span class="n">transactions_df</span><span class="p">:</span> <span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataFrame</span><span class="p">:</span>
    <span class="c1"># ... (see full code in data_preparation/main.py)
</span>    <span class="c1"># Aggregates and joins features for each user at each point in time
</span></code></pre></div></div>

<h4 id="output">Output</h4>

<p>The final processed data is saved as both a CSV (for entity definitions) and a Parquet file (for feature storage) in MinIO:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">entity_df</span><span class="p">.</span><span class="n">write</span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">"header"</span><span class="p">,</span> <span class="bp">True</span><span class="p">).</span><span class="n">mode</span><span class="p">(</span><span class="s">"overwrite"</span><span class="p">).</span><span class="n">csv</span><span class="p">(</span><span class="n">entity_file_name</span><span class="p">)</span>
<span class="n">df</span><span class="p">.</span><span class="n">write</span><span class="p">.</span><span class="n">mode</span><span class="p">(</span><span class="s">"overwrite"</span><span class="p">).</span><span class="n">parquet</span><span class="p">(</span><span class="n">parquet_file_name</span><span class="p">)</span>
</code></pre></div></div>

<p>All of this logic is orchestrated by the <code class="language-plaintext highlighter-rouge">prepare_data</code> component in the pipeline, which launches the Spark job on Kubernetes.</p>

<h3 id="2-feature-engineering-with-feast">2. Feature Engineering with Feast</h3>

<p><a href="https://feast.dev/">Feast</a> is an open source feature store that lets you manage and serve features for both training and inference, ensuring consistency and reducing the risk of training/serving skew. In machine learning, a ‚Äúfeature‚Äù is an individual measurable property or characteristic of the data being analyzed‚Äîin our fraud detection case, features include transaction amounts, distances from previous transactions, merchant types, and user behavior patterns that help the model distinguish between legitimate and fraudulent activity.</p>

<h4 id="container-image-for-feature-engineering">Container Image for Feature Engineering</h4>

<p>This pipeline step uses a custom container image built from <code class="language-plaintext highlighter-rouge">feature_engineering/Containerfile</code>. The image includes:</p>

<ul>
  <li><strong>Feast feature store:</strong> The complete Feast installation for feature management</li>
  <li><strong>Python dependencies:</strong> Required libraries for feature processing and materialization</li>
  <li><strong>Feature repository definition:</strong> The <code class="language-plaintext highlighter-rouge">repo_definition.py</code> file that defines the feature views and entities</li>
  <li><strong>MinIO client libraries:</strong> For uploading the materialized features and online store to object storage</li>
</ul>

<p>The container runs with the entry point <code class="language-plaintext highlighter-rouge">python feast_feature_engineering.py</code>, which handles the Feast operations including applying feature definitions, materializing features, and uploading the results to MinIO.</p>

<p>After data preparation, the pipeline uses Feast to register, materialize, and store features for downstream steps. This process starts with defining the features you want to use. For example, in <code class="language-plaintext highlighter-rouge">feature_repo/repo_definition.py</code>, you‚Äôll find a <code class="language-plaintext highlighter-rouge">FeatureView</code> that lists features like <code class="language-plaintext highlighter-rouge">distance_from_home</code> and <code class="language-plaintext highlighter-rouge">ratio_to_median_purchase_price</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">transactions_fv</span> <span class="o">=</span> <span class="n">FeatureView</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s">"transactions"</span><span class="p">,</span>
    <span class="n">entities</span><span class="o">=</span><span class="p">[</span><span class="n">transaction</span><span class="p">],</span>
    <span class="n">schema</span><span class="o">=</span><span class="p">[</span>
        <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">"user_id"</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">feast</span><span class="p">.</span><span class="n">types</span><span class="p">.</span><span class="n">String</span><span class="p">),</span>
        <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">"distance_from_home"</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">feast</span><span class="p">.</span><span class="n">types</span><span class="p">.</span><span class="n">Float32</span><span class="p">),</span>
        <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">"ratio_to_median_purchase_price"</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">feast</span><span class="p">.</span><span class="n">types</span><span class="p">.</span><span class="n">Float32</span><span class="p">),</span>
        <span class="c1"># ... other features
</span>    <span class="p">],</span>
    <span class="n">online</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">source</span><span class="o">=</span><span class="n">transaction_source</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>

<p>Once the features are defined, the pipeline runs two key Feast commands. First, it applies the feature definitions to the store:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">subprocess</span><span class="p">.</span><span class="n">run</span><span class="p">([</span><span class="s">"feast"</span><span class="p">,</span> <span class="s">"apply"</span><span class="p">],</span> <span class="n">cwd</span><span class="o">=</span><span class="n">feature_repo_path</span><span class="p">,</span> <span class="n">check</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>Then, it materializes the computed features from the Parquet file into Feast‚Äôs online store, making them available for real-time inference:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">subprocess</span><span class="p">.</span><span class="n">run</span><span class="p">([</span><span class="s">"feast"</span><span class="p">,</span> <span class="s">"materialize"</span><span class="p">,</span> <span class="n">start_date</span><span class="p">,</span> <span class="n">end_date</span><span class="p">],</span> <span class="n">cwd</span><span class="o">=</span><span class="n">feature_repo_path</span><span class="p">,</span> <span class="n">check</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>Finally, the resulting feature data and the online store database are uploaded to MinIO, so they‚Äôre accessible to the rest of the pipeline:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">client</span><span class="p">.</span><span class="n">fput_object</span><span class="p">(</span><span class="n">MINIO_BUCKET</span><span class="p">,</span> <span class="n">object_path</span><span class="p">,</span> <span class="n">local_file_path</span><span class="p">)</span>
</code></pre></div></div>

<p>By using Feast in this way, you ensure that the same features are available for both model training and real-time predictions, making your ML workflow robust and reproducible.</p>

<h3 id="3-model-training">3. Model Training</h3>

<p>With the features materialized in Feast, the next step is to train the fraud detection model. The pipeline‚Äôs <code class="language-plaintext highlighter-rouge">train_model</code> component retrieves the processed features and prepares them for training. The features used include behavioral and transaction-based signals such as <code class="language-plaintext highlighter-rouge">distance_from_last_transaction</code>, <code class="language-plaintext highlighter-rouge">ratio_to_median_purchase_price</code>, <code class="language-plaintext highlighter-rouge">used_chip</code>, <code class="language-plaintext highlighter-rouge">used_pin_number</code>, and <code class="language-plaintext highlighter-rouge">online_order</code>.</p>

<h4 id="container-image-for-model-training">Container Image for Model Training</h4>

<p>This pipeline step uses a custom container image built from <code class="language-plaintext highlighter-rouge">train/Containerfile</code>. The image includes:</p>

<ul>
  <li><strong>Machine learning libraries:</strong> TensorFlow/Keras for neural network training, scikit-learn for data preprocessing</li>
  <li><strong>ONNX Runtime:</strong> For converting and exporting the trained model to ONNX format</li>
  <li><strong>PySpark:</strong> For loading and processing the feature data from Parquet files</li>
  <li><strong>MinIO client libraries:</strong> For downloading features and uploading the trained model artifacts</li>
</ul>

<p>The container runs with the entry point <code class="language-plaintext highlighter-rouge">python train.py</code>.</p>

<p>The training script loads the features, splits the data into train, validation, and test sets, and scales the input features for better model performance:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_features</span> <span class="o">=</span> <span class="n">features</span><span class="p">.</span><span class="nb">filter</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="s">"set"</span><span class="p">]</span> <span class="o">==</span> <span class="s">"train"</span><span class="p">)</span>
<span class="n">validate_features</span> <span class="o">=</span> <span class="n">features</span><span class="p">.</span><span class="nb">filter</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="s">"set"</span><span class="p">]</span> <span class="o">==</span> <span class="s">"valid"</span><span class="p">)</span>
<span class="n">test_features</span> <span class="o">=</span> <span class="n">features</span><span class="p">.</span><span class="nb">filter</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="s">"set"</span><span class="p">]</span> <span class="o">==</span> <span class="s">"test"</span><span class="p">)</span>
<span class="c1"># ... select and scale features ...
</span></code></pre></div></div>

<p>It then builds and trains a neural network model using Keras, handling class imbalance and exporting the trained model in ONNX format for portable, high-performance inference:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">(</span><span class="n">feature_indexes</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">),</span> <span class="n">class_weight</span><span class="o">=</span><span class="n">class_weights</span><span class="p">)</span>
<span class="n">save_model</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">model_path</span><span class="p">)</span>  <span class="c1"># Exports to ONNX
</span></code></pre></div></div>

<p>By structuring the training step this way, the pipeline ensures that the model is trained on the same features that will be available at inference time, supporting a robust and reproducible MLOps workflow.</p>

<h3 id="4-model-registration">4. Model Registration</h3>

<p>Once the model is trained, it‚Äôs important to track, version, and manage it before deploying to production. This is where the <a href="https://www.kubeflow.org/docs/components/model-registry/">Kubeflow Model Registry</a> comes in. The Model Registry acts as a centralized service for managing machine learning models and their metadata, making it easier to manage deployments, rollbacks, and audits.</p>

<h4 id="container-image-for-model-registration">Container Image for Model Registration</h4>

<p>This pipeline step uses a custom container image built from <code class="language-plaintext highlighter-rouge">pipeline/Containerfile</code>. The image includes:</p>

<ul>
  <li><strong>Kubeflow Pipelines SDK:</strong> For pipeline orchestration and component definitions</li>
  <li><strong>Model Registry client:</strong> Python libraries for interacting with the Kubeflow Model Registry</li>
  <li><strong>Pipeline orchestration code:</strong> The core pipeline definition and component functions</li>
</ul>

<p>The container is used as the base image for the <code class="language-plaintext highlighter-rouge">register_model</code> component, which executes the model registration logic inline within the pipeline definition. This approach allows the registration step to run lightweight operations without requiring a separate, specialized container image.</p>

<p>In the pipeline, the <code class="language-plaintext highlighter-rouge">register_model</code> component takes the trained model artifact and registers it in the Model Registry. This process includes:</p>

<ul>
  <li><strong>Assigning a unique name and version:</strong> The model is registered with a name (e.g., <code class="language-plaintext highlighter-rouge">"fraud-detection"</code>) and a version, which is typically tied to the pipeline run ID for traceability.</li>
  <li><strong>Storing metadata:</strong> Along with the model artifact, metadata such as the model format, storage location, and additional tags or descriptions can be stored for governance and reproducibility.</li>
  <li><strong>Making the model discoverable:</strong> Registered models can be easily found and referenced for deployment, monitoring, or rollback.</li>
</ul>

<p>Here‚Äôs how the registration step is implemented in the pipeline:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">dsl</span><span class="p">.</span><span class="n">component</span><span class="p">(</span><span class="n">base_image</span><span class="o">=</span><span class="n">PIPELINE_IMAGE</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">register_model</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">Input</span><span class="p">[</span><span class="n">Model</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">NamedTuple</span><span class="p">(</span><span class="s">'outputs'</span><span class="p">,</span> <span class="n">model_name</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">model_version</span><span class="o">=</span><span class="nb">str</span><span class="p">):</span>
    <span class="kn">from</span> <span class="nn">model_registry</span> <span class="kn">import</span> <span class="n">ModelRegistry</span>

    <span class="n">registry</span> <span class="o">=</span> <span class="n">ModelRegistry</span><span class="p">(</span>
        <span class="n">server_address</span><span class="o">=</span><span class="s">"http://model-registry-service.kubeflow.svc.cluster.local"</span><span class="p">,</span>
        <span class="n">port</span><span class="o">=</span><span class="mi">8080</span><span class="p">,</span>
        <span class="n">author</span><span class="o">=</span><span class="s">"fraud-detection-e2e-pipeline"</span><span class="p">,</span>
        <span class="n">user_token</span><span class="o">=</span><span class="s">"non-used"</span><span class="p">,</span>
        <span class="n">is_secure</span><span class="o">=</span><span class="bp">False</span>
    <span class="p">)</span>

    <span class="n">model_name</span> <span class="o">=</span> <span class="s">"fraud-detection"</span>
    <span class="n">model_version</span> <span class="o">=</span> <span class="s">""</span>

    <span class="n">registry</span><span class="p">.</span><span class="n">register_model</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
        <span class="n">uri</span><span class="o">=</span><span class="n">model</span><span class="p">.</span><span class="n">uri</span><span class="p">,</span>
        <span class="n">version</span><span class="o">=</span><span class="n">model_version</span><span class="p">,</span>
        <span class="n">model_format_name</span><span class="o">=</span><span class="s">"onnx"</span><span class="p">,</span>
        <span class="n">model_source_class</span><span class="o">=</span><span class="s">"pipelinerun"</span><span class="p">,</span>
        <span class="n">model_source_group</span><span class="o">=</span><span class="s">"fraud-detection"</span><span class="p">,</span>
        <span class="n">model_source_id</span><span class="o">=</span><span class="s">""</span><span class="p">,</span>
        <span class="n">model_source_kind</span><span class="o">=</span><span class="s">"kfp"</span><span class="p">,</span>
        <span class="n">model_source_name</span><span class="o">=</span><span class="s">"fraud-detection-e2e-pipeline"</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">model_version</span><span class="p">)</span>
</code></pre></div></div>

<p>By registering the model in this way, you ensure that every model deployed for inference is discoverable, reproducible, and governed‚Äîan essential part of any production-grade MLOps workflow.</p>

<h3 id="5-real-time-inference-with-kserve">5. Real-Time Inference with KServe</h3>

<p>The final stage of the pipeline is deploying the registered model as a real-time inference service using KServe. <a href="https://kserve.github.io/website/">KServe</a> is an open source model serving platform for Kubernetes that standardizes how you deploy, scale, and manage machine learning models in production.</p>

<h4 id="container-image-for-real-time-inference">Container Image for Real-Time Inference</h4>

<p>This pipeline step uses a custom container image built from <code class="language-plaintext highlighter-rouge">rest_predictor/Containerfile</code>. The image includes:</p>

<ul>
  <li><strong>KServe Python SDK:</strong> For building custom model serving endpoints</li>
  <li><strong>ONNX Runtime:</strong> For running the trained model in ONNX format</li>
  <li><strong>Feast feature store client:</strong> For retrieving real-time features during inference</li>
  <li><strong>Model Registry client:</strong> For downloading the registered model artifacts</li>
  <li><strong>Custom predictor code:</strong> The <code class="language-plaintext highlighter-rouge">predictor.py</code> script that implements the inference logic</li>
</ul>

<p>The container runs with the entry point <code class="language-plaintext highlighter-rouge">python predictor.py</code>.</p>

<p>The pipeline‚Äôs <code class="language-plaintext highlighter-rouge">serve</code> component creates a KServe InferenceService using this custom Python predictor.</p>

<p>This is done by creating a Kubernetes custom resource (CR) of kind <code class="language-plaintext highlighter-rouge">InferenceService</code>, which tells KServe how to deploy and manage the model server. The resource specifies the container image, command, arguments, and service account to use for serving the model.</p>

<p>Here‚Äôs how the InferenceService is defined and created in the pipeline:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">inference_service</span> <span class="o">=</span> <span class="n">kserve</span><span class="p">.</span><span class="n">V1beta1InferenceService</span><span class="p">(</span>
    <span class="n">api_version</span><span class="o">=</span><span class="n">kserve</span><span class="p">.</span><span class="n">constants</span><span class="p">.</span><span class="n">KSERVE_GROUP</span> <span class="o">+</span> <span class="s">"/v1beta1"</span><span class="p">,</span>
    <span class="n">kind</span><span class="o">=</span><span class="s">"InferenceService"</span><span class="p">,</span>
    <span class="n">metadata</span><span class="o">=</span><span class="n">client</span><span class="p">.</span><span class="n">V1ObjectMeta</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">model_name</span> <span class="o">+</span> <span class="s">"-"</span> <span class="o">+</span> <span class="n">job_id</span><span class="p">,</span>
        <span class="n">namespace</span><span class="o">=</span><span class="n">kserve</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">get_default_target_namespace</span><span class="p">(),</span>
        <span class="n">labels</span><span class="o">=</span><span class="p">{</span>
            <span class="s">"modelregistry/registered-model-id"</span><span class="p">:</span> <span class="n">model</span><span class="p">.</span><span class="nb">id</span><span class="p">,</span>
            <span class="s">"modelregistry/model-version-id"</span><span class="p">:</span> <span class="n">model_version</span><span class="p">.</span><span class="nb">id</span>
        <span class="p">},</span>
    <span class="p">),</span>
    <span class="n">spec</span><span class="o">=</span><span class="n">kserve</span><span class="p">.</span><span class="n">V1beta1InferenceServiceSpec</span><span class="p">(</span>
        <span class="n">predictor</span><span class="o">=</span><span class="n">kserve</span><span class="p">.</span><span class="n">V1beta1PredictorSpec</span><span class="p">(</span>
            <span class="n">service_account_name</span><span class="o">=</span><span class="s">"kserve-sa"</span><span class="p">,</span>
            <span class="n">containers</span><span class="o">=</span><span class="p">[</span>
                <span class="n">V1Container</span><span class="p">(</span>
                    <span class="n">name</span><span class="o">=</span><span class="s">"inference-container"</span><span class="p">,</span>
                    <span class="n">image</span><span class="o">=</span><span class="n">rest_predictor_image</span><span class="p">,</span>
                    <span class="n">command</span><span class="o">=</span><span class="p">[</span><span class="s">"python"</span><span class="p">,</span> <span class="s">"predictor.py"</span><span class="p">],</span>
                    <span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="s">"--model-name"</span><span class="p">,</span> <span class="n">model_name</span><span class="p">,</span> <span class="s">"--model-version"</span><span class="p">,</span> <span class="n">model_version_name</span><span class="p">]</span>
                <span class="p">)</span>
            <span class="p">]</span>
        <span class="p">)</span>
    <span class="p">),</span>
<span class="p">)</span>
<span class="n">ks_client</span> <span class="o">=</span> <span class="n">kserve</span><span class="p">.</span><span class="n">KServeClient</span><span class="p">()</span>
<span class="n">ks_client</span><span class="p">.</span><span class="n">create</span><span class="p">(</span><span class="n">inference_service</span><span class="p">)</span>
</code></pre></div></div>

<p>The custom predictor does more than just run the model: it also integrates directly with the Feast online feature store. When a prediction request arrives with a <code class="language-plaintext highlighter-rouge">user_id</code>, the predictor first fetches the user‚Äôs latest features from Feast and then feeds them to the ONNX model for inference. Here‚Äôs a simplified view of the predictor‚Äôs logic:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ONNXModel</span><span class="p">(</span><span class="n">kserve</span><span class="p">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># ... download model and initialize Feast feature store ...
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">feature_store</span> <span class="o">=</span> <span class="n">FeatureStore</span><span class="p">(</span><span class="n">repo_path</span><span class="o">=</span><span class="n">feature_repo_path</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">ort</span><span class="p">.</span><span class="n">InferenceSession</span><span class="p">(</span><span class="s">"/app/model"</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ready</span> <span class="o">=</span> <span class="bp">True</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">payload</span><span class="p">:</span> <span class="n">Dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
        <span class="n">user_id</span> <span class="o">=</span> <span class="n">payload</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"user_id"</span><span class="p">)</span>
        <span class="n">feature_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">feature_store</span><span class="p">.</span><span class="n">get_online_features</span><span class="p">(</span>
            <span class="n">entity_rows</span><span class="o">=</span><span class="p">[{</span><span class="s">"user_id"</span><span class="p">:</span> <span class="n">user_id</span><span class="p">}],</span>
            <span class="n">features</span><span class="o">=</span><span class="n">features_to_request</span><span class="p">,</span>
        <span class="p">).</span><span class="n">to_dict</span><span class="p">()</span>
        <span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span>
            <span class="p">[</span>
                <span class="n">feature_dict</span><span class="p">[</span><span class="s">"distance_from_last_transaction"</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">feature_dict</span><span class="p">[</span><span class="s">"ratio_to_median_purchase_price"</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">feature_dict</span><span class="p">[</span><span class="s">"used_chip"</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">feature_dict</span><span class="p">[</span><span class="s">"used_pin_number"</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">feature_dict</span><span class="p">[</span><span class="s">"online_order"</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
            <span class="p">]</span>
        <span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="p">{</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">get_inputs</span><span class="p">()[</span><span class="mi">0</span><span class="p">].</span><span class="n">name</span><span class="p">:</span> <span class="n">input_data</span><span class="p">})</span>
        <span class="k">return</span> <span class="p">{</span><span class="s">"user_id"</span><span class="p">:</span> <span class="n">user_id</span><span class="p">,</span> <span class="s">"prediction"</span><span class="p">:</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">tolist</span><span class="p">()}</span>
</code></pre></div></div>

<blockquote>
  <p><strong>Note:</strong><br />
By default, KServe supports several model serving runtimes, including <a href="https://github.com/triton-inference-server/server">Triton Inference Server</a> (often used via the <code class="language-plaintext highlighter-rouge">kserve-tritonserver</code> runtime). However, the official Triton server does not support macOS/arm64, which is why this project uses a custom Python predictor for local development and demonstration.<br />
If you are running on a supported platform (such as x86_64 Linux), you may want to use the <code class="language-plaintext highlighter-rouge">kserve-tritonserver</code> runtime for production workloads, as it offers high performance and native ONNX support.<br />
If you want to use Feast for online feature retrieval at inference time, a custom Python predictor (like the one in this repo) is the most straightforward approach. If you use the standard <code class="language-plaintext highlighter-rouge">kserve-tritonserver</code> runtime, you would need to implement feature fetching as a <a href="https://github.com/triton-inference-server/python_backend">Triton Python backend</a> or as a pre-processing step outside of Triton, since Triton itself does not natively integrate with Feast.</p>
</blockquote>

<p>By structuring the inference step this way, the pipeline ensures that the deployed model always uses the freshest features for each prediction, supporting robust, real-time fraud detection.</p>

<h2 id="importing-and-running-the-pipeline">Importing and Running the Pipeline</h2>

<p>Once your environment is set up and the data is uploaded, you‚Äôre ready to run the pipeline.</p>

<h3 id="import-the-pipeline">Import the Pipeline</h3>

<ol>
  <li>Open the Kubeflow Pipelines UI (usually at <a href="http://localhost:8080">http://localhost:8080</a> if you used the default port-forward).</li>
  <li>Click <strong>Pipelines</strong> in the sidebar, then click <strong>Upload pipeline</strong>.</li>
  <li>Upload the compiled pipeline YAML file (e.g., <code class="language-plaintext highlighter-rouge">pipeline/fraud-detection-e2e.yaml</code>).</li>
</ol>

<h3 id="run-the-pipeline">Run the Pipeline</h3>

<ol>
  <li>After uploading, click on your pipeline in the list.</li>
  <li>Click <strong>Create run</strong>.</li>
  <li>Optionally customize the run name and description (the defaults work fine), then click <strong>Start</strong>.</li>
</ol>

<p>You can monitor the progress and view logs for each step directly in the UI.</p>

<h2 id="testing-the-live-endpoint">Testing the Live Endpoint</h2>

<p>With the inference service running, you can now interact with your deployed model in real time. Let‚Äôs see how to send prediction requests and interpret the results.</p>

<p>Before sending requests, port-forward the inference pod so the service is accessible locally. <strong>Run this command in a separate terminal window:</strong></p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl <span class="nt">-n</span> kubeflow get pods <span class="nt">-l</span> <span class="nv">component</span><span class="o">=</span>predictor <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[*].metadata.name}"</span> | <span class="nb">tr</span> <span class="s1">' '</span> <span class="s1">'\n'</span> | <span class="nb">grep</span> <span class="s1">'^fraud-detection'</span> | <span class="nb">head</span> <span class="nt">-n1</span> | xargs <span class="nt">-I</span> <span class="o">{}</span> kubectl port-forward <span class="nt">-n</span> kubeflow pod/<span class="o">{}</span> 8081:8080
</code></pre></div></div>

<p>With the port-forward active, you can now send a request to the model:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-X</span> POST http://localhost:8081/v1/models/onnx-model:predict <span class="se">\</span>
<span class="nt">-H</span> <span class="s2">"Content-Type: application/json"</span> <span class="se">\</span>
<span class="nt">-d</span> <span class="s1">'{"user_id": "user_0"}'</span>
</code></pre></div></div>

<p>The service retrieves features for <code class="language-plaintext highlighter-rouge">user_0</code>, runs a prediction, and returns the fraud probability.</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="nl">"user_id"</span><span class="p">:</span><span class="s2">"user_0"</span><span class="p">,</span><span class="nl">"prediction"</span><span class="p">:[[</span><span class="mf">0.8173668384552002</span><span class="p">]]}</span><span class="w">
</span></code></pre></div></div>

<blockquote>
  <p>Note: The result of the prediction may vary depending on the initial raw data you uploaded.<br />
Try sending requests with a few different <code class="language-plaintext highlighter-rouge">user_id</code> values (e.g., <code class="language-plaintext highlighter-rouge">"user_1"</code>, <code class="language-plaintext highlighter-rouge">"user_2"</code>, etc.) to see how the predictions change.</p>
</blockquote>

<h2 id="conclusion">Conclusion</h2>

<p>This post has walked you through a complete, reproducible AI/ML workflow‚Äîfrom raw data to a live model serving endpoint‚Äîusing Kubeflow and open source tools. Along the way, you‚Äôve seen how to prepare data with Spark, manage features with Feast, train and register models, and deploy real-time inference services with KServe, all orchestrated in a portable pipeline you can run on your own laptop.</p>

<p>By following this blueprint, you can adapt and extend the process for your own machine learning projects, whether you‚Äôre working locally or scaling up to production. Kubeflow‚Äôs modular platform and ecosystem make it possible to manage the entire ML lifecycle in a consistent, automated, and open way.</p>

<p>Ready to try it yourself? The complete source code for this project is available on <a href="https://github.com/hbelmiro/fraud_detection_e2e_demo/tree/kubeflow">GitHub</a>.</p>]]></content><author><name>Helber Belmiro</name></author><category term="mlops" /><category term="pipelines" /><category term="spark" /><category term="feast" /><category term="model-registry" /><category term="kserve" /><summary type="html"><![CDATA[Are you looking for a practical, reproducible way to take a machine learning project from raw data all the way to a deployed, production-ready model? This post is your blueprint for the AI/ML lifecycle: you‚Äôll learn how to use Kubeflow and open source tools such as Feast to build a workflow you can run on your laptop and adapt to your own projects.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.kubeflow.org/images/logo.png" /><media:content medium="image" url="https://blog.kubeflow.org/images/logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Kubeflow 1.10 Release Announcement</title><link href="https://blog.kubeflow.org/kubeflow-1.10-release/" rel="alternate" type="text/html" title="Kubeflow 1.10 Release Announcement" /><published>2025-03-26T00:00:00-05:00</published><updated>2025-03-26T00:00:00-05:00</updated><id>https://blog.kubeflow.org/kubeflow-1.10-release</id><content type="html" xml:base="https://blog.kubeflow.org/kubeflow-1.10-release/"><![CDATA[<p>Kubeflow 1.10.0 delivers essential updates that enhance the flexibility, efficiency, and scalability of machine learning
workflows. The new features span across several components, improving both user experience and system performance.</p>

<h2 id="highlight-features">Highlight features</h2>

<ul>
  <li>Trainer 2.0</li>
  <li>New UI for Model Registry</li>
  <li>Spark Operator as a core Kubeflow component</li>
  <li>Kubernetes and container security (CISO compatibility)</li>
  <li>Hyperparameter Optimization for LLMs Fine-Tuning</li>
  <li>Loop parallelism in Pipelines</li>
  <li>New parameter distributions for Katib</li>
  <li>Deeper Model Registry integrations with KServe</li>
  <li>New Python SDK, OCI storage, and model caching for KServe</li>
  <li>New security contexts and rootless Istio-CNI integrations for Spark Operator</li>
</ul>

<h2 id="kubeflow-platform-manifests--security">Kubeflow Platform (Manifests &amp; Security)</h2>

<p>The Kubeflow Platform Working Group focuses on simplifying Kubeflow installation, operations, and security. See details below.</p>

<h3 id="manifests">Manifests:</h3>

<ul>
  <li>Spark Operator 2.1.0 included in Kubeflow platform, although not installed yet by default</li>
  <li><a href="https://github.com/kubeflow/manifests/blob/master/README.md">Documentation updates</a> that make it easier to install,
extend and upgrade Kubeflow</li>
  <li>For more details and future plans please consult the <a href="https://github.com/kubeflow/manifests/issues/2763">1.10.0</a> and
<a href="https://github.com/kubeflow/manifests/issues/3038">1.10.1/1.11.0</a> milestones</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Notebooks</th>
      <th style="text-align: center">Dashboard</th>
      <th style="text-align: center">Pipelines</th>
      <th style="text-align: center">Katib</th>
      <th style="text-align: center">Trainer</th>
      <th style="text-align: center">KServe</th>
      <th style="text-align: center">Model Registry</th>
      <th style="text-align: center">Spark</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="https://github.com/kubeflow/kubeflow/issues/7459">1.10</a></td>
      <td style="text-align: center"><a href="https://github.com/kubeflow/kubeflow/tags">1.10</a></td>
      <td style="text-align: center"><a href="https://github.com/kubeflow/pipelines/releases">2.4.1</a></td>
      <td style="text-align: center"><a href="https://github.com/kubeflow/katib/releases">0.18</a></td>
      <td style="text-align: center"><a href="https://github.com/kubeflow/trainer/releases">1.9</a></td>
      <td style="text-align: center"><a href="https://github.com/kserve/kserve/releases">0.14</a></td>
      <td style="text-align: center"><a href="https://github.com/kubeflow/model-registry/releases">0.2.15</a></td>
      <td style="text-align: center"><a href="https://github.com/kubeflow/spark-operator/releases/tag/v2.1.0">2.1.0</a></td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Kubernetes</th>
      <th style="text-align: center">Kind</th>
      <th style="text-align: center">Kustomize</th>
      <th style="text-align: center">Cert Manager</th>
      <th style="text-align: center">Knative</th>
      <th style="text-align: center">Istio</th>
      <th style="text-align: center">Dex</th>
      <th style="text-align: center">Oauth2-proxy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">1.31-1.33</td>
      <td style="text-align: center">0.26</td>
      <td style="text-align: center">5.4.3</td>
      <td style="text-align: center">1.16.1</td>
      <td style="text-align: center">1.16</td>
      <td style="text-align: center">1.24</td>
      <td style="text-align: center">2.41</td>
      <td style="text-align: center">7.7</td>
    </tr>
  </tbody>
</table>

<h3 id="security">Security:</h3>

<ul>
  <li>CVE reductions - regular scanning with trivy</li>
  <li>Kubernetes and container security best practices:
    <ul>
      <li><a href="https://github.com/kubeflow/manifests/issues/2528">Rootless containers</a> / PodSecurityStandards restricted for:
Istio-CNI, Knative, Dex, Oauth2-proxy, Spark</li>
      <li><a href="https://github.com/kubeflow/manifests/pull/3050">50 % done</a>: KFP, Notebooks / Workspaces, Katib, Trainer, Kserve, ‚Ä¶</li>
      <li>Istio-CNI as default for rootless Kubeflow postponed to <a href="https://github.com/kubeflow/manifests/milestone/2">1.10.1</a></li>
    </ul>
  </li>
  <li>OIDC-authservice has been replaced by oauth2-proxy</li>
  <li><a href="https://github.com/kubeflow/manifests#oauth2-proxy">Oauth2-proxy</a> and <a href="https://github.com/kubeflow/manifests#dex">Dex</a>
documentation for external OIDC authentication (Keycloak, and OIDC providers such as Azure, Google etc.)</li>
</ul>

<p>Trivy CVE scans March 25 2025:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Working Group</th>
      <th style="text-align: center">Images</th>
      <th style="text-align: center">Critical CVE</th>
      <th style="text-align: center">High CVE</th>
      <th style="text-align: center">Medium CVE</th>
      <th style="text-align: center">Low CVE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Katib</td>
      <td style="text-align: center">17</td>
      <td style="text-align: center">11</td>
      <td style="text-align: center">101</td>
      <td style="text-align: center">417</td>
      <td style="text-align: center">734</td>
    </tr>
    <tr>
      <td style="text-align: center">Pipelines</td>
      <td style="text-align: center">15</td>
      <td style="text-align: center">57</td>
      <td style="text-align: center">490</td>
      <td style="text-align: center">4030</td>
      <td style="text-align: center">1922</td>
    </tr>
    <tr>
      <td style="text-align: center">Workbenches(Notebooks)</td>
      <td style="text-align: center">12</td>
      <td style="text-align: center">12</td>
      <td style="text-align: center">59</td>
      <td style="text-align: center">179</td>
      <td style="text-align: center">224</td>
    </tr>
    <tr>
      <td style="text-align: center">Kserve</td>
      <td style="text-align: center">16</td>
      <td style="text-align: center">21</td>
      <td style="text-align: center">305</td>
      <td style="text-align: center">6803</td>
      <td style="text-align: center">1588</td>
    </tr>
    <tr>
      <td style="text-align: center">Manifests</td>
      <td style="text-align: center">14</td>
      <td style="text-align: center">8</td>
      <td style="text-align: center">4</td>
      <td style="text-align: center">94</td>
      <td style="text-align: center">52</td>
    </tr>
    <tr>
      <td style="text-align: center">Trainer</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">Model Registry</td>
      <td style="text-align: center">6</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">13</td>
      <td style="text-align: center">153</td>
      <td style="text-align: center">188</td>
    </tr>
    <tr>
      <td style="text-align: center">Spark</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">5</td>
      <td style="text-align: center">37</td>
      <td style="text-align: center">1640</td>
      <td style="text-align: center">141</td>
    </tr>
    <tr>
      <td style="text-align: center">All Images</td>
      <td style="text-align: center">81</td>
      <td style="text-align: center">115</td>
      <td style="text-align: center">1009</td>
      <td style="text-align: center">13275</td>
      <td style="text-align: center">4804</td>
    </tr>
  </tbody>
</table>

<h2 id="pipelines">Pipelines</h2>

<h3 id="support-for-placeholders-in-resource-limits">Support for Placeholders in Resource Limits</h3>

<p>Kubeflow Pipelines 2.4.1 introduces support for <a href="https://github.com/kubeflow/pipelines/pull/11501">placeholders in resource limits</a>,
enhancing flexibility in pipeline execution.This update allows users to define dynamic resource limits using
parameterized values, enabling more adaptable and reusable pipeline definitions.</p>

<h3 id="support-for-loop-parallelism">Support for Loop Parallelism</h3>

<p>Kubeflow Pipelines 2.4.1 introduces a new <a href="https://github.com/kubeflow/pipelines/issues/8718">Parallelism Limit for <code class="language-plaintext highlighter-rouge">ParallelFor</code> tasks</a>,
giving users the ability to run massively parallel inference pipelines, with more control over parallel execution in
their workflows. This feature allows users to specify the maximum number of parallel iterations, preventing resource
overutilization and improving system stability. When running large pipelines with GPUs, proper use of this feature could
save your team thousands of dollars in compute expenses.</p>

<h3 id="implement-subdag-output-resolution">Implement SubDAG Output Resolution</h3>

<p>Kubeflow 1.10 ensures that <a href="https://github.com/kubeflow/pipelines/pull/11196">pipelines using nested DAGs</a> work
correctly and reliably when treated as components. Outputs from deeply nested DAGs will now resolve properly, avoiding
broken dependencies.</p>

<h2 id="model-registry">Model Registry</h2>

<p>Model Registry introduces a new user interface and enhanced model management capabilities.</p>

<h3 id="model-registry-ui">Model Registry UI</h3>

<p>The new Kubeflow <a href="https://www.kubeflow.org/docs/components/model-registry/getting-started/#using-the-model-registry-ui">Model Registry UI</a>
provides a user-friendly web interface for managing machine learning models within the Kubeflow platform. It centralizes
model metadata, version tracking, and artifact management, streamlining MLOps workflows.</p>

<p>Key features include:</p>

<ul>
  <li>Easy model registration with custom metadata</li>
  <li>Comprehensive model management with filtering and sorting</li>
  <li>Archiving capabilities</li>
  <li>Version control</li>
  <li>Metadata editing</li>
</ul>

<p><img src="../images/2025-03-26-kubeflow-1.10-release/model-registry-ui.png" alt="Model Registry UI" /></p>

<p>The UI interacts with the Model Registry‚Äôs REST API, making it accessible to users of all technical backgrounds and
enhancing collaboration across data science, ML engineering, and MLOps teams.</p>

<p>To get started with the Model Registry UI, which is currently in Alpha, you can follow the instructions
<a href="https://www.kubeflow.org/docs/components/model-registry/installation/#installing-on-kubeflow-platform">here</a>.</p>

<p>The Kubeflow Model Registry UI Team would like to conduct user research to identify possible enhancements we can contribute in future iterations of the Kubeflow Model Registry UI. If you are interested in participating in this study, please fill out <a href="https://docs.google.com/forms/d/e/1FAIpQLSeCveL-b0NyUohYa86I3VeTXeynEQLpV5Loj-1HkoUVDwlVCQ/viewform">this survey</a>.</p>
<h3 id="custom-storage-initializer">Custom Storage Initializer</h3>

<p>The Model Registry Custom Storage Initializer (CSI) is a custom implementation of the KServe ClusterStorageContainer.
This feature allows users to utilize Model Registry metadata to download and deploy models efficiently. With the newest
release of the Model Registry, it is now possible to install and use the Custom Storage Initializer (CSI).</p>

<p>You can find detailed installation instructions and a small example in the ‚ÄúGetting Started‚Äù section of the Model
Registry component on the Kubeflow website.</p>

<p>For additional information and future developments towards better integration with KServe, you can refer to the slides
<a href="https://docs.google.com/presentation/d/1wprxN0n23EMkPRX_PaZZcIzZbn_i8Sh_">here</a>.</p>

<h2 id="training-operator-trainer--katib">Training Operator (Trainer) &amp; Katib</h2>

<p>Kubeflow 1.10 enhances the Training Operator and Katib, providing new tools and APIs for hyperparameter optimization,
particularly for large language models.</p>

<p>Moreover, the Kubeflow Training Operator now supports <a href="https://github.com/kubeflow/trainer/issues/1619">JAX for distributed training</a>,
enabling users to leverage JAX‚Äôs capabilities for efficient and scalable model training.</p>

<p>Finally, if you want to get involved with Trainer V2, take a look at this <a href="https://github.com/kubeflow/trainer/tree/master/docs/proposals/2170-kubeflow-trainer-v2">KEP</a>
and <a href="https://github.com/kubeflow/trainer/issues/2170">issue</a>.</p>

<h3 id="hyperparameter-optimization-api-for-llms">Hyperparameter Optimization API for LLMs</h3>

<p>Katib introduces a new high-level <a href="https://github.com/kubeflow/katib/issues/2339">API for hyperparameter tuning</a>,
streamlining LLMOps workflows in Kubernetes. This API integrates Katib and the Training Operator to automate
hyperparameter optimization, reducing manual effort for data scientists fine-tuning large language models. For more
information, refer to the <a href="https://blog.kubeflow.org/gsoc-2024-project-4/">feature release blog post</a>.</p>

<h3 id="support-for-various-parameter-distributions">Support for Various Parameter Distributions</h3>

<p>Katib now adds <a href="https://github.com/kubeflow/katib/issues/2374">support for multiple probability distributions</a>.
Previously limited to uniform distributions, Katib now supports log-uniform, normal, and log-normal distributions,
providing data scientists with greater <a href="https://youtu.be/4myE0DPp6Ko">flexibility in tuning hyperparameters</a>. This is
particularly useful for parameters like learning rates, which benefit from log-uniform sampling, or values expected to
vary around a mean, suited for normal distributions.</p>

<h3 id="push-based-metrics-collection">Push-Based Metrics Collection</h3>

<p>Katib now allows users to push metrics to Katib DB directly. The new push-based design provides administrative and
performanace improvements to the existing pull based design. For further details, please refer to the
<a href="https://blog.kubeflow.org/gsoc-2024-project-6/">Push-Based Metrics Collection blog post</a>.</p>

<h2 id="dashboard--notebooks">Dashboard &amp; Notebooks</h2>

<p>Kubeflow 1.10 improves the observability and usability of Notebooks, while providing updated
<a href="https://github.com/kubeflow/kubeflow/pull/7687">default images</a>.</p>

<h3 id="prometheus-metrics-for-notebooks">Prometheus Metrics for Notebooks</h3>

<p>Both the Notebooks component and CRUD backends now feature Prometheus metrics. Notebooks expose custom metrics using the
prom-client library, and CRUD backends utilize the prometheus_flask_exporter library. This ensures consistent metrics
integration across all backend services.</p>

<h3 id="more-descriptive-error-messages">More Descriptive Error Messages</h3>

<p>Error messages for notebook creation failures due to resource constraints are now more descriptive. Users can quickly
identify issues such as insufficient resources.</p>

<h2 id="spark-operator">Spark Operator</h2>

<p>The Spark Operator, now integrated as a core Kubeflow component, includes several key enhancements focusing on
architecture, security, and <a href="https://blog.kubeflow.org/operators/benchmarking/performance/2025/03/15/kubeflow-spark-operator-benchmarks.html">performance</a>:</p>

<ul>
  <li>Rebuilt with Controller Runtime (v2.0.0): Modernized core architecture using controller-runtime, aligning with
Kubernetes controller patterns for improved structure, extensibility, and testability.</li>
  <li>YuniKorn Gang Scheduling Support (v2.0.0): Enables efficient scheduling of Spark driver &amp; executor pods as a group,
ideal for large-scale data pipelines with resource guarantees.</li>
  <li>Enhanced Security Contexts &amp; SeccompProfile Support (v2.1.1): Adds support for <code class="language-plaintext highlighter-rouge">seccompProfile: RuntimeDefault</code> &amp; 
<code class="language-plaintext highlighter-rouge">readOnlyRootFilesystem</code>, aligning with Kubernetes Pod Security Standards and minimizing security risk.</li>
</ul>

<h2 id="kserve">KServe</h2>

<p>KServe v0.14.1 introduces several essential features that enhance its capabilities for deploying and managing machine
learning models.</p>

<h3 id="new-python-sdk">New Python SDK</h3>

<p>The release includes a new Python SDK with both REST and GRPC inference clients, offering asynchronous support and the
ability to handle tensor data in binary format.</p>

<h3 id="oci-storage-for-models">OCI Storage for Models</h3>

<p>OCI storage for models has also been promoted to a stable feature, with improvements to stability by configuring OCI
models as init containers.</p>

<h3 id="model-cache-feature">Model Cache Feature</h3>

<p>Additionally, the introduction of the Model Cache feature leverages local node storage to reduce model load times,
especially for large models, enhancing scalability.</p>

<h3 id="hugging-face-integration">Hugging Face Integration</h3>

<p>KServe v0.14.1 further expands integration with Hugging Face, enabling direct model deployment from the Hugbing Face hub
via a new <code class="language-plaintext highlighter-rouge">hf://</code> URI schema.</p>

<h2 id="what-comes-next">What comes next?</h2>

<p>If you want to take a peek into the Kubeflow 1.11 roadmap planning and contribute with your ideas, see
<a href="https://github.com/kubeflow/kubeflow/issues/7459">Notebooks</a>,
<a href="https://github.com/kubeflow/manifests/milestone/2">Manifests &amp; Security</a>, Pipelines, Model Registry, Katib,
Training Operator.</p>

<h2 id="how-to-get-started-with-110">How to get started with 1.10</h2>

<p>Visit the Kubeflow 1.10 <a href="https://github.com/kubeflow/manifests/releases">release page</a> or head over to the Getting
Started and Support pages.</p>

<h2 id="join-the-community">Join the Community</h2>

<p>We would like to thank everyone for the contribution to Kubeflow 1.10, especially Ricardo Martinelli De Oliveira for his
work as the v1.10 Release Manager, all the release team and the working group leads, who relentlessly dedicate their
time to this great project.</p>

<p>Release team members : Ricardo Martinelli De Oliveira, Dimitris Poulopoulos, Matteo Mortari, Julius von Kohout
Valentina Rodriguez Sosa, Helber Belmiro, Vraj Bhatt, Diego Lovison, Dagvanorov Lkhagvajav, Sailesh Duddupudi,
Manos Vlassis, Tarek Abouzeid, Milos Grubjesic</p>

<p>Working Group leads : Andrey Velichkevich, Julius von Kohout,  Mathew Wicks, ‚Ä¶</p>

<p>Kubeflow Steering Committee : Andrey Velichkevich, Julius von Kohout, Yuan Tang, Johnu George, Francisco Javier Araceo</p>

<p>Participating Distributions : Charmed Kubeflow (Canonical), Nutanix, OpenShift AI (RedHat), QBO</p>

<p>You can find more details about Kubeflow distributions
<a href="https://www.kubeflow.org/docs/started/installing-kubeflow/#packaged-distributions">here</a>.</p>

<h2 id="want-to-help">Want to help?</h2>

<p>The Kubeflow community Working Groups hold open meetings and are always looking for more volunteers and users to unlock
the potential of machine learning. If you‚Äôre interested in becoming a Kubeflow contributor, please feel free to check
out the resources below. We look forward to working with you!</p>

<ul>
  <li>Visit our <a href="https://www.kubeflow.org/docs/about/community/">Kubeflow website</a> or Kubeflow GitHub Page.</li>
  <li>Join the <a href="https://www.kubeflow.org/docs/about/community/">Kubeflow Slack channel</a>.</li>
  <li>Join the <a href="https://groups.google.com/g/kubeflow-discuss">kubeflow-discuss</a> mailing list.</li>
  <li>Attend our weekly <a href="https://www.kubeflow.org/docs/about/community/#kubeflow-community-call">community meeting</a>.</li>
</ul>]]></content><author><name>Kubeflow 1.10 Release Team, Dimitris Poulopoulos</name></author><category term="release" /><summary type="html"><![CDATA[Kubeflow 1.10.0 delivers essential updates that enhance the flexibility, efficiency, and scalability of machine learning workflows. The new features span across several components, improving both user experience and system performance.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.kubeflow.org/images/logo.png" /><media:content medium="image" url="https://blog.kubeflow.org/images/logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">üöÄ Announcing the Kubeflow Spark Operator Benchmarking Results</title><link href="https://blog.kubeflow.org/operators/benchmarking/performance/2025/03/15/kubeflow-spark-operator-benchmarks.html" rel="alternate" type="text/html" title="üöÄ Announcing the Kubeflow Spark Operator Benchmarking Results" /><published>2025-03-15T00:00:00-05:00</published><updated>2025-03-15T00:00:00-05:00</updated><id>https://blog.kubeflow.org/operators/benchmarking/performance/2025/03/15/kubeflow-spark-operator-benchmarks</id><content type="html" xml:base="https://blog.kubeflow.org/operators/benchmarking/performance/2025/03/15/kubeflow-spark-operator-benchmarks.html"><![CDATA[<p>Kubernetes has become the go-to platform for running large-scale <a href="https://spark.apache.org/">Apache Spark</a> workloads. But as workloads scale, <strong>how do you ensure your Spark jobs run efficiently without hitting bottlenecks?</strong> Managing thousands of concurrent Spark jobs can introduce <strong>severe performance challenges</strong>‚Äîfrom <strong>CPU saturation</strong> in the Spark Operator to <strong>Kubernetes API slowdowns</strong> and <strong>job scheduling inefficiencies</strong>.</p>

<p>To address these challenges, we are excited to introduce the <strong>Kubeflow Spark Operator Benchmarking Results and Toolkit</strong>‚Äîa comprehensive framework to analyze performance, pinpoint bottlenecks, and optimize your Spark on Kubernetes deployments.</p>

<h2 id="-whats-included">üîç What‚Äôs Included?</h2>
<p>This benchmarking effort provides <strong>three key outcomes</strong> to help you take full control of your Spark on Kubernetes deployment:</p>

<p>‚úÖ <strong><a href="https://www.kubeflow.org/docs/components/spark-operator/performance/benchmarking/">Benchmarking Results</a></strong> ‚Äì A detailed evaluation of performance insights and tuning recommendations for large-scale Spark workloads.<br />
üõ† <strong><a href="https://github.com/awslabs/data-on-eks/tree/main/analytics/terraform/spark-k8s-operator/examples/benchmark/spark-operator-benchmark-kit">Benchmarking Test Toolkit</a></strong> ‚Äì A fully reproducible test suite to help users evaluate their own Spark Operator performance and validate improvements.<br />
üìä <strong><a href="https://grafana.com/grafana/dashboards/23032-spark-operator-scale-test-dashboard/">Open-Sourced Grafana Dashboard</a></strong> ‚Äì A <strong>battle-tested</strong> visualization tool designed specifically to track large-scale Spark Operator deployments, providing real-time monitoring of job processing efficiency, API latencies, and system health.</p>

<h2 id="-the-challenges-why-benchmarking-matters">‚ùå The Challenges: Why Benchmarking Matters</h2>
<p>Running <strong>thousands of Spark jobs</strong> on Kubernetes at scale uncovers several <strong>performance roadblocks</strong> that can <strong>cripple efficiency</strong> if left unresolved:</p>

<ul>
  <li><strong>üö¶ Spark Operator Becomes CPU-Bound</strong>: When handling thousands of Spark jobs, the controller pod maxes out CPU resources, limiting job submission rates.</li>
  <li><strong>üê¢ High API Server Latency</strong>: As workloads scale, Kubernetes API responsiveness degrades‚Äîjob status updates slow down, affecting observability and scheduling efficiency.</li>
  <li><strong>üïí Webhook Overhead Slows Job Starts</strong>: Using webhooks adds <strong>~60 seconds</strong> of extra latency per job, reducing throughput in high-concurrency environments.</li>
  <li><strong>üí• Namespace Overload Causes Failures</strong>: Running <strong>6,000+ SparkApplications in a single namespace</strong> resulted in <strong>pod failures</strong> due to excessive environment variables and service object overload.</li>
</ul>

<p>üí° <strong>So, how do you fix these issues and optimize your Spark Operator deployment?</strong><br />
That‚Äôs where our <strong>benchmarking results and toolkit</strong> come in.</p>

<h2 id="-tuning-best-practices-for-spark-operator">üõ† Tuning Best Practices for Spark Operator</h2>
<p>Based on our benchmarking findings, we provide <strong>clear, actionable recommendations</strong> for improving Spark Operator performance at scale.</p>

<p>If you‚Äôre running <strong>thousands of concurrent Spark jobs</strong>, here‚Äôs what you need to do:</p>

<h3 id="deploy-multiple-spark-operator-instances"><strong>Deploy Multiple Spark Operator Instances</strong></h3>
<p>üí° <strong>Why?</strong> A single Spark Operator instance struggles to keep up with high job submission rates.<br />
‚úÖ <strong>Solution</strong>: When a single Spark Operator instance struggles with high job submission rates, leading to CPU saturation and slower job launches, <strong>deploying multiple instances can help</strong>. Distribute the workload by assigning different namespaces to each instance. For example, one instance can manage `<strong>20 namespaces</strong> while another handles a separate set of <strong>20 namespaces</strong>. This prevents bottlenecks and ensures efficient Spark job execution.</p>

<h3 id="disable-webhooks-for-faster-job-starts"><strong>Disable Webhooks for Faster Job Starts</strong></h3>
<p>üí° <strong>Why?</strong> Webhooks introduce <code class="language-plaintext highlighter-rouge">~60 seconds</code> of delay per job due to validation and mutation overhead, reducing throughput in large workloads.
‚úÖ <strong>Solution</strong>: Instead of using <strong>webhooks</strong> for volume mounts, node selectors, or taints, define <strong>Spark Pod Templates</strong> directly within the Spark job definition‚Äîno additional files are needed. Disable webhooks by setting <code class="language-plaintext highlighter-rouge">webhook.enable=false</code> in the Helm chart.</p>

<h3 id="increase-controller-workers"><strong>Increase Controller Workers</strong></h3>
<p>üí° <strong>Why?</strong> By default, the operator runs with <strong>10 controller workers</strong>, but our benchmarks showed increasing this to <strong>20 or 30 workers</strong> improved job throughput.<br />
‚úÖ <strong>Solution</strong>: Set <code class="language-plaintext highlighter-rouge">controller.workers=20</code> if your Operator pod runs on a <code class="language-plaintext highlighter-rouge">36-core</code> CPU or higher to enable faster parallel job execution. For larger workloads (e.g., 72+ cores), increase to 40+ workers for better parallel job execution.</p>

<h3 id="enable-a-batch-scheduler-volcano--yunikorn"><strong>Enable a Batch Scheduler (Volcano / YuniKorn)</strong></h3>
<p>üí° <strong>Why?</strong> Kubernetes‚Äô default scheduler isn‚Äôt optimized for batch workloads, leading to <strong>inefficient job placements</strong>.<br />
‚úÖ <strong>Solution</strong>: Enable <strong>Volcano</strong> or <strong>YuniKorn</strong> (<code class="language-plaintext highlighter-rouge">batchScheduler.enable=true</code>) to optimize job scheduling. These schedulers provide <strong>gang scheduling, queue management, and multi-tenant resource sharing</strong>. Benchmarks show that <strong>Apache YuniKorn</strong> schedules jobs faster than the default Kubernetes scheduler.</p>

<h3 id="optimize-api-server-scaling"><strong>Optimize API Server Scaling</strong></h3>
<p>üí° <strong>Why?</strong> API server latency spikes to <strong>600ms+ under heavy load</strong>, affecting Spark job responsiveness.<br />
‚úÖ <strong>Solution</strong>: Scale API server replicas, allocate more CPU and memory, and optimize event handling. Ensure your <strong>Kubernetes API server and etcd</strong> auto-scale to handle bursty workloads efficiently. Monitor <code class="language-plaintext highlighter-rouge">kube-apiserver</code> metrics and scale <code class="language-plaintext highlighter-rouge">etcd</code> accordingly. If running thousands of Spark pods, consider <strong>manually increasing control plane node sizes</strong>.</p>

<h3 id="distribute-spark-jobs-across-multiple-namespaces"><strong>Distribute Spark Jobs Across Multiple Namespaces</strong></h3>
<p>üí° <strong>Why?</strong> Running too many jobs in a single namespace causes <strong>environment variable overflows</strong>, leading to pod failures.<br />
‚úÖ <strong>Solution</strong>: When too many pods are placed in a single namespace, operations like listing or modifying resources can generate large <strong>API server</strong> responses, increasing latency. For example, retrieving all pods may result in a substantial size in response, consuming significant server resources. Additionally, <strong>etcd</strong>, Kubernetes‚Äô key-value store, can become a bottleneck when handling frequent updates from a high number of pods in one namespace. Heavy read and write operations can strain etcd, causing increased latencies and potential timeouts. To improve performance and stability, it is recommended to <strong>distribute workloads across multiple namespaces</strong>.</p>

<h3 id="monitor--tune-using-the-open-source-grafana-dashboard"><strong>Monitor &amp; Tune Using the Open-Source Grafana Dashboard</strong></h3>
<p>üí° <strong>Why?</strong> Observability is key to identifying performance bottlenecks.<br />
‚úÖ <strong>Solution</strong>: Use our <strong><a href="https://grafana.com/grafana/dashboards/23032-spark-operator-scale-test-dashboard/">Spark Operator Scale Test Dashboard</a></strong> to track job submission rates, API latencies, and CPU utilization in real time.</p>

<h2 id="-learn-more--get-started">üìñ Learn More &amp; Get Started</h2>
<p>The <strong>Kubeflow Spark Operator Benchmarking Results and Toolkit</strong> provide an in-depth <strong>performance playbook</strong> for running Spark at scale on Kubernetes. Whether you‚Äôre troubleshooting an existing deployment or planning for future growth, this toolkit arms you with <strong>data-driven insights</strong> and <strong>best practices</strong> for success.</p>

<p>üöÄ <strong>Ready to optimize your Spark workloads?</strong> Dive into the full results and toolkit below:<br />
üìñ <strong><a href="https://www.kubeflow.org/docs/components/spark-operator/performance/benchmarking/">Kubeflow Spark Operator Benchmarks</a></strong></p>]]></content><author><name>&lt;a href='https://www.linkedin.com/in/varaprofile/'&gt;Vara Bonthu&lt;/a&gt;, &lt;a href='https://www.linkedin.com/in/manabumccloskey/'&gt;Manabu McCloskey&lt;/a&gt;, &lt;a href='https://www.linkedin.com/in/ratnopamc/'&gt;Ratnopam Chakrabarti &lt;/a&gt;, &lt;a href='https://www.linkedin.com/in/alanhalcyon/'&gt;Alan Halcyon&lt;/a&gt;</name></author><category term="operators" /><category term="benchmarking" /><category term="performance" /><summary type="html"><![CDATA[Kubernetes has become the go-to platform for running large-scale Apache Spark workloads. But as workloads scale, how do you ensure your Spark jobs run efficiently without hitting bottlenecks? Managing thousands of concurrent Spark jobs can introduce severe performance challenges‚Äîfrom CPU saturation in the Spark Operator to Kubernetes API slowdowns and job scheduling inefficiencies.]]></summary></entry><entry><title type="html">Optimizing RAG Pipelines with Katib: Hyperparameter Tuning for Better Retrieval &amp;amp; Generation</title><link href="https://blog.kubeflow.org/katib/rag/" rel="alternate" type="text/html" title="Optimizing RAG Pipelines with Katib: Hyperparameter Tuning for Better Retrieval &amp;amp; Generation" /><published>2025-02-21T00:00:00-06:00</published><updated>2025-02-21T00:00:00-06:00</updated><id>https://blog.kubeflow.org/katib/katib-rag-optimization</id><content type="html" xml:base="https://blog.kubeflow.org/katib/rag/"><![CDATA[<h1 id="introduction">Introduction</h1>

<p>As artificial intelligence and machine learning models become more
sophisticated, optimising their performance remains a critical challenge.
Kubeflow provides a robust component, <a href="https://www.kubeflow.org/docs/components/katib/">Katib</a>, designed for
hyperparameter optimization and neural architecture search. As a part of the
Kubeflow ecosystem, Katib enables scalable, automated tuning of underlying
machine learning models, reducing the manual effort required for parameter
selection while improving model performance across diverse ML workflows.</p>

<p>With Retrieval-Augmented Generation (<a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a>) becoming an increasingly
popular approach for improving search and retrieval quality, optimizing its
parameters is essential to achieving high-quality results. RAG pipelines involve
multiple hyperparameters that influence retrieval accuracy, hallucination
reduction, and language generation quality. In this blog, we will explore how
Katib can be leveraged to fine-tune a RAG pipeline, ensuring optimal performance
by systematically adjusting key hyperparameters.</p>

<h1 id="lets-get-started">Let‚Äôs Get Started!</h1>

<h2 id="step-1-setup">STEP 1: Setup</h2>

<p>Since compute resources are scarcer than a perfectly labeled dataset :), we‚Äôll
use a lightweight <a href="https://kind.sigs.k8s.io/">Kind cluster (Kubernetes in Docker)</a>
cluster to run this example locally. Rest assured, this setup can seamlessly
scale to larger clusters by increasing the dataset size and the number of
hyperparameters to tune.</p>

<p>To get started, we‚Äôll first install the Katib control plane in our cluster by
following the steps outlined <a href="https://www.kubeflow.org/docs/components/katib/installation/">in the documentation</a>.</p>

<h2 id="step-2-implementing-rag-pipeline">STEP 2: Implementing RAG pipeline</h2>

<p>In this implementation, we use a <a href="https://www.sciencedirect.com/topics/computer-science/retrieval-model">retriever model</a>, which
encodes queries and documents into vector representations to find the most
relevant matches, to fetch relevant documents based on a query and a generator
model to produce coherent text responses.</p>

<h3 id="implementation-details">Implementation Details:</h3>

<ol>
  <li>Retriever: Sentence Transformer &amp; FAISS (Facebook AI Similarity Search) Index
    <ul>
      <li>A SentenceTransformer model (paraphrase-MiniLM-L6-v2) encodes predefined
documents into vector representations.</li>
      <li><a href="https://ai.meta.com/tools/faiss/">FAISS</a> is used to index these document embeddings and perform
efficient similarity searches to retrieve the most relevant documents.</li>
    </ul>
  </li>
  <li>Generator: Pre-trained GPT-2 Model
    <ul>
      <li>A Hugging Face GPT-2 text generation pipeline (which can be replaced with
any other model) is used to generate responses based on the retrieved
documents. I chose GPT-2 for this example as it is lightweight enough to
run on my local machine while still generating coherent responses.</li>
    </ul>
  </li>
  <li>Query Processing &amp; Response Generation
    <ul>
      <li>When a query is submitted, the retriever encodes it and searches the FAISS
index for the top-k most similar documents.</li>
      <li>These retrieved documents are concatenated to form the input context, which
is then passed to the GPT-2 model to generate a response.</li>
    </ul>
  </li>
  <li>Evaluation: <a href="https://huggingface.co/spaces/evaluate-metric/bleu">BLEU</a> (Bilingual Evaluation Understudy) Score Calculation
    <ul>
      <li>To assess the quality of generated responses, we use the BLEU score, a
popular metric for evaluating text generation.</li>
      <li>The evaluate function takes a query, retrieves documents, generates a
response, and compares it against a ground-truth reference to compute a
BLEU score with smoothing functions from the nltk library.</li>
    </ul>
  </li>
</ol>

<p>To run Katib, we will use the <a href="https://www.kubeflow.org/docs/components/katib/installation/#installing-python-sdk">Katib SDK</a>, which provides a programmatic interface for defining and running 
hyperparameter tuning experiments in Kubeflow.</p>

<p>Katib requires an <a href="https://www.kubeflow.org/docs/components/katib/user-guides/hp-tuning/configure-experiment/#configuring-the-experiment">objective</a> function, which:</p>

<ol>
  <li>Defines what we want to optimize (e.g., BLEU score for text generation quality).</li>
  <li>Executes the RAG pipeline with different hyperparameter values.</li>
  <li>Returns an evaluation metric so Katib can compare different hyperparameter configurations.</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">objective</span><span class="p">(</span><span class="n">parameters</span><span class="p">):</span>
    <span class="c1"># Import dependencies inside the function (required for Katib)
</span>    <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
    <span class="kn">import</span> <span class="nn">faiss</span>
    <span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>
    <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>
    <span class="kn">from</span> <span class="nn">nltk.translate.bleu_score</span> <span class="kn">import</span> <span class="n">sentence_bleu</span><span class="p">,</span> <span class="n">SmoothingFunction</span>
    
    <span class="c1"># Function to fetch documents (Modify as needed)
</span>    <span class="k">def</span> <span class="nf">fetch_documents</span><span class="p">():</span>
        <span class="s">"""Returns a predefined list of documents or loads them from a file."""</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="p">...</span>
        <span class="p">]</span>
        <span class="c1"># OR, to load from a file:
</span>        <span class="c1"># with open("/path/to/documents.json", "r") as f:
</span>        <span class="c1">#     return json.load(f)
</span>
    <span class="c1"># Define the RAG pipeline within the function
</span>    <span class="k">def</span> <span class="nf">rag_pipeline_execute</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">top_k</span><span class="p">,</span> <span class="n">temperature</span><span class="p">):</span>
        <span class="s">"""Retrieves relevant documents and generates a response using GPT-2."""</span>

        <span class="c1"># Initialize retriever
</span>        <span class="n">retriever_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s">"paraphrase-MiniLM-L6-v2"</span><span class="p">)</span>

        <span class="c1"># Sample documents
</span>        <span class="n">documents</span> <span class="o">=</span> <span class="n">fetch_documents</span><span class="p">()</span>

        <span class="c1"># Encode documents
</span>        <span class="n">doc_embeddings</span> <span class="o">=</span> <span class="n">retriever_model</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
        <span class="n">index</span> <span class="o">=</span> <span class="n">faiss</span><span class="p">.</span><span class="n">IndexFlatL2</span><span class="p">(</span><span class="n">doc_embeddings</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">index</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">doc_embeddings</span><span class="p">))</span>

        <span class="c1"># Encode query and retrieve top-k documents
</span>        <span class="n">query_embedding</span> <span class="o">=</span> <span class="n">retriever_model</span><span class="p">.</span><span class="n">encode</span><span class="p">([</span><span class="n">query</span><span class="p">])</span>
        <span class="n">distances</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">index</span><span class="p">.</span><span class="n">search</span><span class="p">(</span><span class="n">query_embedding</span><span class="p">,</span> <span class="n">top_k</span><span class="p">)</span>
        <span class="n">retrieved_docs</span> <span class="o">=</span> <span class="p">[</span><span class="n">documents</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>

        <span class="c1"># Generate response using GPT-2
</span>        <span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s">"text-generation"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s">"gpt2"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="s">"gpt2"</span><span class="p">)</span>
        <span class="n">context</span> <span class="o">=</span> <span class="s">" "</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">retrieved_docs</span><span class="p">)</span>
        <span class="n">generated</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">generated</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s">"generated_text"</span><span class="p">]</span>

    <span class="c1"># TODO: Provide queries and ground truth directly here or load them dynamically from a file/external volume.
</span>    <span class="n">query</span> <span class="o">=</span> <span class="s">""</span>  <span class="c1"># Example: "Tell me about the Eiffel Tower."
</span>    <span class="n">ground_truth</span> <span class="o">=</span> <span class="s">""</span>  <span class="c1"># Example: "The Eiffel Tower is a famous landmark in Paris."
</span>
    <span class="c1"># Extract hyperparameters
</span>    <span class="n">top_k</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s">"top_k"</span><span class="p">])</span>
    <span class="n">temperature</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s">"temperature"</span><span class="p">])</span>

    <span class="c1"># Generate response
</span>    <span class="n">response</span> <span class="o">=</span> <span class="n">rag_pipeline_execute</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">top_k</span><span class="p">,</span> <span class="n">temperature</span><span class="p">)</span>

    <span class="c1"># Compute BLEU score
</span>    <span class="n">reference</span> <span class="o">=</span> <span class="p">[</span><span class="n">ground_truth</span><span class="p">.</span><span class="n">split</span><span class="p">()]</span>  <span class="c1"># Tokenized reference
</span>    <span class="n">candidate</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="n">split</span><span class="p">()</span>  <span class="c1"># Tokenized candidate response
</span>    <span class="n">smoothie</span> <span class="o">=</span> <span class="n">SmoothingFunction</span><span class="p">().</span><span class="n">method1</span>
    <span class="n">bleu_score</span> <span class="o">=</span> <span class="n">sentence_bleu</span><span class="p">(</span><span class="n">reference</span><span class="p">,</span> <span class="n">candidate</span><span class="p">,</span> <span class="n">smoothing_function</span><span class="o">=</span><span class="n">smoothie</span><span class="p">)</span>

    <span class="c1"># Print BLEU score in Katib-compatible format
</span>    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"BLEU=</span><span class="si">{</span><span class="n">bleu_score</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>
<p><em>Note</em>: Make sure to return the result in the format of <code class="language-plaintext highlighter-rouge">&lt;parameter&gt;=&lt;value&gt;</code>
for Katib‚Äôs metrics collector to be able to utilize it. More ways to configure
the output are available in <a href="https://www.kubeflow.org/docs/components/katib/user-guides/metrics-collector/#pull-based-metrics-collector">Katib Metrics
Collector</a> guide.</p>

<h2 id="step-3-run-a-katib-experiment">STEP 3: Run a Katib Experiment</h2>

<p>Once our pipeline is encapsulated within the objective function, we can configure Katib to optimize the <code class="language-plaintext highlighter-rouge">BLEU</code> score by 
tuning the hyperparameters:</p>

<ol>
  <li><code class="language-plaintext highlighter-rouge">top_k</code>: The number of documents retrieved (eg. between 10 and 20).</li>
  <li><code class="language-plaintext highlighter-rouge">temperature</code>: The randomness of text generation (eg. between 0.5 and 1.0).</li>
</ol>

<h1 id="define-hyperparameter-search-space">Define hyperparameter search space</h1>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"top_k"</span><span class="p">:</span> <span class="n">katib</span><span class="p">.</span><span class="n">search</span><span class="p">.</span><span class="nb">int</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">20</span><span class="p">),</span>
    <span class="s">"temperature"</span><span class="p">:</span> <span class="n">katib</span><span class="p">.</span><span class="n">search</span><span class="p">.</span><span class="n">double</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Let‚Äôs submit the experiment! We‚Äôll use the <a href="https://github.com/kubeflow/katib/blob/c18035e1041ca1b87ea7eb7c01cb81b5e2b922b3/sdk/python/v1beta1/kubeflow/katib/api/katib_client.py#L178"><code class="language-plaintext highlighter-rouge">tune</code> API </a> that will run multiple trials to find the optimal <code class="language-plaintext highlighter-rouge">top_k</code> 
and <code class="language-plaintext highlighter-rouge">temperature</code> values for our RAG pipeline.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">katib_client</span> <span class="o">=</span> <span class="n">katib</span><span class="p">.</span><span class="n">KatibClient</span><span class="p">(</span><span class="n">namespace</span><span class="o">=</span><span class="s">"kubeflow"</span><span class="p">)</span>

<span class="n">name</span> <span class="o">=</span> <span class="s">"rag-tuning-experiment"</span>
<span class="n">katib_client</span><span class="p">.</span><span class="n">tune</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
    <span class="n">objective</span><span class="o">=</span><span class="n">objective</span><span class="p">,</span>
    <span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span>
    <span class="n">algorithm_name</span><span class="o">=</span><span class="s">"grid"</span><span class="p">,</span>  <span class="c1"># Grid search for hyperparameter tuning
</span>    <span class="n">objective_metric_name</span><span class="o">=</span><span class="s">"BLEU"</span><span class="p">,</span>
    <span class="n">objective_type</span><span class="o">=</span><span class="s">"maximize"</span><span class="p">,</span>
    <span class="n">objective_goal</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
    <span class="n">max_trial_count</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>  <span class="c1"># Run up to 10 trials
</span>    <span class="n">parallel_trial_count</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># Run 2 trials in parallel
</span>    <span class="n">resources_per_trial</span><span class="o">=</span><span class="p">{</span><span class="s">"cpu"</span><span class="p">:</span> <span class="s">"1"</span><span class="p">,</span> <span class="s">"memory"</span><span class="p">:</span> <span class="s">"2Gi"</span><span class="p">},</span>
    <span class="n">base_image</span><span class="o">=</span><span class="s">"python:3.10-slim"</span><span class="p">,</span>
    <span class="n">packages_to_install</span><span class="o">=</span><span class="p">[</span>
        <span class="s">"transformers==4.36.0"</span><span class="p">,</span>
        <span class="s">"sentence-transformers==2.2.2"</span><span class="p">,</span>
        <span class="s">"faiss-cpu==1.7.4"</span><span class="p">,</span>
        <span class="s">"numpy==1.23.5"</span><span class="p">,</span>
        <span class="s">"huggingface_hub==0.20.0"</span><span class="p">,</span>
        <span class="s">"nltk==3.9.1"</span>
    <span class="p">]</span>
<span class="p">)</span>
</code></pre></div></div>

<p>Once the experiment is submitted, we can see output indicating that Katib has started the trials:</p>

<pre><code class="language-commandline">Experiment Trials status: 0 Trials, 0 Pending Trials, 0 Running Trials, 0 Succeeded Trials, 0 Failed Trials, 0 EarlyStopped Trials, 0 MetricsUnavailable Trials
Current Optimal Trial:
 {'best_trial_name': None,
 'observation': {'metrics': None},
 'parameter_assignments': None}
Experiment conditions:
 [{'last_transition_time': datetime.datetime(2025, 3, 13, 19, 40, 32, tzinfo=tzutc()),
 'last_update_time': datetime.datetime(2025, 3, 13, 19, 40, 32, tzinfo=tzutc()),
 'message': 'Experiment is created',
 'reason': 'ExperimentCreated',
 'status': 'True',
 'type': 'Created'}]
Waiting for Experiment: kubeflow/rag-tuning-experiment to reach Succeeded condition

.....

Experiment Trials status: 9 Trials, 0 Pending Trials, 2 Running Trials, 7 Succeeded Trials, 0 Failed Trials, 0 EarlyStopped Trials, 0 MetricsUnavailable Trials
Current Optimal Trial:
 {'best_trial_name': 'rag-tuning-experiment-66tmh9g7',
 'observation': {'metrics': [{'latest': '0.047040418725887996',
                              'max': '0.047040418725887996',
                              'min': '0.047040418725887996',
                              'name': 'BLEU'}]},
 'parameter_assignments': [{'name': 'top_k', 'value': '10'},
                           {'name': 'temperature', 'value': '0.6'}]}
Experiment conditions:
 [{'last_transition_time': datetime.datetime(2025, 3, 13, 19, 40, 32, tzinfo=tzutc()),
 'last_update_time': datetime.datetime(2025, 3, 13, 19, 40, 32, tzinfo=tzutc()),
 'message': 'Experiment is created',
 'reason': 'ExperimentCreated',
 'status': 'True',
 'type': 'Created'}, {'last_transition_time': datetime.datetime(2025, 3, 13, 19, 40, 52, tzinfo=tzutc()),
 'last_update_time': datetime.datetime(2025, 3, 13, 19, 40, 52, tzinfo=tzutc()),
 'message': 'Experiment is running',
 'reason': 'ExperimentRunning',
 'status': 'True',
 'type': 'Running'}]
Waiting for Experiment: kubeflow/rag-tuning-experiment to reach Succeeded condition
</code></pre>

<p>We can also see the experiments and trials being run to search for the optimized parameter:</p>

<pre><code class="language-commandline">kubectl get experiments.kubeflow.org -n kubeflow
NAME                    TYPE      STATUS   AGE
rag-tuning-experiment   Running   True     10m
</code></pre>

<pre><code class="language-commandline">kubectl get trials --all-namespaces
NAMESPACE   NAME                             TYPE      STATUS   AGE
kubeflow    rag-tuning-experiment-7wskq9b9   Running   True     10m
kubeflow    rag-tuning-experiment-cll6bt4z   Running   True     10m
kubeflow    rag-tuning-experiment-hzxrzq2t   Running   True     10m
</code></pre>

<p>The list of completed trials and their results will be shown in the UI like
below. Steps to access Katib UI are available <a href="https://www.kubeflow.org/docs/components/katib/user-guides/katib-ui/">in the documentation</a>:</p>

<p><img src="/images/2025-02-21-katib-rag-optimization/katib_experiment_run.jpeg" alt="completed_runs" />
<img src="/images/2025-02-21-katib-rag-optimization/katib_ui.jpeg" alt="trial details" /></p>

<h1 id="conclusion">Conclusion</h1>

<p>In this experiment, we leveraged Kubeflow Katib to optimize a
Retrieval-Augmented Generation (RAG) pipeline, systematically tuning key
hyperparameters like top_k and temperature to enhance retrieval precision and
generative response quality.</p>

<p>For anyone working with RAG systems or hyperparameter optimization, Katib is a
powerful tool‚Äîenabling scalable, efficient, and intelligent tuning of machine
learning models! We hope this tutorial helps you streamline hyperparameter
tuning and unlock new efficiencies in your ML workflows!</p>]]></content><author><name>Varsha Prasad Narsing (@varshaprasad96)</name></author><category term="katib" /><summary type="html"><![CDATA[Leveraging Katib for efficient RAG optimization.]]></summary></entry></feed>