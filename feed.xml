<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://blog.kubeflow.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://blog.kubeflow.org/" rel="alternate" type="text/html" /><updated>2022-09-08T11:15:13-05:00</updated><id>https://blog.kubeflow.org/feed.xml</id><title type="html">Kubeflow</title><subtitle>The Machine Learning Toolkit for Kubernetes.</subtitle><entry><title type="html">Kubeflow v1.6 delivers support for Kubernetes v1.22 and introduces an alpha release of the Kubeflow Pipeline v2 functionality</title><link href="https://blog.kubeflow.org/kubeflow-1.6-release/" rel="alternate" type="text/html" title="Kubeflow v1.6 delivers support for Kubernetes v1.22 and introduces an alpha release of the Kubeflow Pipeline v2 functionality" /><published>2022-08-24T00:00:00-05:00</published><updated>2022-08-24T00:00:00-05:00</updated><id>https://blog.kubeflow.org/kubeflow-1.6-release</id><content type="html" xml:base="https://blog.kubeflow.org/kubeflow-1.6-release/">&lt;p&gt;The Kubeflow Community is excited to announce the availability of the Kubeflow v1.6 software release, which includes
 support for Kubernetes v1.22 and introduces an alpha release of the Kubeflow Pipeline v2 (KFP v2) functionality.&lt;/p&gt;

&lt;p&gt;Kubeflow v1.6 also adds new hyperparameter support for the population based algorithm in Katib, and provides a combined
 Python SDK for PyTorch, MXNet, MPI, XGBoost in Kubeflow’s distributed Training Operator. For model serving, v1.6 has
 new ClusterServingRuntime and ServingRuntime CRDs, and a new Model Spec was introduced to the InferenceService
 Predictor Spec, which provides a new way to specify models in KServe 0.8. Additionally, v1.6 cleans up a few CVEs in
 the central dashboard and enables the PodDefaults webhook to pick-up new certificate updates.   For the Kubernetes upgrade,
 the community developed and tested each Kubeflow component manifest with Kubernetes v1.22.  As v1.22 introduced some breaking changes, the upgrade was a team effort, and this Kubeflow release management process will be useful in the community’s future updates of Kubernetes and other software dependencies. The software is available &lt;a href=&quot;https://github.com/kubeflow/kubeflow/releases/tag/v1.6.0&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In parallel to developing these new software features, the Kubeflow Community also completed its annual user survey.
 The survey generated many good user insights into requirements for Kubeflow, and you can see more on the
 &lt;a href=&quot;https://github.com/kubeflow/blog/pull/121&quot;&gt;survey results&lt;/a&gt;. Of the many
 highlights, we would like to identify the growing request for model monitoring as shown in the chart below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-08-24-kubeflow-1.6-release/2022-user-survey-gaps-in-ml-activities-and-workflow.png&quot; alt=&quot;2022 User Survey Gaps In ML Activities and Workflow&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We believe the increased focus on monitoring identifies Kubeflow’s maturing user base. The Community is making efforts
 to explain Kubeflow’s current functionality and build into the users’ new model monitoring requirements. In addition,
 the user survey results and Kubeflow Working Group roadmaps will be discussed in the upcoming Kubeflow Summit. You can
 learn more about the Kubeflow Summit &lt;a href=&quot;https://docs.google.com/document/d/1fLg1EqVtJekcXxT8GM_Rqg0-G-vIDVCrzuxE06Oq9dI/edit&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To round back to the current software delivery, the following takes a deeper look at the v1.6 highlights in more detail.&lt;/p&gt;

&lt;h2 id=&quot;kubeflow-v16-details&quot;&gt;Kubeflow v1.6 details&lt;/h2&gt;

&lt;p&gt;From a new feature introduction standpoint, Kubeflow v1.6 includes an experimental release of the KFP v2’s new front-end,
 back-end and SDK, which deliver a modern UI and DAG, first class support for metadata, and a simplified component
 authoring experience. This slide deck, &lt;a href=&quot;https://docs.google.com/presentation/d/1HzMwtI2QN67xQp2lSxmuXhitEsukLB7mvZx4KAPub3A/edit#slide=id.gecbd775238_0_20&quot;&gt;KFP v2 Introduction&lt;/a&gt;,
 provides a good overview of v2.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The KFP v2 alpha introduces:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;An Argo-agnostic approach for creating and running pipelines&lt;/li&gt;
  &lt;li&gt;A brand new DAG visualization, which uses the Pipeline Template and MLMD in this &lt;a href=&quot;https://github.com/kubeflow/pipelines/blob/master/api/v2alpha1/pipeline_spec.proto&quot;&gt;pipeline spec&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.google.com/presentation/d/1HzMwtI2QN67xQp2lSxmuXhitEsukLB7mvZx4KAPub3A/edit#slide=id.gecbd775238_0_438&quot;&gt;Streamlined Component Authoring&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;You can learn more about KFP v2 and the related breaking changes in these docs:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.google.com/presentation/d/1HzMwtI2QN67xQp2lSxmuXhitEsukLB7mvZx4KAPub3A/edit#slide=id.gb4a3fac3a8_7_1911&quot;&gt;KFP v2 Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kubeflow-pipelines.readthedocs.io/en/master/&quot;&gt;The SDK reference&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.google.com/document/d/1nCUUVRXexXbQ0LDkGHsMIBDSu1WvJA9Upy1JzybNVMk/edit&quot;&gt;Breaking changes&lt;/a&gt; including an SDK change from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kfp.v2&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kfp&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;For those users who are still relying on the KFP v1 functionality, Kubeflow v1.6 and its KFP v2 component are fully
 tested and supported with the mature features in the legacy Kubeflow Pipelines v1.8 SDK. This provides the same
 functionality that thousands of KFP v1 users leverage in production today.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In addition to the KFP v2 alpha, Kubeflow 1.6 includes feature enhancements and operational improvements for Katib
 users. These include support for:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The population based training algorithm&lt;/li&gt;
  &lt;li&gt;Enhanced validation checks for configurations, which will save time debugging parameter misconfiguration&lt;/li&gt;
  &lt;li&gt;Security fixes&lt;/li&gt;
  &lt;li&gt;MetricsUnavailable Status support, which will make debugging easier&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In Kubeflow v1.6, the Training Operator Working Group added these valuable enhancements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Python SDK for PyTorch, MXNet, MPI, XGBoost&lt;/li&gt;
  &lt;li&gt;The Clientset (Golang) is also generated for PyTorch, MXNet, MPI, XGBoost&lt;/li&gt;
  &lt;li&gt;Gang scheduling support for MPI&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Kubeflow v1.6 has several functional and operational improvements to Kubeflow notebooks, central dashboard, webapps and
 controllers. The following highlights the closed PRs that were considered medium-sized or larger. You can review
 the full list of the closed PRs &lt;a href=&quot;https://github.com/kubeflow/kubeflow/pulls?page=1&amp;amp;q=is%3Apr+is%3Aclosed&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/kubeflow/kubeflow/issues/6353&quot;&gt;Support for K8s 1.22&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/kubeflow/kubeflow/issues/5560&quot;&gt;PodDefaults webhook picking up new certificates&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Show objects from all names spaces in Central Dashboard, Jupyter, Tensorboard, Volumes Mgr&lt;/li&gt;
  &lt;li&gt;Updated the hosting container registry of images, note - this could be a breaking change for users with custom manifests&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/kubeflow/kubeflow/pull/6519&quot;&gt;CVE fixes for Central Dashboard&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The Manifest Working Group contributed several enhancements in 1.6. These enhancements include the testing of each
 Kubeflow’s component manifest for:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/kubeflow/manifests/pull/2230&quot;&gt;Compatibility with K8s 1.22&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/kubeflow/manifests/pull/2230&quot;&gt;The manifests that can be applied&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/kubeflow/manifests/pull/2230&quot;&gt;The Pods of components that can become ready&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/kubeflow/manifests/pull/2249&quot;&gt;The basic objects that can be created&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;For model serving, Kubeflow v1.6 incorporates the KServe v0.8.0 release, which includes these enhancements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ClusterServingRuntime and ServingRuntime CRDs&lt;/li&gt;
  &lt;li&gt;A new Model Spec is introduced to the InferenceService Predictor Spec as a new way to specify models&lt;/li&gt;
  &lt;li&gt;Support for Knative 1.0+&lt;/li&gt;
  &lt;li&gt;gRPC for transformer to predictor network communication&lt;/li&gt;
  &lt;li&gt;Multi-namespace support for the ModelMesh alternative backend&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;KServe has these breaking changes to the Python SDK:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;KFModel is renamed to Model&lt;/li&gt;
  &lt;li&gt;KFServer is renamed to ModelServer&lt;/li&gt;
  &lt;li&gt;KFModelRepository is renamed to ModelRepository&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To learn more please see the &lt;a href=&quot;https://github.com/kserve/kserve/releases/tag/v0.8.0&quot;&gt;KServe v0.8 release notes&lt;/a&gt; and
 Release &lt;a href=&quot;https://kserve.github.io/website/0.9/blog/articles/2022-02-18-KServe-0.8-release/&quot;&gt;blog post&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;join-the-community&quot;&gt;Join the community&lt;/h2&gt;

&lt;p&gt;We would like to thank everyone for their contribution to Kubeflow 1.6, especially Anna Jung for her work as the v1.6 Release Manager. The Kubeflow community is incredibly pleased
 to have Amazon Web Services extending their support by offering &lt;a href=&quot;https://aws.amazon.com/awscredits/&quot;&gt;AWS promotional credits&lt;/a&gt;.
 We hope this sponsorship will enable many Kubeflow Working groups to sustainably host their testing and CI/CD
 infrastructure on AWS, which is essential for maintaining the community’s high development velocity. As you can see,
 the Kubeflow community is vibrant and diverse, solving real-world problems for organizations worldwide.&lt;/p&gt;

&lt;p&gt;Want to help? The Kubeflow community &lt;a href=&quot;https://github.com/kubeflow/community/blob/master/wg-list.md&quot;&gt;Working Group&lt;/a&gt; hold
 open meetings and are always looking for more volunteers and users to unlock the potential of machine learning. If
 you’re interested in becoming a Kubeflow contributor, please feel free to check out the resources below. We look
 forward to working with you!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Visit our &lt;a href=&quot;https://www.kubeflow.org/&quot;&gt;Kubeflow website&lt;/a&gt; or &lt;a href=&quot;https://github.com/kubeflow&quot;&gt;Kubeflow GitHub Page&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Join the &lt;a href=&quot;https://join.slack.com/t/kubeflow/shared_invite/enQtMjgyMzMxNDgyMTQ5LWUwMTIxNmZlZTk2NGU0MmFiNDE4YWJiMzFiOGNkZGZjZmRlNTExNmUwMmQ2NzMwYzk5YzQxOWQyODBlZGY2OTg&quot;&gt;Kubeflow Slack channel&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Join the &lt;a href=&quot;https://groups.google.com/g/kubeflow-discuss&quot;&gt;kubeflow-discuss&lt;/a&gt; mailing list&lt;/li&gt;
  &lt;li&gt;Attend a &lt;a href=&quot;https://www.kubeflow.org/docs/about/community/&quot;&gt;weekly community meeting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Kubeflow 1.6 Release Team</name></author><category term="release" /><summary type="html">The Kubeflow Community is excited to announce the availability of the Kubeflow v1.6 software release, which includes support for Kubernetes v1.22 and introduces an alpha release of the Kubeflow Pipeline v2 (KFP v2) functionality.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.kubeflow.org/images/logo.png" /><media:content medium="image" url="https://blog.kubeflow.org/images/logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Kubeflow 1.5.1 Release Announcement</title><link href="https://blog.kubeflow.org/kubeflow-1.5.1-release/" rel="alternate" type="text/html" title="Kubeflow 1.5.1 Release Announcement" /><published>2022-07-18T00:00:00-05:00</published><updated>2022-07-18T00:00:00-05:00</updated><id>https://blog.kubeflow.org/kubeflow-1.5.1-release</id><content type="html" xml:base="https://blog.kubeflow.org/kubeflow-1.5.1-release/">&lt;p&gt;The Kubeflow Community is excited to announce the release of &lt;a href=&quot;https://github.com/kubeflow/manifests/releases/tag/v1.5.1&quot;&gt;Kubeflow 1.5.1&lt;/a&gt;. The Kubeflow 1.5.1 release provides a valuable enhancement to the caching feature of Kubeflow Pipeline (KFP) for any Kubernetes distribution with strict security policies for user application certificates. Caching is one of the crucial features of KFP which enables skipping a step that has already been executed which saves time and money. This release unblocks the caching feature to be used on distributions like AWS, the Mercedes Benz cluster, etc.&lt;/p&gt;

&lt;p&gt;Kubeflow 1.5.1 includes the enhancements for the Kubeflow Pipelines cache server and the cert generation mechanism &lt;a href=&quot;https://github.com/kubeflow/manifests/issues/2165&quot;&gt;#2165&lt;/a&gt;. These enhancements improve the operations of Kubernetes clusters with strict security policies, especially in clusters where non-Kubelet applications cannot create API server type certificates. Kubeflow Pipelines introduced dependency on cert-manager for this enhancement.&lt;/p&gt;

&lt;p&gt;A number of Kubeflow distribution providers are supporting Kubeflow 1.5.1. These cloud provider and vendor distributions include additional enhancements and professional support, which many users find valuable. Please find a list of the Kubeflow distribution providers &lt;a href=&quot;https://www.kubeflow.org/docs/started/installing-kubeflow/#install-a-packaged-kubeflow-distribution&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Special thanks to Kartik Kalamadi, Suraj Kota, Kimonas Sotirchos and James Liu for driving this effort!&lt;/p&gt;</content><author><name>Kubeflow 1.5 Release Team</name></author><category term="release" /><summary type="html">The Kubeflow Community is excited to announce the release of Kubeflow 1.5.1. The Kubeflow 1.5.1 release provides a valuable enhancement to the caching feature of Kubeflow Pipeline (KFP) for any Kubernetes distribution with strict security policies for user application certificates. Caching is one of the crucial features of KFP which enables skipping a step that has already been executed which saves time and money. This release unblocks the caching feature to be used on distributions like AWS, the Mercedes Benz cluster, etc.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.kubeflow.org/images/logo.png" /><media:content medium="image" url="https://blog.kubeflow.org/images/logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Kubeflow v1.5 improves ML model accuracy, reduces infrastructure costs and optimizes MLOps</title><link href="https://blog.kubeflow.org/kubeflow-1.5-release/" rel="alternate" type="text/html" title="Kubeflow v1.5 improves ML model accuracy, reduces infrastructure costs and optimizes MLOps" /><published>2022-04-01T00:00:00-05:00</published><updated>2022-04-01T00:00:00-05:00</updated><id>https://blog.kubeflow.org/kubeflow-1.5-release</id><content type="html" xml:base="https://blog.kubeflow.org/kubeflow-1.5-release/">&lt;p&gt;The Kubeflow v1.5 software release improves ML model accuracy, lowers infrastructure costs, and simplifies operations by providing a more consistent user experience between components.&lt;/p&gt;

&lt;h2 id=&quot;lower-infrastructure-costs-and-improve-model-accuracy&quot;&gt;Lower infrastructure costs and improve model accuracy&lt;/h2&gt;

&lt;p&gt;Several enhancements in Kubeflow v1.5 lower infrastructure costs and help improve model accuracy.  For example, new elastic training augments the Kubeflow training operator for PyTorch and enables PyTorch workers to be scaled up and down, providing fault tolerant and elastic training. This allows the training jobs to continue without restarting from scratch even if a worker fails. Elastic training can also enable the use of ephemeral or spot instances, which can save infrastructure costs. See the &lt;a href=&quot;https://github.com/kubeflow/community/blob/b56452405e44c05ed60145a19bb86be55f3833d5/proposals/pytorch-elastic-proposal.md&quot;&gt;elastic training proposal&lt;/a&gt; for more details.&lt;/p&gt;

&lt;p&gt;In addition, v1.5 extends notebook monitoring and culling. Kernel monitoring shuts down notebook servers that have been inactive when a configurable timer expires. Kubeflow v1.5 introduces greater precision in assessing notebook server UI activity.&lt;/p&gt;

&lt;p&gt;Finally, v1.5 includes &lt;a href=&quot;https://github.com/kubeflow/katib/pull/1709&quot;&gt;early validation of hyperparameter tuning&lt;/a&gt;.  This feature improves model accuracy by reducing the overfitting of the model to the data sets used during hyperparameter tuning.  It can also reduce the infrastructure used as it will stop hyperparameter tuning when configurable thresholds are reached.&lt;/p&gt;

&lt;h2 id=&quot;simplified-operations&quot;&gt;Simplified operations&lt;/h2&gt;

&lt;p&gt;v1.5 simplifies operations in several ways including easier support of high availability for the Katib Controller via the new hyperparameter leader &lt;a href=&quot;https://github.com/kubeflow/katib/pull/1713&quot;&gt;election&lt;/a&gt;.  v1.5 also adds the MPI framework to the Unified Training Operator, which deploys a single operator for handling the most popular frameworks: Tensorflow, Pytorch, MXNet, XGBoost and MPI.  Kubeflow Pipelines (KFP) has incorporated support for the &lt;a href=&quot;https://www.kubeflow.org/docs/components/pipelines/installation/choose-executor/#emissary-executor&quot;&gt;Emissary Executor&lt;/a&gt; as the default executor, which means that users are not required to manually configure this option to Emissary.  This enhancement also enables KFP to support the newer Kubernetes versions that require a Container Runtime Interface (CRI) and provides support for Docker containers.&lt;/p&gt;

&lt;p&gt;In addition, KServe v0.7 is integrated with Kubeflow v1.5.  For users who are still using KFServing v0.6.1, v1.5 does support the installation of the KFServing v0.6.1 release.  For users migrating from KFServing to KServe, there is helpful documentation, which you can find &lt;a href=&quot;https://kserve.github.io/website/0.7/admin/migration/&quot;&gt;here&lt;/a&gt;.  From a feature standpoint, KServe v0.7 has added a beta of &lt;a href=&quot;https://github.com/kserve/modelmesh-serving&quot;&gt;ModelMesh&lt;/a&gt;, which enables easier scaling of model serving and provides an architecture that overcomes pod and IP limitations that limit the number of models that can run on a single node and/or cluster.&lt;/p&gt;

&lt;p&gt;Note on K8s 1.22: Although many of the Kubeflow services (KFP, AutoML, Training Operator, KServe) have started testing with K8s 1.22, v1.5 was not thoroughly tested with K8s 1.22.   Additionally, critical work is still outstanding for Kubeflow Notebooks and other central services.  While the Community always encourages testing, K8s 1.22 is not formally supported with Kubeflow v1.5.  K8s 1.22 support is on the roadmap for Kubeflow’s next release and if you are interested in K8s 1.22 support, please add a comment on this tracking &lt;a href=&quot;https://github.com/kubeflow/kubeflow/issues/6098&quot;&gt;issue&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;streamlined-user-experience&quot;&gt;Streamlined user experience&lt;/h2&gt;

&lt;p&gt;In v1.5, the Kubeflow User Interface (UI) dashboards for Notebooks, Tensorboards and Volume Manager more closely match KFP’s dashboard.   These enhancements include how dashboard fields are displayed and how operations are performed.  This gives users a consistent look and feel when working in the UI. Additionally, the Kubeflow notebooks manager web app form and its configuration template have been updated for allowing users to fully define the PVC objects that will be created for a notebook. This will give end users the ability to modify crucial parts of the PVCs, such as StorageClasses, and to more easily support popular storage offerings, including NFS. In addition, AutoML has also updated its SDK, CI framework and parameter settings across frameworks (goptuna, optuna, hyperopt).&lt;/p&gt;

&lt;h2 id=&quot;installation-tutorials-and-documentation&quot;&gt;Installation, tutorials and documentation&lt;/h2&gt;

&lt;p&gt;The Kubeflow v1.5 release process includes improvements to Kubeflow’s installation, tutorial and documentation.  For installation, the Manifest Working Group’s &lt;a href=&quot;https://github.com/kubeflow/manifests&quot;&gt;documentation&lt;/a&gt; provided the starting place for the Kubeflow &lt;a href=&quot;https://www.kubeflow.org/docs/started/installing-kubeflow/&quot;&gt;distributions&lt;/a&gt;, and this collaboration has increased the maturity of the code and documentation for Kubeflow’s GitOps installation pattern.  The v1.5 release process included a tracking &lt;a href=&quot;https://github.com/kubeflow/website/issues/3130&quot;&gt;issue&lt;/a&gt; for the critical web pages that are updated in each release.  Additionally, there are many Working Group documentation updates and we invite you to open issues when you find documentation that needs improvement.  Kubeflow v1.5 also includes two tutorials, which are described below:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tutorial 1&lt;/strong&gt; - &lt;a href=&quot;https://github.com/kubeflow/pipelines/blob/master/samples/contrib/kubeflow-e2e-mnist/kubeflow-e2e-mnist.ipynb&quot;&gt;E2E MNIST with Kubeflow Tutorial&lt;/a&gt;,which provides an end-to-end test sequence i.e. start a notebook, run a pipeline, execute training, hyperparameter tuning and model serving.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tutorial 2&lt;/strong&gt; - &lt;a href=&quot;https://www.kubeflow.org/docs/external-add-ons/kserve/first_isvc_kserve/&quot;&gt;Deploy and run your first interfere service with KServe Tutorial&lt;/a&gt; Run your first interface service on Kubeflow 1.5 i.e. define, create and check an InferenceService, post an interfere request and receive response.  Optionally run performance tests.&lt;/p&gt;

&lt;h2 id=&quot;join-the-community&quot;&gt;Join the community&lt;/h2&gt;

&lt;p&gt;The v1.5 Release Team would like to thank everyone for their efforts on Kubeflow v1.5, especially the users, code contributors, working group leads, and distribution team leads. As you can see from the extensive contributions to Kubeflow 1.5, the Kubeflow Community is vibrant and diverse, and solving real world problems for organizations around the globe.&lt;/p&gt;

&lt;p&gt;Excited to contribute your great ideas? The Kubeflow Community &lt;a href=&quot;https://github.com/kubeflow/community/blob/master/wg-list.md&quot;&gt;Working Groups&lt;/a&gt; hold open meetings, public lists, and are always looking for more volunteers and users to unlock the potential of machine learning. If you’re interested in becoming a Kubeflow contributor, please feel free to check out the resources below. We look forward to working with you!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Visit our &lt;a href=&quot;https://www.kubeflow.org/&quot;&gt;Kubeflow website&lt;/a&gt; or &lt;a href=&quot;https://github.com/kubeflow&quot;&gt;Kubeflow Github page&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Join the &lt;a href=&quot;https://join.slack.com/t/kubeflow/shared_invite/enQtMjgyMzMxNDgyMTQ5LWUwMTIxNmZlZTk2NGU0MmFiNDE4YWJiMzFiOGNkZGZjZmRlNTExNmUwMmQ2NzMwYzk5YzQxOWQyODBlZGY2OTg&quot;&gt;Kubeflow Slack Channel&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Join the &lt;a href=&quot;https://groups.google.com/forum/#!forum/kubeflow-discuss&quot;&gt;kubeflow-discuss&lt;/a&gt; mailing list&lt;/li&gt;
  &lt;li&gt;Attend a &lt;a href=&quot;https://www.kubeflow.org/docs/about/community/&quot;&gt;weekly community meeting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Kubeflow 1.5 Release Team</name></author><category term="release" /><summary type="html">The Kubeflow v1.5 software release improves ML model accuracy, lowers infrastructure costs, and simplifies operations by providing a more consistent user experience between components.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.kubeflow.org/images/logo.png" /><media:content medium="image" url="https://blog.kubeflow.org/images/logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Unified Training Operator release announcement</title><link href="https://blog.kubeflow.org/unified-training-operator-1.3-release/" rel="alternate" type="text/html" title="Unified Training Operator release announcement" /><published>2021-10-13T00:00:00-05:00</published><updated>2021-10-13T00:00:00-05:00</updated><id>https://blog.kubeflow.org/unified-training-operator-release</id><content type="html" xml:base="https://blog.kubeflow.org/unified-training-operator-1.3-release/">&lt;p&gt;The Kubeflow Training Operator Working Group introduced several enhancements in the recent Kubeflow 1.4 release. The most significant was the introduction of the new unified &lt;a href=&quot;https://github.com/kubeflow/training-operator&quot;&gt;training operator&lt;/a&gt; that enables Kubernetes custom resources (CR) for many of the popular training frameworks: Tensorflow, Pytorch, MXNet and XGboost. In addition, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf-operator&lt;/code&gt; repository has been renamed to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;training-operator&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;This single operator provides several valuable benefits:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Better resource utilisation&lt;/strong&gt; - For  releases prior to 1.4, each framework had a separate corresponding controller managing its distributed job. The unified training operator manages all distributed jobs across frameworks, which improves resource utilization and performance.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Less maintenance overhead&lt;/strong&gt; - Unified training operator reduces the maintenance efforts in managing distributed jobs across the framework. By default, all supported schemas(TFJob, PyTorchJob, MXNetJob, XGBoostJob) are enabled. However, specific schemas can be enabled using the flag ‘enable-scheme’. Setting this flag enables the user to enable the framework(s) that are necessary for the deployment environment.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Easy adoption of new operators&lt;/strong&gt; - Common code is abstracted from all framework implementations, which makes it easy for adopting new operators with less code. The common infrastructure &lt;a href=&quot;https://github.com/kubeflow/common&quot;&gt;code&lt;/a&gt; can be reused for many of the new operator efforts. Reference: Paddle operator &lt;a href=&quot;https://github.com/kubeflow/community/pull/502&quot;&gt;proposal&lt;/a&gt;, DGL operator &lt;a href=&quot;https://github.com/kubeflow/community/pull/512&quot;&gt;proposal&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Better developer experience&lt;/strong&gt; - Common features can be shared across frameworks without code duplication thereby, creating a developer friendly environment. For example, Prometheus Monitoring and Job Scheduling features are common, making them available to all frameworks without any extra code.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The unified training operator’s &lt;a href=&quot;https://github.com/kubeflow/manifests/tree/v1.4-branch/apps/training-operator/upstream&quot;&gt;manifests&lt;/a&gt; include an enhanced training operator, which manages custom resource definitions for TFJob, PyTorchJob, MXNet Job and  XGBoostJob. All individual operator repositories, including &lt;a href=&quot;https://github.com/kubeflow/pytorch-operator&quot;&gt;pytorch-operator&lt;/a&gt;, &lt;a href=&quot;https://github.com/kubeflow/mxnet-operator&quot;&gt;mxnet-operator&lt;/a&gt;, &lt;a href=&quot;https://github.com/kubeflow/xgboost-operator&quot;&gt;xgboost-operator&lt;/a&gt;, will be archived soon. Please check out the latest &lt;a href=&quot;https://github.com/kubeflow/training-operator/releases/tag/v1.3.0&quot;&gt;release&lt;/a&gt; for more details and give it a try!&lt;/p&gt;

&lt;h2 id=&quot;release-highlights&quot;&gt;Release highlights&lt;/h2&gt;

&lt;p&gt;Kubeflow 1.4 release includes the following major changes to training.&lt;/p&gt;

&lt;h3 id=&quot;universal-training-operator-changes&quot;&gt;Universal Training Operator changes&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Unified Training Operator for TF, PyTorch, MXNet, XGBoost &lt;a href=&quot;https://github.com/kubeflow/tf-operator/pull/1302&quot;&gt;#1302&lt;/a&gt; &lt;a href=&quot;https://github.com/kubeflow/tf-operator/pull/1295&quot;&gt;#1295&lt;/a&gt; &lt;a href=&quot;https://github.com/kubeflow/tf-operator/pull/1294&quot;&gt;#1294&lt;/a&gt; &lt;a href=&quot;https://github.com/kubeflow/tf-operator/pull/1293&quot;&gt;#1293&lt;/a&gt; &lt;a href=&quot;https://github.com/kubeflow/tf-operator/pull/1296&quot;&gt;#1296&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;More common code refactoring for reusability &lt;a href=&quot;https://github.com/kubeflow/tf-operator/pull/1297&quot;&gt;#1297&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;API code restructuring to consistent format &lt;a href=&quot;https://github.com/kubeflow/tf-operator/pull/1300&quot;&gt;#1300&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Prometheus counters for all frameworks &lt;a href=&quot;https://github.com/kubeflow/tf-operator/pull/1375&quot;&gt;#1375&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Python SDK for all frameworks &lt;a href=&quot;https://github.com/kubeflow/tf-operator/pull/1420&quot;&gt;#1420&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;API doc for all frameworks &lt;a href=&quot;https://github.com/kubeflow/tf-operator/pull/1370&quot;&gt;#1370&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Restructuring of examples across all frameworks &lt;a href=&quot;https://github.com/kubeflow/tf-operator/pull/1373&quot;&gt;#1373&lt;/a&gt; &lt;a href=&quot;https://github.com/kubeflow/tf-operator/pull/1391&quot;&gt;#1391&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;common-package-updates&quot;&gt;Common package updates&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Make training container port customizable to support profiling &lt;a href=&quot;https://github.com/kubeflow/common/pull/131&quot;&gt;#131&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Optimize the TTL setting of all Jobs &lt;a href=&quot;https://github.com/kubeflow/common/pull/137&quot;&gt;#137&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;More appropriate use of expectation for Jobs &lt;a href=&quot;https://github.com/kubeflow/common/pull/139&quot;&gt;#139&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;mpi-operator-updates&quot;&gt;MPI Operator updates&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Scalability improvements to reduce pressure on kube-apiserver &lt;a href=&quot;https://github.com/kubeflow/mpi-operator/pull/360&quot;&gt;#360&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;V2beta1 MPIJob API &lt;a href=&quot;https://github.com/kubeflow/mpi-operator/pull/366&quot;&gt;#366&lt;/a&gt; &lt;a href=&quot;https://github.com/kubeflow/mpi-operator/pull/378&quot;&gt;#378&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Intel MPI Support &lt;a href=&quot;https://github.com/kubeflow/mpi-operator/pull/389&quot;&gt;#389&lt;/a&gt; &lt;a href=&quot;https://github.com/kubeflow/mpi-operator/pull/403&quot;&gt;#403&lt;/a&gt; &lt;a href=&quot;https://github.com/kubeflow/mpi-operator/pull/417&quot;&gt;#417&lt;/a&gt; &lt;a href=&quot;https://github.com/kubeflow/mpi-operator/pull/425&quot;&gt;#425&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;mpi-operator-roadmap&quot;&gt;MPI Operator roadmap&lt;/h2&gt;

&lt;p&gt;The MPI framework integration with the unified training operator is under development and is planned for delivery in the next release i.e. post 1.4. Currently, it needs to be separately installed using MPIJob &lt;a href=&quot;https://github.com/kubeflow/manifests/tree/v1.4-branch/apps/mpi-job/upstream&quot;&gt;manifests&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgement&quot;&gt;Acknowledgement&lt;/h2&gt;

&lt;p&gt;The unified training operator is the outcome of efforts from all existing Kubeflow training operators and aims to provide a unified and simplified experience for both users and developers. We’d like to thank everyone who has contributed to and maintained the original operators.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;PyTorch Operator: &lt;a href=&quot;https://github.com/kubeflow/pytorch-operator/graphs/contributors&quot;&gt;list of contributors&lt;/a&gt; and &lt;a href=&quot;https://github.com/kubeflow/pytorch-operator/blob/master/OWNERS&quot;&gt;maintainers&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;MPI Operator: &lt;a href=&quot;https://github.com/kubeflow/mpi-operator/graphs/contributors&quot;&gt;list of contributors&lt;/a&gt; and &lt;a href=&quot;https://github.com/kubeflow/mpi-operator/blob/master/OWNERS&quot;&gt;maintainers&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;XGBoost Operator: &lt;a href=&quot;https://github.com/kubeflow/xgboost-operator/graphs/contributors&quot;&gt;list of contributors&lt;/a&gt; and &lt;a href=&quot;https://github.com/kubeflow/xgboost-operator/blob/master/OWNERS&quot;&gt;maintainers&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;MXNet Operator: &lt;a href=&quot;https://github.com/kubeflow/mxnet-operator/graphs/contributors&quot;&gt;list of contributors&lt;/a&gt; and &lt;a href=&quot;https://github.com/kubeflow/mxnet-operator/blob/master/OWNERS&quot;&gt;maintainers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;join-the-wg-training&quot;&gt;Join the WG-Training&lt;/h2&gt;

&lt;p&gt;If you want to help, or are looking for issues to work on, feel free to check the resources below!&lt;/p&gt;

&lt;p&gt;Slack: &lt;a href=&quot;https://kubeflow.slack.com/archives/C018N3M6QKB&quot;&gt;#wg-training&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Community: &lt;a href=&quot;https://github.com/kubeflow/community/tree/master/wg-training&quot;&gt;wg-training&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Issues: https://github.com/kubeflow/training-operator/issues&lt;/p&gt;</content><author><name>Johnu George, Jiaxin Shan, Josh Bottum</name></author><category term="release" /><summary type="html">The Kubeflow Training Operator Working Group introduced several enhancements in the recent Kubeflow 1.4 release. The most significant was the introduction of the new unified training operator that enables Kubernetes custom resources (CR) for many of the popular training frameworks: Tensorflow, Pytorch, MXNet and XGboost. In addition, the tf-operator repository has been renamed to training-operator.</summary></entry><entry><title type="html">Kubeflow’s 1.4 release lays the foundation for advanced ML metadata workflows</title><link href="https://blog.kubeflow.org/kubeflow-1.4-release/" rel="alternate" type="text/html" title="Kubeflow’s 1.4 release lays the foundation for advanced ML metadata workflows" /><published>2021-10-12T00:00:00-05:00</published><updated>2021-10-12T00:00:00-05:00</updated><id>https://blog.kubeflow.org/kubeflow-1.4-release</id><content type="html" xml:base="https://blog.kubeflow.org/kubeflow-1.4-release/">&lt;p&gt;The Kubeflow 1.4 release lays several important building blocks for the use of advanced metadata workflows. A quick summary of
1.4’s top deliveries includes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Advanced metadata workflows with improved metric visualization and pipeline
step &lt;a href=&quot;https://www.kubeflow.org/docs/components/pipelines/caching-v2/&quot;&gt;caching&lt;/a&gt;
in Kubeflow Pipelines (KFP) via the KFP Software Development Kit (SDK)&lt;/li&gt;
  &lt;li&gt;A new KFServing
&lt;a href=&quot;https://www.kubeflow.org/docs/components/kfserving/webapp/&quot;&gt;model user interface&lt;/a&gt;
that displays ML model status, configuration, yaml, logs, and metrics&lt;/li&gt;
  &lt;li&gt;New &lt;a href=&quot;https://github.com/optuna/optuna&quot;&gt;Optuna&lt;/a&gt; Suggestion Service with
multivariate TPE algorithm and Sobol’s
&lt;a href=&quot;https://github.com/kubeflow/katib/pull/1523&quot;&gt;Quasirandom&lt;/a&gt; Sequence support
for hyperparameter tuning&lt;/li&gt;
  &lt;li&gt;A new, unified training operator that supports all deep learning frameworks
with a Python SDK, enhanced monitoring and advanced scheduling support&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Kubeflow 1.4 enables the use of metadata in advanced machine learning (ML) workflows, especially
in the Kubeflow Pipelines SDK. With the Pipelines SDK and its new
&lt;a href=&quot;https://www.kubeflow.org/docs/components/pipelines/sdk/v2/v2-compatibility/&quot;&gt;V2-compatible mode&lt;/a&gt;,
users can create advanced ML pipelines with Python functions that use the &lt;a href=&quot;https://github.com/google/ml-metadata&quot;&gt;MLMD&lt;/a&gt; as
input/output arguments. This simplifies
&lt;a href=&quot;https://www.kubeflow.org/docs/components/pipelines/sdk/output-viewer/#v2-sdk-use-sdk-visualization-apis&quot;&gt;metrics visualization&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Another enhancement to Pipelines is the option to use the
&lt;a href=&quot;https://www.kubeflow.org/docs/components/pipelines/installation/choose-executor/&quot;&gt;Emissary executor&lt;/a&gt;
for non-Docker Kubernetes container runtime requirements. In addition, 1.4 can 
support metadata-based workflows to streamline the creation of TensorBoard 
visualizations and to serve ML models.&lt;/p&gt;

&lt;h2 id=&quot;core-improvements-to-code-process-and-documentation&quot;&gt;Core improvements to code, process, and documentation&lt;/h2&gt;

&lt;p&gt;For the Kubeflow Working Groups, 1.4 was primarily a maintenance release, 
which enabled the Community to concentrate on core improvements to code, 
process, and documentation. In the
&lt;a href=&quot;https://blog.kubeflow.org/kubeflow-continues-to-move-to-production&quot;&gt;2021 Kubeflow User Survey&lt;/a&gt;,
users requested documentation improvements (please see the figure below). The
Kubeflow 1.4 release cycle included the 1.4 Docs Sprint that generated nearly
fifty (50) PRs. These PRs were tracked in this
&lt;a href=&quot;https://github.com/kubeflow/website/issues/2879&quot;&gt;issue&lt;/a&gt; and this
&lt;a href=&quot;https://github.com/orgs/kubeflow/projects/46&quot;&gt;Kanban&lt;/a&gt; board, and we encourage
more users to contribute by reading and improving the Kubeflow documentation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-10-12-kubeflow-1.4-release/docs-sprint.png&quot; alt=&quot;docs sprint&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The 1.4 release improvements simplify future feature development by reducing
redundant code, increasing CI/CD, and automating testing. An important delivery
was the new Unified Training Operator for Tensorflow, PyTorch, MXNet, and
XGBoost &lt;a href=&quot;https://github.com/kubeflow/tf-operator/pull/1302&quot;&gt;PR#1302&lt;/a&gt;. 1.4 also
initiated the Community’s adoption of a defined release process in its new
Kubeflow &lt;a href=&quot;https://github.com/kubeflow/manifests/pull/1907&quot;&gt;Release Handbook&lt;/a&gt;.
The Handbook defines the stages of the release and contributors’ roles, which
has helped to improve responsibilities and quality.&lt;/p&gt;

&lt;h2 id=&quot;simplified-installation&quot;&gt;Simplified installation&lt;/h2&gt;

&lt;p&gt;As shown in the Kubeflow User Survey (see the figure above), users have also
asked for installation improvements. In Kubeflow 1.3, the Community refactored
the Kubeflow deployment pattern to use manifests files (in yaml or json), which
are stored in Git repositories, and then deployed using the Kustomize
installation tool. This flexible installation pattern simplifies customization
by overlaying manifests. This pattern is now being exploited in 1.4.&lt;/p&gt;

&lt;p&gt;In 1.4, the Community provides an upstream set of base manifests in the
Kubeflow manifest repo. Third parties have built custom installation guides
or distributions with overlays that extend the base manifests. In 1.4, the
third party overlays were removed from the Kubeflow
&lt;a href=&quot;https://github.com/kubeflow/manifests&quot;&gt;manifest repo&lt;/a&gt; and moved to the
repository of their choosing. This pattern provides third parties more
flexibility to upgrade and document their overlays. You can see a full set of
installation guides and distributions
&lt;a href=&quot;https://www.kubeflow.org/docs/started/installing-kubeflow/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In addition, on-prem Kubeflow users can use the base installation manifests
which utilize open source solutions like &lt;a href=&quot;https://istio.io/&quot;&gt;Istio&lt;/a&gt;,
&lt;a href=&quot;https://dexidp.io/&quot;&gt;Dex&lt;/a&gt;, and
&lt;a href=&quot;https://github.com/arrikto/oidc-authservice&quot;&gt;AuthService&lt;/a&gt; for authentication.
The Community and the Manifests Working Group are actively working to provide
extra overlays and patches to accommodate more advanced use cases and
installations. For example, we recently
&lt;a href=&quot;https://github.com/kubeflow/manifests/pull/2048&quot;&gt;configured&lt;/a&gt; Knative to work
with the AuthService and Dex.&lt;/p&gt;

&lt;h2 id=&quot;dependencies-change-logs-tracking-issues-and-roadmaps&quot;&gt;Dependencies, change logs, tracking issues and roadmaps&lt;/h2&gt;

&lt;p&gt;Kubeflow has many software dependencies. In 1.4, the top dependencies used in
testing are defined below:&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;strong&gt;Kubeflow Dependency&lt;/strong&gt;&lt;/td&gt;
    &lt;td&gt;&lt;strong&gt;Version&lt;/strong&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Kubernetes&lt;/td&gt;
    &lt;td&gt;1.19.0&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Istio&lt;/td&gt;
    &lt;td&gt;1.9.6&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Knative&lt;/td&gt;
    &lt;td&gt;0.22.1&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Kustomize&lt;/td&gt;
    &lt;td&gt;3.2.0&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;This chart provides links to important details from the Working Groups,
including their 1.4 tracking issues, change logs, and roadmaps. Please 
note that the Working Groups use version numbers that are specific to 
their project. As a result, many Kubeflow components, which have been 
incorporated and tested in Kubeflow 1.4, may have a different version 
number than 1.4.&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;strong&gt;Working Group&lt;/strong&gt;&lt;/td&gt;
    &lt;td&gt;&lt;strong&gt;Changelog / Release Notes&lt;/strong&gt;&lt;/td&gt;
    &lt;td&gt;&lt;strong&gt;Roadmap&lt;/strong&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;strong&gt;Notebooks&lt;/strong&gt;&lt;/td&gt;
    &lt;td&gt;
      1.4,&lt;br /&gt;
      &lt;a href=&quot;https://github.com/kubeflow/kubeflow/releases/tag/v1.4.0&quot;&gt;
        releases/tag/v1.4.0
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;a href=&quot;https://github.com/kubeflow/kubeflow/issues/5978&quot;&gt;
        kubeflow/issues/5978
      &lt;/a&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;strong&gt;Training Operators&lt;/strong&gt;&lt;/td&gt;
    &lt;td&gt;
      1.3,&lt;br /&gt;
      &lt;a href=&quot;https://github.com/kubeflow/training-operator/blob/master/CHANGELOG.md&quot;&gt;
        Training Operator Changelog
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;a href=&quot;https://github.com/kubeflow/common/blob/master/ROADMAP.md&quot;&gt;
        Training Operators Roadmap
      &lt;/a&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;strong&gt;Katib&lt;/strong&gt;&lt;/td&gt;
    &lt;td&gt;
      V0.12,&lt;br /&gt;
      &lt;a href=&quot;https://github.com/kubeflow/katib/releases/tag/v0.12.0&quot;&gt;
        Katib Release Notes
      &lt;/a&gt;&lt;br /&gt;
      &lt;a href=&quot;https://github.com/kubeflow/katib/blob/aa452f07eb8a1f395b76d63fa233a2a01aacdeba/CHANGELOG.md&quot;&gt;
        PR for v0.12  
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;a href=&quot;https://github.com/kubeflow/katib/blob/master/ROADMAP.md&quot;&gt;
        Katib Roadmap
      &lt;/a&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;strong&gt;Kubeflow Pipelines&lt;/strong&gt;&lt;/td&gt;
    &lt;td&gt;
      v1.7&lt;br /&gt;
      &lt;a href=&quot;https://github.com/kubeflow/pipelines/releases/tag/1.7.0&quot;&gt;
        Release Notes, Changelog
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;a href=&quot;https://github.com/kubeflow/pipelines/blob/master/ROADMAP.md&quot;&gt;
        Pipelines Roadmap
      &lt;/a&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;strong&gt;KFServing&lt;/strong&gt;&lt;/td&gt;
    &lt;td&gt;
      v0.6.1,&lt;br /&gt;
      &lt;a href=&quot;https://github.com/kubeflow/kfserving/releases/tag/v0.6.1&quot;&gt;
        kfserving/releases/tag/v0.6.1
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;a href=&quot;https://github.com/kubeflow/kfserving/blob/master/ROADMAP.md&quot;&gt;
        KFServing Roadmap
      &lt;/a&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h2 id=&quot;kubeflow-14-video-update-and-tutorials&quot;&gt;Kubeflow 1.4 video update and tutorials&lt;/h2&gt;

&lt;p&gt;The Kubeflow Working Group representatives have recorded a
&lt;a href=&quot;https://youtu.be/gG61gHw4J14&quot;&gt;presentation&lt;/a&gt; on Kubeflow 1.4’s new
features, which you can find on the
&lt;a href=&quot;https://www.youtube.com/kubeflow&quot;&gt;Kubeflow YouTube channel&lt;/a&gt;. Additionally,
Kubeflow 1.4’s new features are easy to try in these tutorials:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;AutoML &lt;a href=&quot;https://codelabs.arrikto.com/codelabs/minikf-kale-automl/index.html?index=../..index#0&quot;&gt;Tutorial&lt;/a&gt;
with metadata based workflows to build TensorBoards and to serve models&lt;/li&gt;
  &lt;li&gt;Run Katib from your &lt;strong&gt;local laptop&lt;/strong&gt; by following &lt;a href=&quot;https://github.com/kubeflow/katib/tree/master/examples/v1beta1/kind-cluster&quot;&gt;this example&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;KFP &lt;a href=&quot;https://www.kubeflow.org/docs/components/pipelines/sdk/v2/build-pipeline/&quot;&gt;Tutorial&lt;/a&gt;
using Pipelines SDK v2 to orchestrate your ML workflow as a pipeline&lt;/li&gt;
  &lt;li&gt;KFServing &lt;a href=&quot;https://github.com/kserve/kserve/tree/release-0.6/docs/samples&quot;&gt;Tutorial&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Training Operator &lt;a href=&quot;https://github.com/kubeflow/training-operator/tree/master/examples&quot;&gt;Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;whats-coming&quot;&gt;What’s coming&lt;/h2&gt;

&lt;p&gt;The Kubeflow Community is working on Kubeflow 1.5 planning and the Kubeflow
Conformance Program &lt;a href=&quot;https://github.com/kubeflow/community/pull/524&quot;&gt;proposal&lt;/a&gt;.
Please watch &lt;a href=&quot;https://blog.kubeflow.org/&quot;&gt;blog.kubeflow.org&lt;/a&gt; for updates on
these topics and more.&lt;/p&gt;

&lt;h2 id=&quot;join-the-community&quot;&gt;Join the community&lt;/h2&gt;

&lt;p&gt;We would like to thank everyone for their efforts on Kubeflow 1.4, especially
the users, code contributors and working group leads. As you can see from the
extensive contributions to Kubeflow 1.4, the Kubeflow Community is vibrant and
diverse, and solving real world problems for organizations around the world.&lt;/p&gt;

&lt;p&gt;Want to help? The Kubeflow Community
&lt;a href=&quot;https://github.com/kubeflow/community/blob/master/wg-list.md&quot;&gt;Working Groups&lt;/a&gt;
hold open meetings, public lists, and are always looking for more volunteers
and users to unlock the potential of machine learning. If you’re interested in
becoming a Kubeflow contributor, please feel free to check out the resources
below. We look forward to working with you!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Visit our &lt;a href=&quot;https://www.kubeflow.org/&quot;&gt;Kubeflow website&lt;/a&gt; or
&lt;a href=&quot;https://github.com/kubeflow&quot;&gt;Kubeflow GitHub page&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Join the &lt;a href=&quot;https://join.slack.com/t/kubeflow/shared_invite/enQtMjgyMzMxNDgyMTQ5LWUwMTIxNmZlZTk2NGU0MmFiNDE4YWJiMzFiOGNkZGZjZmRlNTExNmUwMmQ2NzMwYzk5YzQxOWQyODBlZGY2OTg&quot;&gt;Kubeflow Slack channel&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Join the
&lt;a href=&quot;https://groups.google.com/forum/#!forum/kubeflow-discuss&quot;&gt;kubeflow-discuss&lt;/a&gt;
mailing list&lt;/li&gt;
  &lt;li&gt;Attend a
&lt;a href=&quot;https://www.kubeflow.org/docs/about/community/&quot;&gt;weekly community meeting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Josh Bottum, Kimonas Sotirchos, Thea Lamkin</name></author><category term="release" /><summary type="html">The Kubeflow 1.4 release lays several important building blocks for the use of advanced metadata workflows. A quick summary of 1.4’s top deliveries includes:</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.kubeflow.org/images/logo.png" /><media:content medium="image" url="https://blog.kubeflow.org/images/logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">KServe: The next generation of KFServing</title><link href="https://blog.kubeflow.org/release/official/2021/09/27/kfserving-transition.html" rel="alternate" type="text/html" title="KServe: The next generation of KFServing" /><published>2021-09-27T00:00:00-05:00</published><updated>2021-09-27T00:00:00-05:00</updated><id>https://blog.kubeflow.org/release/official/2021/09/27/kfserving-transition</id><content type="html" xml:base="https://blog.kubeflow.org/release/official/2021/09/27/kfserving-transition.html">&lt;h4 id=&quot;by-dan-sun-and-animesh-singh-on-behalf-of-the-kubeflow-serving-working-group&quot;&gt;By &lt;strong&gt;Dan Sun&lt;/strong&gt; and &lt;strong&gt;Animesh Singh&lt;/strong&gt; on behalf of the Kubeflow Serving Working Group&lt;/h4&gt;

&lt;h3 id=&quot;kfserving-is-now-kserve&quot;&gt;&lt;strong&gt;KFServing is now KServe&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;We are excited to announce the next chapter for KFServing.
In coordination with the Kubeflow Project Steering Group, the &lt;a href=&quot;https://github.com/kubeflow/kfserving&quot;&gt;&lt;u&gt;KFServing GitHub repository&lt;/u&gt;&lt;/a&gt; has now been
transferred to an independent &lt;a href=&quot;https://github.com/kserve/kserve&quot;&gt;&lt;u&gt;KServe GitHub organization&lt;/u&gt;&lt;/a&gt; under the stewardship of the Kubeflow Serving Working
Group leads.&lt;/p&gt;

&lt;p&gt;The project has been rebranded from &lt;strong&gt;KFServing&lt;/strong&gt; to &lt;strong&gt;KServe&lt;/strong&gt;, and we are planning to graduate the project from Kubeflow Project later this year.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-09-27-kfserving-transition/image1.png&quot; style=&quot;width:6.5in;height:4in&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Developed collaboratively by Google, IBM, Bloomberg, NVIDIA, and Seldon in 2019, KFServing was published as open source in early 2019. 
The project sets out to provide the following features:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A simple, yet powerful, Kubernetes Custom Resource for deploying machine learning (ML) models on production across ML frameworks.&lt;/li&gt;
  &lt;li&gt;Provide performant, standardized inference protocol.&lt;/li&gt;
  &lt;li&gt;Serverless inference according to live traffic patterns, supporting “Scale-to-zero” on both CPUs and GPUs.&lt;/li&gt;
  &lt;li&gt;Complete story for production ML Model Serving including prediction, pre/post-processing, explainability, and monitoring.&lt;/li&gt;
  &lt;li&gt;Support for deploying thousands of models at scale and inference graph capability for multiple models.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;KFServing was created to address the challenges of deploying and monitoring machine learning models on production for organizations.
After publishing the open source project, we’ve seen an explosion in demand for the software, leading to strong adoption and community growth.
The scope of the project has since increaded, and we have developed multiple components along the way, including our own growing body of documentation
that needs it’s own website and independent GitHub organization.&lt;/p&gt;

&lt;h3 id=&quot;whats-next&quot;&gt;&lt;strong&gt;What’s Next&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Over the coming weeks, we will be releasing &lt;strong&gt;KServe 0.7&lt;/strong&gt; outside of the Kubeflow Project and will provide more details on how to migrate from KFServing to
KServe with minimal disruptions. KFServing 0.5.x/0.6.x releases are still supported in next six months after KServe 0.7 release. We are also working on
integrating core Kubeflow APIs and standards for &lt;a href=&quot;https://docs.google.com/document/d/1a9ufoe_6DB1eSjpE9eK5nRBoH3ItoSkbPfxRA0AjPIc&quot;&gt;the conformance program&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For contributors, please follow the KServe &lt;a href=&quot;https://github.com/kserve/website/blob/main/docs/developer/developer.md&quot;&gt;developer&lt;/a&gt; and 
&lt;a href=&quot;https://github.com/kserve/website/blob/main/docs/help/contributor/mkdocs-contributor-guide.md&quot;&gt;doc contribution&lt;/a&gt; guide to make code or doc contributions.
We are excited to work with you to make KServe better and promote its adoption by more and more users!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-09-27-kfserving-transition/kserve.png&quot; style=&quot;width:8in;height:4in&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;kserve-key-links&quot;&gt;&lt;strong&gt;KServe Key Links&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://kserve.github.io/website/&quot;&gt;&lt;u&gt;Website&lt;/u&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/kserve/kserve/&quot;&gt;&lt;u&gt;Github&lt;/u&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kubeflow.slack.com/join/shared_invite/zt-n73pfj05-l206djXlXk5qdQKs4o1Zkg#/&quot;&gt;&lt;u&gt;Slack(#kubeflow-kfserving)&lt;/u&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;contributor-acknowledgement&quot;&gt;&lt;strong&gt;Contributor Acknowledgement&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;We’d like to thank all the KServe contributors for this transition work!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/andyi2it&quot;&gt;Andrews Arokiam&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/animeshsingh&quot;&gt;Animesh Singh&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/chinhuang007&quot;&gt;Chin Huang&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://github.com/yuzisun&quot;&gt;Dan Sun&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/jagadeeshi2i&quot;&gt;Jagadeesh&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/jinchihe&quot;&gt;Jinchi He&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/njhill&quot;&gt;Nick Hill&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/pvaneck&quot;&gt;Paul Van Eck&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/Iamlovingit&quot;&gt;Qianshan Chen&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/Suresh-Nakkeran&quot;&gt;Suresh Nakkiran&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/sukumargaonkar&quot;&gt;Sukumar Gaonkar&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/theofpa&quot;&gt;Theofilos Papapanagiotou&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/Tomcli&quot;&gt;Tommy Li&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/js-ts&quot;&gt;Vedant Padwal&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/PatrickXYS&quot;&gt;Yao Xiao&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/yuzliu&quot;&gt;Yuzhui Liu&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="release" /><category term="official" /><summary type="html">By Dan Sun and Animesh Singh on behalf of the Kubeflow Serving Working Group</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.kubeflow.org/images/2021-09-27-kserve-transition/image1.png" /><media:content medium="image" url="https://blog.kubeflow.org/images/2021-09-27-kserve-transition/image1.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Blog: Running Kubeflow at Intuit: Enmeshed in the service mesh</title><link href="https://blog.kubeflow.org/running-kubeflow-at-intuit/" rel="alternate" type="text/html" title="Blog: Running Kubeflow at Intuit: Enmeshed in the service mesh" /><published>2021-05-03T00:00:00-05:00</published><updated>2021-05-03T00:00:00-05:00</updated><id>https://blog.kubeflow.org/kubeflow-1.3-install-post</id><content type="html" xml:base="https://blog.kubeflow.org/running-kubeflow-at-intuit/">&lt;p&gt;Deploying Kubeflow 1.3 in an enterprise with existing Kubernetes infrastructure, a Service Mesh (Istio), and Argo, presents a host of challenges.
This blog will address how those challenges were overcome while retaining the best practices that both the organization and Kubeflow prescribe.&lt;/p&gt;

&lt;h2 id=&quot;lay-of-the-land-at-intuit&quot;&gt;Lay of the land at Intuit&lt;/h2&gt;

&lt;p&gt;Intuit has invested heavily in building out a robust Kubernetes infrastructure that powers all of Intuit’s products: TurboTax, QuickBooks, and Mint. There are thousands of services that run on over a hundred Kubernetes clusters. Managing these clusters is the Intuit Kubernetes Service (IKS) control plane. The IKS control plane provides services such as namespace management, role management, and isolation, etc. Connecting the services is an advanced, Istio-based service mesh, which complements Intuit’s API Gateway. In combination, they provide robust authentication, authorization, rate limiting, and other routing capabilities.&lt;/p&gt;

&lt;p&gt;The Intuit ML Platform is built on this ecosystem and provides model training, inference, and feature management capabilities, leveraging the best of Intuit’s Kubernetes infrastructure and AWS SageMaker. This is the backdrop against which we started exploring Kubeflow to provide advanced orchestration, experimentation, and other services.&lt;/p&gt;

&lt;h2 id=&quot;kubeflow-and-istio&quot;&gt;Kubeflow and Istio&lt;/h2&gt;

&lt;p&gt;Our first challenge with running Kubeflow was the compatibility of Kubeflow’s Istio with Intuit’s existing Service Mesh built on top of Istio. Two key problems emerged: version compatibility and operational maintenance.&lt;/p&gt;

&lt;p&gt;Kubeflow v1.3 defaults to Istio (v1.9), and luckily it is compatible with the older versions of Istio (v1.6), which is what Intuit runs on. Running two Istio versions is impractical, as that would defeat the benefit of a large, interconnected existing service mesh. Hence, we wanted Kubeflow to work seamlessly with Intuit’s service mesh running Istio v1.6.&lt;/p&gt;

&lt;p&gt;If you are new to Istio, you might want a primer on these key &lt;a href=&quot;https://istio.io/latest/docs/reference/config/networking/&quot;&gt;Traffic Management Components&lt;/a&gt; and &lt;a href=&quot;https://istio.io/latest/docs/reference/config/security/&quot;&gt;Security Components&lt;/a&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;VirtualService&lt;/li&gt;
  &lt;li&gt;DestinationRule&lt;/li&gt;
  &lt;li&gt;Gateway&lt;/li&gt;
  &lt;li&gt;EnvoyFilter&lt;/li&gt;
  &lt;li&gt;AuthorizationPolicy&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;step-1-remove-default-istio-configurations-and-argo-from-kubeflow&quot;&gt;Step 1: Remove default Istio configurations and Argo from Kubeflow&lt;/h3&gt;

&lt;p&gt;The first step to running Kubeflow was to remove the Istio and Argo bundled with Kubeflow so that it could be integrated with the Intuit service mesh.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To Remove Kubeflow’s default Istio&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We have used Kustomize to build the manifest we need for our Kubeflow installation and we are using &lt;a href=&quot;https://argoproj.github.io/argo-cd/&quot;&gt;ArgoCD&lt;/a&gt; to deploy the Kubeflow Kubernetes manifests.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;.
├── base                        # Base folder for the kubeflow out of the box manifests
│   ├── kustomization.yaml      
│   ├── pipelines               # Folder for Kubeflow Pipelines module
│   │   ├── kustomization.yaml
│   ├── other modules           # Similar to the Pipelines module you can bring other modules as well
│       ├── kustomization.yaml
├── envs                        # Folder for all the Kubeflow environments
│   ├── prod               
│   │   ├── kustomization.yaml
│   ├── dev               
│       ├── kustomization.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;base -&amp;gt; kustomization.yaml&lt;/code&gt;&lt;/p&gt;
&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kustomize.config.k8s.io/v1beta1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Kustomization&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;github.com/kubeflow/manifests/common/kubeflow-roles/base?ref=v1.3.0&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;github.com/kubeflow/manifests/common/kubeflow-namespace/base?ref=v1.3.0&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;github.com/kubeflow/manifests/common/oidc-authservice/base?ref=v1.3.0&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;github.com/kubeflow/manifests/apps/admission-webhook/upstream/overlays/cert-manager?ref=v1.3.0&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;github.com/kubeflow/manifests/apps/profiles/upstream/overlays/kubeflow?ref=v1.3.0&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;github.com/kubeflow/manifests/apps/centraldashboard/upstream/overlays/istio?ref=v1.3.0&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;pipelines&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;base -&amp;gt; pipelines -&amp;gt; kustomization.yaml&lt;/code&gt;&lt;/p&gt;
&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kustomize.config.k8s.io/v1beta1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Kustomization&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;bases&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;github.com/kubeflow/pipelines/manifests/kustomize/base/installs/multi-user?ref=1.5.0&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;github.com/kubeflow/pipelines/manifests/kustomize/base/metadata/base?ref=1.5.0&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;github.com/kubeflow/pipelines/manifests/kustomize/base/metadata/options/istio?ref=1.5.0&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# To remove the default Argo from Pipelines module&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# - github.com/kubeflow/pipelines/manifests/kustomize/third-party/argo/installs/cluster?ref=1.5.0&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;github.com/kubeflow/pipelines/manifests/kustomize/third-party/mysql/base?ref=1.5.0&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;github.com/kubeflow/pipelines/manifests/kustomize/third-party/mysql/options/istio?ref=1.5.0&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;github.com/kubeflow/pipelines/manifests/kustomize/third-party/minio/base?ref=1.5.0&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;github.com/kubeflow/pipelines/manifests/kustomize/third-party/minio/options/istio?ref=1.5.0&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;github.com/kubeflow/pipelines/manifests/kustomize/third-party/metacontroller/base?ref=1.5.0&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Identifier for application manager to apply ownerReference.&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# The ownerReference ensures the resources get garbage collected&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# when application is deleted.&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;commonLabels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;application-crd-id&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubeflow-pipelines&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# !!! If you want to customize the namespace,&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# please also update base/cache-deployer/cluster-scoped/cache-deployer-clusterrolebinding.yaml&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubeflow&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note: we had to create a separate folder for pipelines because we didn’t want to use Argo, which comes with the Pipelines module. If you can use default Argo, then you can simply use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https://github.com/kubeflow/manifests/apps/pipeline/upstream/env/platform-agnostic-multi-user-pns?ref=v1.3.0&lt;/code&gt; instead of the pipelines folder.&lt;/p&gt;

&lt;p&gt;If you don’t want to use ArgoCD, you can build the manifest using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kustomize build&lt;/code&gt; command, which is essentially what ArgoCD does. The configuration above has been tested for Kustomize 3.8.x and 4.0.x, and it works with both.&lt;/p&gt;

&lt;h3 id=&quot;step-2-kustomize-the-kubeflow-manifests&quot;&gt;Step 2: Kustomize the Kubeflow manifests&lt;/h3&gt;

&lt;p&gt;Given the managed Kubernetes ecosystem at Intuit, protocols for service to service communication and namespace isolation is opinionated, and we had to make the following changes:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Enable Kubeflow namespace for &lt;a href=&quot;https://istio.io/latest/docs/setup/additional-setup/sidecar-injection/#automatic-sidecar-injection&quot;&gt;Istio injection&lt;/a&gt; by adding the label &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-injection: enabled&lt;/code&gt; in the namespace specification. This label is then used by Istio to add the sidecar into the namespace.&lt;/li&gt;
  &lt;li&gt;Enable sidecar injection to all the deployments and statefulsets in Kubeflow by adding the annotation &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sidecar.istio.io/inject: &quot;true&quot;&lt;/code&gt;, along with some Intuit-specific custom labels and annotations to the Deployments and StatefulSets.&lt;/li&gt;
  &lt;li&gt;Intuit’s security policies forbid the direct use of external container registries. Intuit’s internal container registry runs regular vulnerability scans and certifies Docker images for use in various environments. The internal container registry also has an allow list that enables external registries to be proxied and held to the same, high-security standards. We enabled it for all Kubeflow containers.&lt;/li&gt;
  &lt;li&gt;Changes in VirtualService to route all the traffic from one central gateway instead of using Kubeflow gateway.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We have used &lt;a href=&quot;https://kustomize.io/&quot;&gt;Kustomize&lt;/a&gt; to modify the Kubeflow application manifest.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;For adding labels, we have used LabelTransformer
    &lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;     &lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;builtin&lt;/span&gt;
     &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;LabelTransformer&lt;/span&gt;
     &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
       &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;deployment-labels&lt;/span&gt;
     &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&amp;lt;Intuit custom labels&amp;gt;&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;istio-injected&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;true&quot;&lt;/span&gt;
     &lt;span class=&quot;na&quot;&gt;fieldSpecs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
     &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;spec/template/metadata/labels&lt;/span&gt;
       &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Deployment&lt;/span&gt;
       &lt;span class=&quot;na&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
     &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;spec/template/metadata/labels&lt;/span&gt;
       &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;StatefulSet&lt;/span&gt;
       &lt;span class=&quot;na&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For adding annotations, we have used AnnotationsTransformer&lt;/p&gt;

    &lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;builtin&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;AnnotationsTransformer&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;deployment-annotations&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;annotations&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;s&quot;&gt;&amp;lt;Intuit custom annotations&amp;gt;&lt;/span&gt;
   &lt;span class=&quot;s&quot;&gt;sidecar.istio.io/inject&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;true&quot;&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;fieldSpecs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
 &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;spec/template/metadata/annotations&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Deployment&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
 &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;spec/template/metadata/annotations&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;StatefulSet&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For replacing docker image URLs, we used ImageTagTransformer&lt;/p&gt;

    &lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;builtin&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ImageTagTransformer&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;image-transformer-1&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;imageTag&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;gcr.io/ml-pipeline/cache-deployer&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;newName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;docker.intuit.com/gcr-rmt/ml-pipeline/cache-deployer&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;It will be helpful for any organization which has a proxy for accessing the internet, cloning all the container images local to your org is the way to go as the internet will not be required to access those images.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For transforming VirtualServices&lt;/p&gt;

    &lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;remove&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/spec/hosts/0&lt;/span&gt;
 &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;replace&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/spec/gateways/0&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;custom gateway&amp;gt;&lt;/span&gt;
 &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;add&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/spec/hosts/0&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;kubflow host name&amp;gt;&lt;/span&gt;
 &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;add&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/spec/exportTo&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;.&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Putting it all together&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;envs -&amp;gt; prod/dev -&amp;gt; kustomization.yaml&lt;/code&gt;&lt;/p&gt;
    &lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kustomize.config.k8s.io/v1beta1&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Kustomization&lt;/span&gt;

 &lt;span class=&quot;na&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
 &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;../base&lt;/span&gt;

 &lt;span class=&quot;na&quot;&gt;transformers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
 &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;transformers/image-transformers.yaml&lt;/span&gt;
 &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;transformers/label-transformers.yaml&lt;/span&gt;
 &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;transformers/annotations-transformers.yaml&lt;/span&gt;

 &lt;span class=&quot;na&quot;&gt;patchesJson6902&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
 &lt;span class=&quot;c1&quot;&gt;# patch VirtualService with explicit host&lt;/span&gt;
 &lt;span class=&quot;c1&quot;&gt;# add multiple targets like below for all the VirtualServices which you need&lt;/span&gt;
 &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;patches/virtual-service-hosts.yaml&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
     &lt;span class=&quot;na&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;networking.istio.io&lt;/span&gt;
     &lt;span class=&quot;na&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1alpha3&lt;/span&gt;
     &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;VirtualService&lt;/span&gt;
     &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;centraldashboard&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;You might face issues with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;metadata_envoy&lt;/code&gt; service, in our case we were getting the following error
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; [debug][init] [external/envoy/source/common/init/watcher_impl.cc:27] init manager Server destroyed
 unable to bind domain socket with id=0 (see --base-id option)
 2021-01-29T23:32:26.680310Z error Epoch 0 exited with error: exit status 1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;After looking up, we found that, when you run this docker image with Istio Sidecar injection, this problem occurs.
 The reason is, both these containers are essentially envoyproxy containers and the default base-id for both containers is set to 0.&lt;/p&gt;

    &lt;p&gt;So to make it work, we had to change CMD in this &lt;a href=&quot;https://github.com/kubeflow/pipelines/blob/1.4.1/third_party/metadata_envoy/Dockerfile#L27&quot;&gt;Dockerfile&lt;/a&gt;&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; CMD [&quot;/etc/envoy.yaml&quot;, &quot;--base-id&quot;, &quot;1&quot;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;step-3-custom-changes-needed-for-sso&quot;&gt;Step 3: Custom changes needed for SSO&lt;/h3&gt;

&lt;p&gt;There are two major components around authentication using SSO:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Authservice: It is a StatefulSet that runs the oidc-auth service. It runs in the istio-system namespace and directly talks to an OIDC service for authentication&lt;/li&gt;
  &lt;li&gt;Authn-filter: It’s an EnvoyFilter that filters the traffic to authservice and checks the Kubeflow auth header and redirects to authservice if the request is not authorized, check the presence of header called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubeflow-userid&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Note: Intuit SSO supports OIDC, so we did not need to use dex for the integration. If your org’s SSO does not support OIDC, then you can use dex in the middle; details can be found &lt;a href=&quot;https://github.com/kubeflow/manifests#dex&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For our installation, we needed the authservice to be mesh-enabled, and it made more sense to move authservice to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubeflow&lt;/code&gt; namespace as well, which was already enabled for Istio sidecar injection.&lt;/p&gt;

&lt;p&gt;After enabling Istio mesh on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;authservice&lt;/code&gt;, some more changes were required in the default manifest for it to work. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;authservice&lt;/code&gt; pod was not able to communicate with the Intuit SSO HTTPS URL, because outbound traffic from the main container pod is intercepted by Istio sidecar to enforce mtls (default behavior). So, we had to exclude the HTTPS port (443) to disable mtls. This can be done using the annotation &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;traffic.sidecar.istio.io/excludeOutboundPorts: &quot;443&quot;&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;step-4-setting-up-ingress&quot;&gt;Step 4: Setting up ingress&lt;/h3&gt;

&lt;p&gt;We exposed the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-ingressgateway&lt;/code&gt; service as LoadBalancer using the following mechanism:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Setting up public hosted zone in Route 53, add hostname you would like to use, like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;example.com&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Setting up an ACM certificate for the hostname you want to use for the Kubeflow installation, the hostname can be &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubeflow.example.com&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Updating the service manifest by adding a few annotations:
    &lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Note that the backend talks over HTTP.&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;service.beta.kubernetes.io/aws-load-balancer-backend-protocol&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;http&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# TODO: Fill in with the ARN of your certificate.&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;service.beta.kubernetes.io/aws-load-balancer-ssl-cert&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;cert arn from step 2&amp;gt;&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;service.beta.kubernetes.io/aws-load-balancer-security-groups&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;to restrict access within org&amp;gt;&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Only run SSL on the port named &quot;https&quot; below.&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;service.beta.kubernetes.io/aws-load-balancer-ssl-ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;https&quot;&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;external-dns.alpha.kubernetes.io/hostname&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubeflow.example.com&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;After applying the new manifest, AWS will automatically add the appropriate A and TXT entries in your hosted zone (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;example.com&lt;/code&gt;) and Kubeflow will be accessible at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubeflow.example.com&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;To secure the &lt;a href=&quot;https://www.kubeflow.org/docs/started/k8s/kfctl-istio-dex/#secure-with-https&quot;&gt;Gateway with https&lt;/a&gt;, you can also change the gateway port and add the key and certificate in the Gateway.&lt;/p&gt;

&lt;p&gt;More about these annotations can be found at &lt;a href=&quot;https://aws.amazon.com/premiumsupport/knowledge-center/terminate-https-traffic-eks-acm/&quot;&gt;Terminate HTTPS traffic on Amazon EKS&lt;/a&gt; and &lt;a href=&quot;https://kubernetes.io/docs/concepts/services-networking/service/#ssl-support-on-aws&quot;&gt;SSL support on AWS&lt;/a&gt; blog.&lt;/p&gt;

&lt;h3 id=&quot;step-5-using-an-external-argo-installation&quot;&gt;Step 5: Using an external Argo installation&lt;/h3&gt;

&lt;p&gt;Kubfelow uses Argo workflows internally to run the pipeline in a workflow fashion. Argo generates artifacts after the workflow steps and all we need to do is configure the artifact store if we are planning to use the external Argo:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/argoproj/argo-workflows/blob/master/docs/quick-start.md#install-argo-workflows&quot;&gt;Install Argo workflows&lt;/a&gt; in your cluster, it gets installed in a namespace called argo.&lt;/li&gt;
  &lt;li&gt;Remove all the Argo-related manifests from Kubeflow.&lt;/li&gt;
  &lt;li&gt;To override the artifact store, you need to change the ConfigMap &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;workflow-controller-configmap&lt;/code&gt; which comes with the &lt;a href=&quot;https://github.com/kubeflow/manifests/blob/v1.3.0/apps/pipeline/upstream/third-party/argo/base/workflow-controller-configmap-patch.yaml&quot;&gt;Kubeflow manifest&lt;/a&gt;. It uses minio as the store but you can configure it to use S3 as well. More details can be found from the &lt;a href=&quot;https://github.com/argoproj/argo-workflows/blob/master/docs/workflow-controller-configmap.md&quot;&gt;Argo&lt;/a&gt;&lt;a href=&quot;https://github.com/argoproj/argo-workflows/blob/master/docs/workflow-controller-configmap.md&quot;&gt;Workflow Controller Configmap GitHub page&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;The latest version of Argo has the option to override &lt;a href=&quot;https://argoproj.github.io/argo-workflows/artifact-repository-ref/&quot;&gt;artifact store for namespace&lt;/a&gt; as well.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Debugging tricks&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Check if EnvoyFilter is getting applied: you should have the &lt;strong&gt;istioctl&lt;/strong&gt; cmd tool:&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istioctl proxy-config listeners &amp;lt;pod name&amp;gt; --port 15001 -o json&lt;/code&gt;&lt;/p&gt;

    &lt;p&gt;See if the envoy filter is getting listed in the output. More about Istio proxy debugging can be found &lt;a href=&quot;https://istio.io/latest/docs/ops/diagnostic-tools/proxy-cmd/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Check istio-ingressgateway:&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; # Port forward to the first istio-ingressgateway pod
 kubectl -n istio-system port-forward $(kubectl -n istio-system get pods -listio=ingressgateway -o=jsonpath=&quot;{.items[0].metadata.name}&quot;) 15000

 # Get the http routes from the port-forwarded ingressgateway pod (requires jq)
 curl --silent http://localhost:15000/config_dump | jq '\''.configs.routes.dynamic_route_configs[].route_config.virtual_hosts[]| {name: .name, domains: .domains, route: .routes[].match.prefix}'\''

 # Get the logs of the first istio-ingressgateway pod
 # Shows what happens with incoming requests and possible errors
 kubectl -n istio-system logs $(kubectl -n istio-system get pods -listio=ingressgateway -o=jsonpath=&quot;{.items[0].metadata.name}&quot;) --tail=300

 # Get the logs of the first istio-pilot pod
 # Shows issues with configurations or connecting to the Envoy proxies
 kubectl -n istio-system logs $(kubectl -n istio-system get pods -listio=pilot -o=jsonpath=&quot;{.items[0].metadata.name}&quot;) discovery --tail=300
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Check the authservice connectivity: istio-ingressgateway pod should be able to access authservice. You can check that using the following command:&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl -n istio-system exec $(kubectl -n istio-system get pods -listio=pilot -o=jsonpath=&quot;{.items[0].metadata.name}&quot;) -- curl -v http://authservice.istio-system.svc.cluster.local:8080&lt;/code&gt;&lt;/p&gt;

    &lt;p&gt;Also, make sure authservice can reach dex:&lt;/p&gt;

    &lt;p&gt;In our case, authservice is in the kubeflow namespace so we made changes accordingly using the command below:&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl -n kubeflow exec authservice-0 -- wget -q -S -O '-' &amp;lt;oidc auth url&amp;gt;/.well-known/openid-configuration&lt;/code&gt;&lt;/p&gt;

    &lt;p&gt;It should look something similar to:&lt;/p&gt;
    &lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
   &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;issuer&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;http://dex.kubeflow.svc.cluster.local:5556/dex&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
   &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;authorization_endpoint&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;http://dex.kubeflow.svc.cluster.local:5556/dex/auth&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
   &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;token_endpoint&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;http://dex.kubeflow.svc.cluster.local:5556/dex/token&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
   &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;jwks_uri&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;http://dex.kubeflow.svc.cluster.local:5556/dex/keys&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
   &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;userinfo_endpoint&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;http://dex.kubeflow.svc.cluster.local:5556/dex/userinfo&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
   &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;response_types_supported&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;code&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
   &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;subject_types_supported&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;public&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
   &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;id_token_signing_alg_values_supported&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;RS256&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
   &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;scopes_supported&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;openid&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;email&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;groups&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;profile&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;offline_access&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
   &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;token_endpoint_auth_methods_supported&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;client_secret_basic&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
   &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;claims_supported&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;aud&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;email&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;email_verified&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;iat&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;iss&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;locale&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
     &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;sub&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Check connectivity between services: try using &lt;strong&gt;curl&lt;/strong&gt; or &lt;strong&gt;wget&lt;/strong&gt; from one service to another. Usually one or the other is always available, otherwise, you can always install using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apt-get&lt;/code&gt; command. Example use case: from the ml-pipeline deployment pod you can check if pipeline APIs are accessible.&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl -n kubeflow exec $(kubectl -n kubeflow get pods -lapp=ml-pipeline-ui -o=jsonpath=&quot;{.items[0].metadata.name}&quot;)  -- wget -q -S -O '-' ml-pipeline.kubeflow.svc.cluster.local:8888/apis/v1beta1/pipelines&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;asks-for-the-kubeflow-community&quot;&gt;Asks for the Kubeflow Community&lt;/h2&gt;

&lt;p&gt;The challenges that we encountered at Intuit are not unique and will be faced by any enterprise that wants to adopt Kubeflow.&lt;/p&gt;

&lt;p&gt;It would be nice to have Kubeflow play well with the available Kubernetes infrastructure in an enterprise, rather than mandating its own set of infrastructure. Here are some suggestions/bugs for improving the ecosystem, some of which Intuit will work with the community to build out:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;We saw Kubeflow manifest repo went through &lt;a href=&quot;https://github.com/kubeflow/manifests/issues/1735&quot;&gt;major folder restructuring&lt;/a&gt; for v1.3 but we think there is still room for improvements.&lt;/li&gt;
  &lt;li&gt;Multi-Cluster / Multi-Region support. &lt;a href=&quot;https://github.com/kubeflow/kubeflow/issues/5467&quot;&gt;#5467&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Upgrade seems to be an issue in general, should figure out a way to manage this better. &lt;a href=&quot;https://github.com/kubeflow/kubeflow/issues/5440&quot;&gt;#5440&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Multi-tenancy with group support. &lt;a href=&quot;https://github.com/kubeflow/kubeflow/issues/4188&quot;&gt;#4188&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Installing Kubeflow in any custom namespace. &lt;a href=&quot;https://github.com/kubeflow/kubeflow/issues/5647&quot;&gt;#5647&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Existing metadata service is not performant, we did try some settings with more resources and horizontal scaling. The community is already working on &lt;a href=&quot;https://docs.google.com/document/d/1fHU29oScMEKPttDA1Th1ibImAKsFVVt2Ynr4ZME05i0/edit&quot;&gt;KFP v2.0&lt;/a&gt;, which might address a lot of concerns around metadata service.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.google.com/document/d/1fHU29oScMEKPttDA1Th1ibImAKsFVVt2Ynr4ZME05i0/edit&quot;&gt;Kubeflow Pipelines (KFP) v2 System Design&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://istio.io/latest/docs/reference/config/networking/&quot;&gt;Traffic Management Components&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://istio.io/v1.6/docs/ops/deployment/architecture/&quot;&gt;Istio 1.6 Architecture&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://istio.io/v1.3/docs/concepts/what-is-istio/#architecture&quot;&gt;Istio 1.3 Architecture&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/premiumsupport/knowledge-center/terminate-https-traffic-eks-acm/&quot;&gt;Terminate HTTPS traffic on Amazon EKS&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kubernetes.io/docs/concepts/services-networking/service/#ssl-support-on-aws&quot;&gt;SSL support on AWS&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.developermarch.com/developersummit/downloadPDF/Intuit%20Modern%20SaaS%20Platform%20-%20GIDS.pdf&quot;&gt;Intuit’s Modern SaaS Platform&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=EWyNbBn1vns&quot;&gt;Stitching a Service Mesh Across Hundreds of Discrete Networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://istio.io/latest/blog/2020/multi-cluster-mesh-automation/&quot;&gt;Multicluster Istio configuration and service discovery using Admiral&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/intuit-engineering/genius-of-admiral-3307e63e3ab6&quot;&gt;Genius of Admiral&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>&lt;a href='https://www.linkedin.com/in/deepk2u/'&gt;Deepak Kumar&lt;/a&gt;</name></author><category term="kubeflow" /><category term="kubeflow 1.3" /><category term="install kubeflow" /><category term="kubeflow pipelines" /><category term="intuit" /><category term="istio" /><category term="service mesh" /><category term="argo" /><summary type="html">Deploying Kubeflow 1.3 in an enterprise with existing Kubernetes infrastructure, a Service Mesh (Istio), and Argo, presents a host of challenges. This blog will address how those challenges were overcome while retaining the best practices that both the organization and Kubeflow prescribe.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.kubeflow.org/images/logo.png" /><media:content medium="image" url="https://blog.kubeflow.org/images/logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">The Kubeflow 1.3 software release streamlines ML workflows and simplifies ML platform operations</title><link href="https://blog.kubeflow.org/kubeflow-1.3-release/" rel="alternate" type="text/html" title="The Kubeflow 1.3 software release streamlines ML workflows and simplifies ML platform operations" /><published>2021-04-23T00:00:00-05:00</published><updated>2021-04-23T00:00:00-05:00</updated><id>https://blog.kubeflow.org/kubeflow-1.3-release</id><content type="html" xml:base="https://blog.kubeflow.org/kubeflow-1.3-release/">&lt;p&gt;The Kubeflow 1.3 release delivers simplified ML workflows and additional Kubernetes integrated features to optimize operational and infrastructure efficiencies. 
In addition to new User Interfaces (UIs), which improve ML workflows for pipeline building, model tuning, serving and monitoring, 1.3 also enables “headless” GitOps-inspired installation patterns.
The latest version of Kubeflow provides users with a mature foundation and delivers a modern ML platform with best-in-class Key Performance Indicators (KPIs).&lt;/p&gt;

&lt;p&gt;The Kubeflow user community is growing quickly, which was demonstrated in our recent survey results.  When compared to &lt;a href=&quot;https://www.youtube.com/watch?v=4228OEenuGc&quot;&gt;last year’s survey&lt;/a&gt;, the &lt;a href=&quot;https://blog.kubeflow.org/kubeflow-continues-to-move-to-production&quot;&gt;2021 Survey&lt;/a&gt; showed a 50% increase in responses and a whopping 300% increase in users supporting production deployments.   As shown below, the user survey responses, especially from ML engineers, architects and data scientists, have identified where the Kubeflow contributors should focus their efforts.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://blog.kubeflow.org/kubeflow-continues-to-move-to-production#using-kubeflow-goes-beyond-just-training&quot;&gt;Kubeflow User Survey Results&lt;/a&gt; - March 2021&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-04-23-kubeflow-1.3/image1.png&quot; alt=&quot;survey_results&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;streamlined-ml-workflows-delivered-via-new-uis&quot;&gt;Streamlined ML workflows delivered via new UIs&lt;/h2&gt;

&lt;p&gt;Data scientists will like the new and updated user interfaces (UIs) for Katib, TensorBoard, Persistent Volumes, Pipelines and Kale. These new UIs address many of the ML tasks that are time consuming and technically challenging. The UIs reduce the need for a data scientist to learn kfctl or docker CLI commands.&lt;/p&gt;

&lt;p&gt;Below please find details on the UIs’ benefits for ML workflows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Katib (&lt;a href=&quot;https://www.youtube.com/watch?v=VDINH5WkBhA&quot;&gt;Video Tour&lt;/a&gt;)
    &lt;ul&gt;
      &lt;li&gt;The Katib UI is integrated with the central dashboard and streamlines hyperparameter tuning by presenting a visualization graph and a table that compares each trial’s performance along with its hyperparameters. You can also review the details of each trial’s algorithm, metrics collector and yaml. (Project &lt;a href=&quot;https://github.com/kubeflow/katib/projects/1&quot;&gt;1&lt;/a&gt;)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;TensorBoard (&lt;a href=&quot;https://www.youtube.com/watch?v=eMDF2Bk8YRY&quot;&gt;Video tour&lt;/a&gt;)
    &lt;ul&gt;
      &lt;li&gt;The TensorBoard UI streamlines the TensorBoard configuration tasks, especially for logging of training jobs which are running in Notebooks or Pipelines. It simplifies accessibility to metrics, which helps you to improve model accuracy , identify performance bottlenecks, and reduce unproductive training jobs.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Volume Manager (&lt;a href=&quot;https://www.youtube.com/watch?v=jU2DtSWahdA&quot;&gt;Video tour&lt;/a&gt;)
    &lt;ul&gt;
      &lt;li&gt;The Volume Manager enables you to manage your data and persistent volumes. For the volumes in your namespace, it streamlines the creation and deletion of volumes, which then can be easily attached to your notebooks. PR &lt;a href=&quot;https://github.com/kubeflow/kubeflow/pull/5684&quot;&gt;5684&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Kale (&lt;a href=&quot;https://www.youtube.com/watch?v=ANBkUySirGg&quot;&gt;Video tour&lt;/a&gt;)
    &lt;ul&gt;
      &lt;li&gt;The updated Kale UI, a JupyterLab extension, simplifies your hyperparameter tuning trial set-up. The UI walks you through these steps: enter your hyperparameters as a list or a range, pick your search algorithm (Grid, Random, Bayesian) and the parameter to be optimized i.e. minimize loss. Then with a click of a button, your Katib trials are set-up, snapshotted, tracked, and run.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Kubeflow Pipelines (KFP)
    &lt;ul&gt;
      &lt;li&gt;The KFP UI has been reorganized for a more unified experience (PR &lt;a href=&quot;https://github.com/kubeflow/pipelines/pull/4925&quot;&gt;4925&lt;/a&gt;), and includes the ability to manage recurring runs via new “JobsList” and “AllJobslist” pages (PR &lt;a href=&quot;https://github.com/kubeflow/pipelines/pull/5131&quot;&gt;5131&lt;/a&gt;) and simplified view of dependency graphs.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Beyond the UIs, data scientists can also tie Notebooks with Serving more closely than ever before. In addition to the aforementioned integration with TensorBoard, Kubeflow Notebooks also now support first class deployments with TensorFlow 2.0, PyTorch, VS Code and RStudio.&lt;/p&gt;

&lt;p&gt;KFServing enhancements include simplified canary rollouts with traffic splitting at the Knative revisions level. It also delivers extended ML framework support for:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;TorchServe predict and PyTorch Captum Explain&lt;/li&gt;
  &lt;li&gt;PMMLServer, PR &lt;a href=&quot;https://github.com/kubeflow/kfserving/pull/1141&quot;&gt;1141&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;LightGBM&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;infrastructure-and-operational-efficiencies&quot;&gt;Infrastructure and operational efficiencies&lt;/h2&gt;

&lt;p&gt;ML engineers will like 1.3’s delivery of operational and infrastructure efficiencies, which are coupled with streamlined installation patterns and upgraded Istio version support. The following chart provides a summary of the top production features in 1.3.&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Feature&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Benefits&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;a href=&quot;https://github.com/yuzliu/kfserving/blob/master/docs/MULTIMODELSERVING_GUIDE.md&quot;&gt;Multi-model serving&lt;/a&gt; (Alpha)
   &lt;/td&gt;
   &lt;td&gt;More models on same infra and workaround cluster limits        i.e. # of pods &amp;amp; ip addresses
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Pod affinity
   &lt;/td&gt;
   &lt;td&gt;Avoid unnecessary usage on GPU or large CPU nodes
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;gRPC support 
   &lt;/td&gt;
   &lt;td&gt;Fewer messages, less bandwidth for KFServing workloads
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Katib trial templates
   &lt;/td&gt;
   &lt;td&gt;Simplifies hyperparameter tuning set-up for custom model types
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Katib early stopping
   &lt;/td&gt;
   &lt;td&gt;Stops hyperparameter tuning trials that are unproductive 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Pipelines step caching
   &lt;/td&gt;
   &lt;td&gt;Re-use results from previously run steps
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Multi-user pipelines
   &lt;/td&gt;
   &lt;td&gt;User and resource isolation for non-GCP environments.
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Manifests refactoring
   &lt;/td&gt;
   &lt;td&gt;Simplifies Kubeflow installation and upgrades
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Istio upgradability
   &lt;/td&gt;
   &lt;td&gt;Improved security, day 2 operations, compatibility and support
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;We are pleased to announce that the user documentation on Kubeflow.org has also been updated (PR &lt;a href=&quot;https://github.com/kubeflow/website/issues/2546&quot;&gt;2546&lt;/a&gt;). Additional detailed documentation, especially on the valuable working group deliveries, can be found here:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Kubeflow Pipeline 1.3 Project (PR &lt;a href=&quot;https://github.com/kubeflow/pipelines/projects/12&quot;&gt;12&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kubeflow.org/docs/components/pipelines/sdk/pipelines-with-tekton/&quot;&gt;Kubeflow Pipelines SDK with Tekton&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.kubeflow.org/release/official/2021/03/08/kfserving-0.5.html&quot;&gt;Operationalize, scale and infuse trust in AI models using KFServing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.kubeflow.org/katib/&quot;&gt;Kubeflow Katib: Scalable, portable and cloud native system for AutoML&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;simplified-installation-and-improved-documentation&quot;&gt;Simplified installation and improved documentation&lt;/h2&gt;

&lt;p&gt;ML Engineers, who are installing Kubeflow, have a clear path to installation success as Kubeflow 1.3 includes new manifests and upgraded Istio support. For more information on installation patterns for each distribution, please visit the &lt;a href=&quot;https://www.kubeflow.org/docs/started/installing-kubeflow/&quot;&gt;Getting Started&lt;/a&gt; page on Kubeflow.org. If you are supporting a distribution or just interested in low-level details, please review the Kubeflow 1.3 Manifest &lt;a href=&quot;https://github.com/kubeflow/manifests/tree/v1.3.0-rc.0#readme&quot;&gt;readme&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;kubeflow-13-tutorials&quot;&gt;Kubeflow 1.3 tutorials&lt;/h2&gt;

&lt;p&gt;Kubeflow 1.3 new features are easy to try on these tutorials:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Open Vaccine &lt;a href=&quot;https://arrik.to/democ2p&quot;&gt;Tutorial&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Use the new UIs to build an ML Pipeline, tune your model, and then deploy and monitor it. This tensorflow-based example was modified from a Kaggle tutorial for building a Covid 19 vaccine from bases in an mRNA molecule. The tutorial is easy to run on AWS and GCP in about 1 hour.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Model Risk Management &lt;a href=&quot;https://www.fairly.ai/tutorial-kubeflow&quot;&gt;Tutorial&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;This model produces a SR11-7 compliance report for financial institutions who are regulated by the Federal Reserve. The example provides reporting on bias in a home mortgage lending model. The tutorial is easy to run on AWS and GCP in about 1 hour.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;join-the-community&quot;&gt;Join the community&lt;/h2&gt;

&lt;p&gt;We would like to thank everyone for their efforts on Kubeflow 1.3, especially the code contributors and working group leads. As you can see from the extensive contributions to Kubeflow 1.3, the Kubeflow Community is vibrant and diverse, and solving real world problems for organizations around the world.&lt;/p&gt;

&lt;p&gt;Want to help? The Kubeflow Community &lt;a href=&quot;https://github.com/kubeflow/community/blob/master/wg-list.md&quot;&gt;Working Groups&lt;/a&gt; hold open meetings, public lists, and are always looking for more volunteers and users to unlock the potential of machine learning. If you’re interested in becoming a Kubeflow contributor, please feel free to check out the resources below, we look forward to working with you!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Visit our&lt;a href=&quot;https://www.kubeflow.org/&quot;&gt; Kubeflow website&lt;/a&gt; or&lt;a href=&quot;https://github.com/kubeflow&quot;&gt; Kubeflow GitHub Page&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Join the&lt;a href=&quot;https://join.slack.com/t/kubeflow/shared_invite/enQtMjgyMzMxNDgyMTQ5LWUwMTIxNmZlZTk2NGU0MmFiNDE4YWJiMzFiOGNkZGZjZmRlNTExNmUwMmQ2NzMwYzk5YzQxOWQyODBlZGY2OTg&quot;&gt; Kubeflow Slack channel&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Join the&lt;a href=&quot;https://groups.google.com/forum/#!forum/kubeflow-discuss&quot;&gt; kubeflow-discuss&lt;/a&gt; mailing list&lt;/li&gt;
  &lt;li&gt;Attend a&lt;a href=&quot;https://www.kubeflow.org/docs/about/community/&quot;&gt; weekly community meeting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Josh Bottum, Thea Lamkin, David Aronchick</name></author><category term="release" /><summary type="html">The Kubeflow 1.3 release delivers simplified ML workflows and additional Kubernetes integrated features to optimize operational and infrastructure efficiencies. In addition to new User Interfaces (UIs), which improve ML workflows for pipeline building, model tuning, serving and monitoring, 1.3 also enables “headless” GitOps-inspired installation patterns. The latest version of Kubeflow provides users with a mature foundation and delivers a modern ML platform with best-in-class Key Performance Indicators (KPIs).</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.kubeflow.org/images/logo.png" /><media:content medium="image" url="https://blog.kubeflow.org/images/logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Kubeflow Continues to Move into Production</title><link href="https://blog.kubeflow.org/kubeflow-continues-to-move-to-production" rel="alternate" type="text/html" title="Kubeflow Continues to Move into Production" /><published>2021-03-19T00:00:00-05:00</published><updated>2021-03-19T00:00:00-05:00</updated><id>https://blog.kubeflow.org/kubeflow-continues-to-move-to-production</id><content type="html" xml:base="https://blog.kubeflow.org/kubeflow-continues-to-move-to-production">&lt;p&gt;Kubeflow Users are maturing and the community is growing, forty eight percent of users are supporting deployments in production.&lt;/p&gt;

&lt;p&gt;The Spring 2021 &lt;a href=&quot;http://kubeflow.org/&quot;&gt;Kubeflow&lt;/a&gt; Community User Survey collected input from Kubeflow users on the benefits, gaps and requirements for machine learning use cases. It is the largest survey to date with 179 responses—a 50% increase from the &lt;a href=&quot;https://medium.com/kubeflow/kubeflow-community-user-survey-fall-2019-a84776c71743&quot;&gt;Kubeflow 1.0 Community User Survey&lt;/a&gt; a year ago. The Survey respondents span a spectrum of skill sets. While 42% are machine learning (ML) engineers, and 24% are ML architects, the titles of the respondents vary from DevOps Engineers to data scientists and product managers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;images/2021-03-11-survey/image1.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;kubeflow-continues-to-move-into-production&quot;&gt;Kubeflow Continues to Move Into Production&lt;/h1&gt;

&lt;p&gt;Forty eight percent of users are supporting deployments that are in production, up from 15% last year. Further, one question that many folks have is “do people upgrade a production deployment or just install a new cluster and start over?” It appears that the latter is far more common: just 8% have upgraded their environment.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;images/2021-03-11-survey/image2.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Similar to previous years, Kubeflow Pipelines and Notebooks are the most popular components, but other components are now being widely deployed as well. Interest in TensorBoard has grown, joining KFServing, Katib (AutoML), and Distributed Training as top additional services.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;images/2021-03-11-survey/image3.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Although the usage patterns for Kubeflow components are mixed, the vast majority of users need at least two Kubeflow components in their ML Platform.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;images/2021-03-11-survey/image4.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;TensorFlow is the leading ML Framework, followed by Scikit-learn, PyTorch, Keras, and XGBoost. However, with Kubeflow’s built-in extensibility, the type of ML tools people use in Kubeflow go beyond just training frameworks, and include MLFlow, Airflow, and Spark.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;images/2021-03-11-survey/image5.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;vs-code-and-jupyter-lead-dev-environments&quot;&gt;VS Code and Jupyter Lead Dev Environments&lt;/h1&gt;

&lt;p&gt;From an Integrated Development Environment (IDE) perspective, most users are developing models in Jupyter Notebooks and Visual Studio Code, and about one third are using PyCharm.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;images/2021-03-11-survey/image6.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From a gap perspective, the users would like improved documentation, tutorials, and installation, along with more automation, support and security.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;images/2021-03-11-survey/image7.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;using-kubeflow-goes-beyond-just-training&quot;&gt;Using Kubeflow Goes Beyond Just Training&lt;/h1&gt;

&lt;p&gt;Users identified that data preprocessing and transformation are both the most time consuming and challenging steps. We also received feedback that pipeline building and feature engineering are both time consuming and challenging. Distributed training, model serving and monitoring appear to be more technically challenging than time consuming.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;images/2021-03-11-survey/image8.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ML model delivery commonly requires multiple teams to work together i.e. data engineers, data scientists, ML engineers and devops engineers. ML workflows often include manual processes and there can be gaps in the handoffs between these groups. In particular, connecting data pipelines to ML pipelines is an example of a process that could be better automated, along with pipeline building and model monitoring.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;images/2021-03-11-survey/image9.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The vast majority of Kubeflow users are self-reliant in solving complex problems:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;images/2021-03-11-survey/image10.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And many are using tutorials created by Cloud Service Providers (i.e. Google, AWS and Azure) and MiniKF from Arrikto.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;images/2021-03-11-survey/image11.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The majority of ML models have a fairly short life: ~50% run in production for 3 months or less. On the other end of the spectrum, 25% of the models remain in production for 6 months or longer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;images/2021-03-11-survey/image12.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And ~70% of all models take up to 15 iterations to produce  a final model suitable for production.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;images/2021-03-11-survey/image13.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Users have a wide range of success with their models: ~43% are getting more than half of their models to deliver business value. On the other side, 39% are getting a very small percentage (10%) of their models into production and delivering business value.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;images/2021-03-11-survey/image14.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;user-requests&quot;&gt;User Requests&lt;/h1&gt;

&lt;p&gt;We provided a section for free-form responses and we received a great deal of feedback. Here are some good examples of user requests:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Metadata storage and versioning&lt;/li&gt;
  &lt;li&gt;More robust access control and permission granularity for model/data sharing&lt;/li&gt;
  &lt;li&gt;More visibility on your roadmap&lt;/li&gt;
  &lt;li&gt;Installation patterns, stability, multi tenancy&lt;/li&gt;
  &lt;li&gt;More real life case studies&lt;/li&gt;
  &lt;li&gt;Updated and more in-depth documentation&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;key-takeaways&quot;&gt;Key Takeaways&lt;/h1&gt;

&lt;p&gt;In addition to learning about how users are operating Kubeflow in production clusters, the Community Survey has given us important data that we can use to enhance our processes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Improvements to the release management process, which is being driven by better inter-Working Group collaboration. This, coupled with core upgrades to Istio and a clean-up of the installation manifests, will improve the testing, documentation, and installation patterns.&lt;/li&gt;
  &lt;li&gt;Automation of pipeline building and feature engineering tasks, especially with continued integrations of Kubeflow with Kale and Feast, which are enabling new end-to-end workflows and tutorials.&lt;/li&gt;
  &lt;li&gt;Enhancements for data preprocessing and transformation as well as streamlined connections of Data Pipelines to ML Pipelines.  Additionally, a renewed effort to develop a Spark operator.&lt;/li&gt;
  &lt;li&gt;Several new UIs are under development i.e. Katib, Model Management, Volumes Management and TensorBoard Management, which will help the user experience.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more details on the Community’s and specific Working Group’s deliveries, please review the Kubeflow 1.3 Release Blog post.&lt;/p&gt;

&lt;h1 id=&quot;join-the-community&quot;&gt;Join the Community&lt;/h1&gt;

&lt;p&gt;We would like to thank everyone for their participation in the survey. As you can see from the survey results, the Kubeflow Community is vibrant and diverse, solving real world problems for organizations around the world.&lt;/p&gt;

&lt;p&gt;Want to help? The Kubeflow Community &lt;a href=&quot;https://github.com/kubeflow/community/blob/master/wg-list.md&quot;&gt;Working Groups&lt;/a&gt; hold open meetings, public lists, and are always looking for more volunteers and users to unlock the potential of machine learning. If you’re interested in becoming a Kubeflow contributor, please feel free to check out the resources below. We look forward to working with you!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Visit our&lt;a href=&quot;https://www.kubeflow.org/&quot;&gt; Kubeflow website&lt;/a&gt; or&lt;a href=&quot;https://github.com/kubeflow&quot;&gt; Kubeflow GitHub Page&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Join the&lt;a href=&quot;https://join.slack.com/t/kubeflow/shared_invite/enQtMjgyMzMxNDgyMTQ5LWUwMTIxNmZlZTk2NGU0MmFiNDE4YWJiMzFiOGNkZGZjZmRlNTExNmUwMmQ2NzMwYzk5YzQxOWQyODBlZGY2OTg&quot;&gt; Kubeflow Slack channel&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Join the&lt;a href=&quot;https://groups.google.com/forum/#!forum/kubeflow-discuss&quot;&gt; kubeflow-discuss&lt;/a&gt; mailing list&lt;/li&gt;
  &lt;li&gt;Attend a&lt;a href=&quot;https://www.kubeflow.org/docs/about/community/&quot;&gt; weekly community meeting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Josh Bottum, David Aronchick, Thea Lamkin</name></author><category term="release" /><summary type="html">Kubeflow Users are maturing and the community is growing, forty eight percent of users are supporting deployments in production.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.kubeflow.org/images/logo.png" /><media:content medium="image" url="https://blog.kubeflow.org/images/logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Elastic Training with MPI Operator and Practice</title><link href="https://blog.kubeflow.org/elastic%20training/operators/2021/03/15/elastic-training.html" rel="alternate" type="text/html" title="Elastic Training with MPI Operator and Practice" /><published>2021-03-15T00:00:00-05:00</published><updated>2021-03-15T00:00:00-05:00</updated><id>https://blog.kubeflow.org/elastic%20training/operators/2021/03/15/elastic-training</id><content type="html" xml:base="https://blog.kubeflow.org/elastic%20training/operators/2021/03/15/elastic-training.html">&lt;p&gt;With increase in the size of dataset and deep learning models, distributed training emerges as the mainstream approach for training neural network models in industry. While it is feasible now to launch a massive distributed training job on Kubernetes with Kubeflow, advanced features like elastic workload and other cost mitigation approaches remain leashed when we talk about deep learning jobs on Kubernetes.&lt;/p&gt;

&lt;p&gt;To address issues on cost and resource utilization, the &lt;a href=&quot;https://intl.cloud.tencent.com/product/tke&quot;&gt;TKE (Tencent Kubernetes Engine)&lt;/a&gt; AI team designs and implements &lt;strong&gt;Elastic Training&lt;/strong&gt; in Kubeflow community.&lt;/p&gt;

&lt;p&gt;Here we present how the elastic training is performed on Kubernetes. Validated with experiments under circumstances, elastic training lowers cost for distributed training on cloud.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;Let’s first recap training deep learning models. When we talk about ‘training’, it refers generally to iteratively optimizing parameters in a neural network model with its gradient descent. Accelerated with GPUs, the training can speed up for 10-100 times.&lt;/p&gt;

&lt;p&gt;When manufacturers try to integrate more computational resources like GPUs into a single machine, to hold training experiments with more and more data or model parameters, the cost grows exponentially. Therefore, after initially proposed by &lt;a href=&quot;https://www.usenix.org/conference/osdi14/technical-sessions/presentation/li_mu&quot;&gt;Mu Li on OSDI’14&lt;/a&gt;, distributed training takes over training on a single machine when researchers play with massive dataset or large model.&lt;/p&gt;

&lt;p&gt;For distributed training in data-parallelism, &lt;a href=&quot;https://github.com/horovod/horovod&quot;&gt;Horovod&lt;/a&gt; is widely adopted given its excellent support on deep learning frameworks like &lt;a href=&quot;https://www.tensorflow.org&quot;&gt;TensorFlow&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org&quot;&gt;PyTorch&lt;/a&gt;, communication optimization and easier programming pattern. In Horovod, all training processes are equal participants, each of which process the gradient calculation and communication.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-03-15-elastic-training/horovod-allreduce.png&quot; width=&quot;&quot; alt=&quot;alt_text&quot; title=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Because of significant acceleration of training speed as well as the programming pattern that are easier to understand, data-parallelism distributed training, represented by Horovod, is getting more and more attention. However, there still remain some issues:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The cost of training on the cloud is still the hurdle. While researchers no longer face the complexity when training on cloud thanks to Kubernetes and Kubeflow, the cost of training on cloud quells some users.&lt;/li&gt;
  &lt;li&gt;Compared with training on a single machine, multi-node distributed training accumulates the probability of training failure. The entire training experiment fails when any of its training process issues an error. This problem becomes even severer when some experiments take days or even weeks.&lt;/li&gt;
  &lt;li&gt;When collocating training tasks with other workloads (with higher priority), the resources demand fluctuates as the request for these other workloads may change periodically. This unbalance of resources availability throws cold water on the idea of using hybrid-deployment to maximize resource utilization.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;elastic-training&quot;&gt;Elastic Training&lt;/h2&gt;

&lt;p&gt;Researchers and engineers proposed &lt;strong&gt;Elastic Training&lt;/strong&gt; as the key to solve the puzzle.&lt;/p&gt;

&lt;p&gt;Traditionally, the resource configuration for a distributed training job is fixed. Elastic training breaks this rule and enables users to change the number of instances participating in a distributed training job, bringing the following benefits to clusters with distributed training jobs:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Fault Tolerance: any worker instance can fail as long as at least one is surviving.&lt;/li&gt;
  &lt;li&gt;Resources Utilization: when the resources stress piles, the cluster is able to reduce the replicas of workloads with lower priority (distributed training workloads), releasing resource to other workloads (such as prediction service), ensuring SLA for business; after resources released from workloads, elastic training job is able to absorb these resource by scaling up workload replicas.&lt;/li&gt;
  &lt;li&gt;Training on Cloud: there is a type of resource on the cloud that is called “spot” or “preemptible” instances; it comes with unexpected low price tags but may be retrieved after guaranteed hour expires.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Elastic training appears a perfect match to public cloud. Combined with spot instances, we cut the cost for GPUs from ¥16.21/hour to ¥1.62/hour, reducing the overall cost for the training job by nearly 70%. Under the same budget, elastic training employs more GPUs and accelerates the training speed by 5 to 10 times.&lt;/p&gt;

&lt;h2 id=&quot;elastic-horovod&quot;&gt;Elastic Horovod&lt;/h2&gt;

&lt;p&gt;As the major player in distributed training framework, Horovod v0.20.0 offers its solution to elastic training, &lt;a href=&quot;https://horovod.readthedocs.io/en/stable/elastic_include.html&quot;&gt;Elastic Horovod&lt;/a&gt;. Here we quotes the architecture differences between Elastic Horovod and existing Horovod from &lt;a href=&quot;https://docs.google.com/document/d/15ZoHA5AeSI_boeyIBapg9WPXKrYXMRvPytPzQWTCTn4/edit#&quot;&gt;RFC Elastic Horovod&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-03-15-elastic-training/horovod-elastic.png&quot; width=&quot;&quot; alt=&quot;alt_text&quot; title=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;All collective operations are coordinated within a hvd.elastic.run function.&lt;/li&gt;
  &lt;li&gt;State is synchronized between workers before the user’s training function is executed.&lt;/li&gt;
  &lt;li&gt;Worker failure or worker added events will result in a reset event on other workers.&lt;/li&gt;
  &lt;li&gt;Reset events act as barriers to:
    &lt;ul&gt;
      &lt;li&gt;Determine whether the job should continue based on worker exit codes.&lt;/li&gt;
      &lt;li&gt;Blacklist failing hosts.&lt;/li&gt;
      &lt;li&gt;Launch workers on new hosts.&lt;/li&gt;
      &lt;li&gt;Update the rank information on existing workers.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;State is synchronized following a reset event.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When launching an elastic training job, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;horovodrun&lt;/code&gt; requires a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;discover_hosts.sh&lt;/code&gt; script to detect available hosts and slots in real time. In the following section, we refer this script as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;discover_hosts.sh&lt;/code&gt;. Nevertheless the script needs not to be named as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;discover_hosts.sh&lt;/code&gt;. An example of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;discover_hosts.sh&lt;/code&gt; can be found &lt;a href=&quot;https://horovod.readthedocs.io/en/stable/elastic_include.html#running-with-horovodrun&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;elastic-horovod-on-kubernetes&quot;&gt;Elastic Horovod on Kubernetes&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/kubeflow/mpi-operator&quot;&gt;MPI-Operator&lt;/a&gt; is designed to deploy Horovod jobs on Kubernetes. While the operator releases multiple versions, the general idea stays unchanged. It includes:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-03-15-elastic-training/mpi-operator.png&quot; width=&quot;&quot; alt=&quot;alt_text&quot; title=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MPIJob Controller creates a launcher pod and worker pods according to the replicas configuration in MPIJobs&lt;/li&gt;
  &lt;li&gt;For each MPIJob, the controller creates a &lt;a href=&quot;https://kubernetes.io/docs/concepts/configuration/configmap/&quot;&gt;ConfigMap&lt;/a&gt;, which delivers two files: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hostfile&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubexec.sh&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;With all worker pods ready, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mpirun&lt;/code&gt; on launcher pod (granted with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pod/exec&lt;/code&gt; permission) uses &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubexec.sh&lt;/code&gt; to launch processes on worker pods&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Launching an Elastic Horovod job is not feasible as there exist several incompatibilities between Elastic Horovod and MPIJob Controller. We take controller-v1 as the example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;No built-in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;discover_hosts.sh&lt;/code&gt; available on launcher pod&lt;/li&gt;
  &lt;li&gt;After worker replica number is turned down, worker pods that are no longer wanted will not be deleted by the controller, leaving the size of the distributed training unchanged&lt;/li&gt;
  &lt;li&gt;After worker replica number is turned up, the controller does not update rule in the &lt;a href=&quot;https://kubernetes.io/docs/reference/access-authn-authz/rbac/&quot;&gt;Role&lt;/a&gt; binded to the launcher pod, preventing the launcher pod from creating processes on newly created pods&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To address these compatibility issues, we pushed multiple pull requests regarding Horovod and MPI-Operator, including &lt;a href=&quot;https://github.com/kubeflow/mpi-operator/pull/335&quot;&gt;mpi-operator-pull-335&lt;/a&gt; and &lt;a href=&quot;https://github.com/horovod/horovod/pull/2199&quot;&gt;horovod-pull-2199&lt;/a&gt;. As providing an MPI-Operator-specific &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;discover_hosts.sh&lt;/code&gt; is most critical to the launcher pod for Elastic Horovod, we consider two scenarios for converting worker pods with a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Running&lt;/code&gt; phase into a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;discover_hosts.sh&lt;/code&gt; script.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A &lt;strong&gt;dynamic&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;discover_hosts.sh&lt;/code&gt; composed by MPIJob controller and synchronized to the launcher pod via ConfigMap
    &lt;ul&gt;
      &lt;li&gt;MPIJob controller has a podLister, which can be used to list worker pods readily&lt;/li&gt;
      &lt;li&gt;the controller filters worker pods with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;status.phase == Running&lt;/code&gt; and encode the result into the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;discover_hosts.sh&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;the ConfigMap is updated when &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;discover_hosts.sh&lt;/code&gt; is modified and the change will be propagated to the launcher pod by Kubernetes&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;static&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;discover_hosts.sh&lt;/code&gt; using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl&lt;/code&gt; in the launcher pod to list all running worker pods from &lt;a href=&quot;https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/&quot;&gt;APIServer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Scenario 2 changes the delivery image instead of the controller. However, as we cannot limit how frequently users will execute the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;discover_hosts.sh&lt;/code&gt; script, it poses a potential threat to the APIServer, especially when the count of worker pods is massive.&lt;/p&gt;

&lt;p&gt;An fixture to scenario 2 is to replace the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl&lt;/code&gt; with a podLister process, removing extra stress from the APIServer. In this way, we install two processes in launcher pod but lack a proper mechanism to keep the podLister alive. Once the podLister dies, there leaves no elasticity for the training job.&lt;/p&gt;

&lt;p&gt;Therefore we choose the first scenario and map the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;disocver_hosts.sh&lt;/code&gt; under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/mpi/&lt;/code&gt;. We also fixed the other compatibility issues after the worker replica configuration changes. For users choose non-elastic mode, just simply ignore &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/mpi/discover_hosts.sh&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Concerns comes to scenario 1 as well. There is a delay between the ConfigMap and what &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;horovodrun&lt;/code&gt; sees from the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;discover_hosts.sh&lt;/code&gt; in the launcher pod. This delay, on one hand, can be tweaked by cluster admin and on the other hand, can be considered as tiny compared to the training elapsed time or the time for Elastic Horovod to handle worker changes.&lt;/p&gt;

&lt;h2 id=&quot;demo&quot;&gt;Demo&lt;/h2&gt;

&lt;p&gt;We present a demo to show how to operate an Elastic Horovod job with MPI Operator.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash-5.0&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; ./tensorflow-mnist-elastic.yaml
mpijob.kubeflow.org/tensorflow-mnist-elastic 
createdbash-5.0&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get po
NAME    READY   STATUS    RESTARTS  AGE
tensorflow-mnist-elastic-launcher   1/1     Running   0          14s
tensorflow-mnist-elastic-worker-0   1/1     Running   0          14s
tensorflow-mnist-elastic-worker-1   1/1     Running   0          14s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The job is created with two workers. After the training begins, we change &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MPIJob.Spec.MPIReplicaSpecs[&quot;Worker&quot;].Replicas&lt;/code&gt; to &lt;strong&gt;3&lt;/strong&gt;, adding another worker. Let’s check how the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;discover_hosts.sh&lt;/code&gt; changes:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash-5.0&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl &lt;span class=&quot;nb&quot;&gt;exec &lt;/span&gt;tensorflow-mnist-elastic-launcher &lt;span class=&quot;nt&quot;&gt;--&lt;/span&gt; /etc/mpi/discover_hosts.sh
tensorflow-mnist-elastic-worker-0:1
tensorflow-mnist-elastic-worker-1:1
bash-5.0&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; ./patch_r3.yaml
spec:
  mpiReplicaSpecs:
    &lt;span class=&quot;s2&quot;&gt;&quot;Worker&quot;&lt;/span&gt;:
      replicas: 3
bash-5.0&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl patch mpijob tensorflow-mnist-elastic &lt;span class=&quot;nt&quot;&gt;--patch&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cat &lt;/span&gt;patch_r3.yaml&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;merge
mpijob.kubeflow.org/tensorflow-mnist-elastic patched
bash-5.0&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl &lt;span class=&quot;nb&quot;&gt;exec &lt;/span&gt;tensorflow-mnist-elastic-launcher &lt;span class=&quot;nt&quot;&gt;--&lt;/span&gt; /etc/mpi/discover_hosts.sh
tensorflow-mnist-elastic-worker-0:1
tensorflow-mnist-elastic-worker-1:1
tensorflow-mnist-elastic-worker-2:1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We reduce the replica count to 1, retrieving 2 worker instances.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash-5.0&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; ./patch_r1.yaml
spec:
  mpiReplicaSpecs:
    &lt;span class=&quot;s2&quot;&gt;&quot;Worker&quot;&lt;/span&gt;:
      replicas: 1
bash-5.0&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl patch mpijob tensorflow-mnist-elastic &lt;span class=&quot;nt&quot;&gt;--patch&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cat &lt;/span&gt;patch_r1.yaml&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;merge
mpijob.kubeflow.org/tensorflow-mnist-elastic patched
bash-5.0&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get po
NAME               READY   STATUS        RESTARTS   AGE
tensorflow-mnist-elastic-launcher   1/1     Running       0          4m48s
tensorflow-mnist-elastic-worker-0   1/1     Running       0          4m48s
tensorflow-mnist-elastic-worker-1   1/1     Terminating   0          4m48s
tensorflow-mnist-elastic-worker-2   1/1     Terminating   0          2m21s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The elastic training persists.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Thu Mar 11 01:53:18 2021[1]&amp;lt;stdout&amp;gt;:Step &lt;span class=&quot;c&quot;&gt;#40    Loss: 0.284265&lt;/span&gt;
Thu Mar 11 01:53:18 2021[0]&amp;lt;stdout&amp;gt;:Step &lt;span class=&quot;c&quot;&gt;#40    Loss: 0.259497&lt;/span&gt;
Thu Mar 11 01:53:18 2021[2]&amp;lt;stdout&amp;gt;:Step &lt;span class=&quot;c&quot;&gt;#40    Loss: 0.229993&lt;/span&gt;
Thu Mar 11 01:54:27 2021[2]&amp;lt;stderr&amp;gt;:command terminated with &lt;span class=&quot;nb&quot;&gt;exit &lt;/span&gt;code 137
Process 2 &lt;span class=&quot;nb&quot;&gt;exit &lt;/span&gt;with status code 137.
Thu Mar 11 01:54:27 2021[0]&amp;lt;stderr&amp;gt;:command terminated with &lt;span class=&quot;nb&quot;&gt;exit &lt;/span&gt;code 137
Process 0 &lt;span class=&quot;nb&quot;&gt;exit &lt;/span&gt;with status code 137.
Thu Mar 11 01:54:57 2021[1]&amp;lt;stderr&amp;gt;:[2021-03-11 01:54:57.532928: E /tmp/pip-install-2jy0u7mn/horovod/horovod/common/operations.cc:525] Horovod background loop uncaught exception: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/tmp/pip-install-2jy0u7mn/horovod/third_party/compatible_gloo/gloo/transport/tcp/pair.cc:575] Connection closed by peer &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;10.244.2.27]:54432
WARNING:root:blacklist failing host: tensorflow-mnist-elastic-worker-2
WARNING:root:blacklist failing host: tensorflow-mnist-elastic-worker-1
Thu Mar 11 01:54:58 2021[1]&amp;lt;stdout&amp;gt;:Step &lt;span class=&quot;c&quot;&gt;#50    Loss: 0.207741&lt;/span&gt;
Thu Mar 11 01:55:00 2021[1]&amp;lt;stdout&amp;gt;:Step &lt;span class=&quot;c&quot;&gt;#60    Loss: 0.119361&lt;/span&gt;
Thu Mar 11 01:55:02 2021[1]&amp;lt;stdout&amp;gt;:Step &lt;span class=&quot;c&quot;&gt;#70    Loss: 0.131966&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As we can see, Elastic Horovod on MPI-Operator now supports tweaking worker replicas dynamically. As a future work, we aim to support  &lt;a href=&quot;https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Horizontal Pod Autoscaler&lt;/code&gt;&lt;/a&gt; to MPIJob as well as other features like designated worker deletion.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;When the concept of cloud native and distributed training fuse to elastic training on Kubernetes, it lowers the cost and gives robustness and flexibility. As a team, we are working with PyTorch, Horovod and other communities to propel elastic training. We wish to further introduce our work on elasticity with PS/Worker training mode, optimization for resource and job priority and other topics on cloud native AI.&lt;/p&gt;</content><author><name>&lt;a href='https://www.linkedin.com/in/gaocegege/'&gt;Ce Gao&lt;/a&gt;, &lt;a href='https://www.linkedin.com/in/wang-zhang'&gt;Wang Zhang&lt;/a&gt;</name></author><category term="elastic training" /><category term="operators" /><summary type="html">With increase in the size of dataset and deep learning models, distributed training emerges as the mainstream approach for training neural network models in industry. While it is feasible now to launch a massive distributed training job on Kubernetes with Kubeflow, advanced features like elastic workload and other cost mitigation approaches remain leashed when we talk about deep learning jobs on Kubernetes.</summary></entry></feed>