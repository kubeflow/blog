<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>From Raw Data to Model Serving: A Blueprint for the AI/ML Lifecycle with Kubeflow | Kubeflow</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="From Raw Data to Model Serving: A Blueprint for the AI/ML Lifecycle with Kubeflow" />
<meta name="author" content="Helber Belmiro" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="From Raw Data to Model Serving: A Blueprint for the AI/ML Lifecycle with Kubeflow" />
<meta property="og:description" content="From Raw Data to Model Serving: A Blueprint for the AI/ML Lifecycle with Kubeflow" />
<link rel="canonical" href="https://blog.kubeflow.org/fraud-detection-e2e/" />
<meta property="og:url" content="https://blog.kubeflow.org/fraud-detection-e2e/" />
<meta property="og:site_name" content="Kubeflow" />
<meta property="og:image" content="https://blog.kubeflow.org/images/logo.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-07-15T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://blog.kubeflow.org/fraud-detection-e2e/","@type":"BlogPosting","headline":"From Raw Data to Model Serving: A Blueprint for the AI/ML Lifecycle with Kubeflow","dateModified":"2025-07-15T00:00:00-05:00","datePublished":"2025-07-15T00:00:00-05:00","image":"https://blog.kubeflow.org/images/logo.png","author":{"@type":"Person","name":"Helber Belmiro"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.kubeflow.org/fraud-detection-e2e/"},"description":"From Raw Data to Model Serving: A Blueprint for the AI/ML Lifecycle with Kubeflow","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://blog.kubeflow.org/feed.xml" title="Kubeflow" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-135379910-2','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="apple-touch-icon" sizes="180x180" href="/images/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/images/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/images/favicons/favicon-16x16.png">
<link rel="shortcut icon" type="image/x-icon" href="/images/favicons/favicon.ico">
<link rel="manifest" href="/images/favicons/site.webmanifest">
<link rel="mask-icon" href="/images/favicons/safari-pinned-tab.svg" color="#ffffff">
<meta name="msapplication-config" content="/images/favicons/browserconfig.xml" />
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="theme-color" content="#ffffff"><!-- remove conflicting design language, especially for unvisited links: <link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" /> -->
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css">

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Kubeflow</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">From Raw Data to Model Serving: A Blueprint for the AI/ML Lifecycle with Kubeflow</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2025-07-15T00:00:00-05:00" itemprop="datePublished">
        Jul 15, 2025
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Helber Belmiro</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      22 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#mlops">mlops</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#pipelines">pipelines</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#spark">spark</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#feast">feast</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#model-registry">model-registry</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#kserve">kserve</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="from-raw-data-to-model-serving-a-blueprint-for-the-aiml-lifecycle-with-kubeflow">From Raw Data to Model Serving: A Blueprint for the AI/ML Lifecycle with Kubeflow</h1>

<p>Are you looking for a practical, reproducible way to take a machine learning project from raw data all the way to a deployed, production-ready model? This post is your blueprint for the AI/ML lifecycle: you’ll learn how to use <a href="https://www.kubeflow.org">Kubeflow</a> and open source tools such as <a href="https://github.com/feast-dev/feast">Feast</a> to build a workflow you can run on your laptop and adapt to your own projects.</p>

<p>We’ll walk through the entire ML lifecycle—from data preparation to live inference—leveraging the Kubeflow platform to create a cohesive, production-grade MLOps workflow.</p>

<h2 id="project-overview">Project Overview</h2>

<p>The project implements a complete MLOps workflow for a fraud detection use case. Fraud detection is a critical application in financial services, where organizations need to identify potentially fraudulent transactions in real-time while minimizing false positives that could disrupt legitimate customer activity.</p>

<p>Our fraud detection system leverages machine learning to analyze large volumes of transaction data, learn patterns from historical behavior, and flag suspicious transactions that deviate from normal patterns. The model considers various features such as transaction amounts, location data, merchant information, and user behavior patterns to make predictions. This makes fraud detection an ideal use case for demonstrating MLOps concepts because it requires:</p>

<ul>
  <li><strong>Real-time inference</strong>: Fraud detection decisions must be made instantly as transactions occur</li>
  <li><strong>Feature consistency</strong>: The same features used in training must be available during inference to ensure model accuracy</li>
  <li><strong>Scalability</strong>: The system must handle high transaction volumes</li>
  <li><strong>Continuous learning</strong>: Models need regular retraining as fraud patterns evolve</li>
  <li><strong>Compliance and auditability</strong>: Financial services require comprehensive model tracking and governance</li>
</ul>

<p>The workflow ingests raw transaction data, proceeds through data preparation and feature engineering, then model training and registration, and finally deploys the model as a production-ready inference service that can evaluate transactions in real-time.</p>

<p>The entire workflow is orchestrated as a Kubeflow Pipeline, which provides a powerful framework for defining, deploying, and managing complex machine learning pipelines on Kubernetes.</p>

<p>Here is a high-level overview of the pipeline:</p>

<p><img src="../images/2025-07-15-fraud-detection-e2e/pipeline.png" alt="pipeline.png" /></p>

<h2 id="a-note-on-the-data">A Note on the Data</h2>

<p>The pipeline assumes that the initial datasets (<code class="language-plaintext highlighter-rouge">train.csv</code>, <code class="language-plaintext highlighter-rouge">test.csv</code>, etc.) are already available. For readers who wish to follow along or generate their own sample data, a script is provided in the <code class="language-plaintext highlighter-rouge">synthetic_data_generation</code> directory. This script was used to create the initial data for this project but is not part of the automated Kubeflow pipeline itself.</p>

<h2 id="why-kubeflow">Why Kubeflow?</h2>

<p>This project demonstrates the power of using Kubeflow to abstract away the complexity of Kubernetes infrastructure, allowing AI Engineers, Data Scientists, and ML engineers to focus on what matters most: the data and model performance.</p>

<h3 id="key-benefits">Key Benefits</h3>

<p><strong>Infrastructure Abstraction</strong>: Instead of manually managing Kubernetes deployments, service accounts, networking, and storage configurations, the pipeline handles all the infrastructure complexity behind the scenes. You define your ML workflow as code, and Kubeflow takes care of orchestrating the execution across your Kubernetes cluster.</p>

<p><strong>Focus on AI, Not DevOps</strong>: With the infrastructure automated, you can spend your time on the activities that directly impact model performance:</p>

<ul>
  <li>Experimenting with different feature engineering approaches</li>
  <li>Tuning hyperparameters and model architectures</li>
  <li>Analyzing prediction results and model behavior</li>
  <li>Iterating on data preparation and validation strategies</li>
</ul>

<p><strong>Reproducible and Scalable</strong>: The pipeline ensures that every run follows the same steps with the same environment configurations, making your experiments reproducible. When you’re ready to scale up, the same pipeline can run on larger Kubernetes clusters without code changes.</p>

<p><strong>Production-Ready from Day One</strong>: By using production-grade tools like KServe for model serving, Feast for feature management, and the Model Registry for governance, your development pipeline is already structured for production deployment.</p>

<p><strong>Portable and Cloud-Agnostic</strong>: The entire workflow runs on standard Kubernetes, making it portable across different cloud providers or on-premises environments. What works on your laptop will work in production.</p>

<p>This approach shifts the cognitive load from infrastructure management to data science innovation, enabling faster experimentation and more reliable production deployments.</p>

<hr />

<h2 id="getting-started-prerequisites-and-cluster-setup">Getting Started: Prerequisites and Cluster Setup</h2>

<p>Before diving into the pipeline, you need to set up your local environment. This project is designed to run on a local Kubernetes cluster using <code class="language-plaintext highlighter-rouge">kind</code>.</p>

<h3 id="prerequisites">Prerequisites</h3>

<ul>
  <li>A container engine, like <a href="https://podman.io/">Podman</a> or <a href="https://www.docker.com/get-started">Docker</a>.</li>
  <li><a href="https://www.python.org/downloads/">Python</a> (3.11 or newer).</li>
  <li><a href="https://github.com/astral-sh/uv">uv</a>: A fast Python package installer.</li>
  <li><a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">kubectl</a></li>
  <li><a href="https://kind.sigs.k8s.io/docs/user/quick-start/#installation">kind</a></li>
  <li><a href="https://min.io/docs/minio/linux/reference/minio-mc.html">mc (MinIO Client)</a></li>
</ul>

<blockquote>
  <p><strong>Note:</strong> This setup was tested on a VM with 12GB RAM, 8 CPUs, and 150GB of disk space.</p>
</blockquote>

<h3 id="1-create-a-local-kubernetes-cluster">1. Create a Local Kubernetes Cluster</h3>

<p>First, create a <code class="language-plaintext highlighter-rouge">kind</code> cluster. The following command will set up a new cluster with a specific node image compatible with the required components:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kind create cluster <span class="nt">-n</span> fraud-detection-e2e-demo <span class="nt">--image</span> kindest/node:v1.31.6
</code></pre></div></div>

<h3 id="2-deploy-kubeflow-pipelines">2. Deploy Kubeflow Pipelines</h3>

<p>With your cluster running, the next step is to deploy Kubeflow Pipelines. For this project, the <a href="https://www.kubeflow.org/docs/components/pipelines/operator-guides/installation/#deploying-kubeflow-pipelines">standalone installation</a> is recommended, as it’s lighter and faster to set up than a full Kubeflow deployment.</p>

<p>Follow the official <a href="https://www.kubeflow.org/docs/components/pipelines/operator-guides/installation/#deploying-kubeflow-pipelines">Kubeflow Pipelines standalone installation guide</a> for the latest instructions.</p>

<h3 id="3-upload-the-raw-data-to-minio">3. Upload the Raw Data to MinIO</h3>

<p><a href="https://min.io/">MinIO</a> is an open source, S3-compatible object storage system. In this project, MinIO is used to store raw datasets, intermediate artifacts, and model files, making them accessible to all pipeline components running in Kubernetes.</p>

<p>Before uploading, you need to port-forward the MinIO service so it’s accessible locally. <strong>Run the following command in a separate terminal window:</strong></p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl port-forward <span class="nt">--namespace</span> kubeflow svc/minio-service 9000:9000
</code></pre></div></div>

<p>Next, generate the synthetic data and copy it to <code class="language-plaintext highlighter-rouge">feature_engineering/feature_repo/data/input/</code> if you haven’t done yet. The synthetic data generation script creates the <code class="language-plaintext highlighter-rouge">raw_transaction_datasource.csv</code> file that serves as the primary input for the pipeline.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>synthetic_data_generation
uv <span class="nb">sync
source</span> .venv/bin/activate
python synthetic_data_generation.py
<span class="nb">cp </span>raw_transaction_datasource.csv ../feature_engineering/feature_repo/data/input
<span class="nb">cd</span> ..
</code></pre></div></div>

<p>You should see an output similar to the following. The generation may take a few minutes depending on your hardware.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Using CPython 3.11.11
Creating virtual environment at: .venv
Resolved 7 packages <span class="k">in </span>14ms
Installed 6 packages <span class="k">in </span>84ms
 + <span class="nv">numpy</span><span class="o">==</span>2.3.0
 + <span class="nv">pandas</span><span class="o">==</span>2.3.0
 + python-dateutil<span class="o">==</span>2.9.0.post0
 + <span class="nv">pytz</span><span class="o">==</span>2025.2
 + <span class="nv">six</span><span class="o">==</span>1.17.0
 + <span class="nv">tzdata</span><span class="o">==</span>2025.2
loading data...
generating transaction level data...
        0 of 1,000,000 <span class="o">(</span>0%<span class="o">)</span> <span class="nb">complete
  </span>100,000 of 1,000,000 <span class="o">(</span>10%<span class="o">)</span> <span class="nb">complete
  </span>200,000 of 1,000,000 <span class="o">(</span>20%<span class="o">)</span> <span class="nb">complete
  </span>300,000 of 1,000,000 <span class="o">(</span>30%<span class="o">)</span> <span class="nb">complete
  </span>400,000 of 1,000,000 <span class="o">(</span>40%<span class="o">)</span> <span class="nb">complete
  </span>500,000 of 1,000,000 <span class="o">(</span>50%<span class="o">)</span> <span class="nb">complete
  </span>600,000 of 1,000,000 <span class="o">(</span>60%<span class="o">)</span> <span class="nb">complete
  </span>700,000 of 1,000,000 <span class="o">(</span>70%<span class="o">)</span> <span class="nb">complete
  </span>800,000 of 1,000,000 <span class="o">(</span>80%<span class="o">)</span> <span class="nb">complete
  </span>900,000 of 1,000,000 <span class="o">(</span>90%<span class="o">)</span> <span class="nb">complete</span>
</code></pre></div></div>

<p>Next, install and configure the <a href="https://min.io/docs/minio/linux/reference/minio-mc.html">MinIO Client (<code class="language-plaintext highlighter-rouge">mc</code>)</a> if you haven’t already. Then, set up the alias and upload the datasets:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mc <span class="nb">alias set </span>minio-local http://localhost:9000 minio minio123
mc mb minio-local/mlpipeline
mc <span class="nb">cp</span> <span class="nt">-r</span> feature_engineering/feature_repo/data/input/ minio-local/mlpipeline/artifacts/feature_repo/data/
mc <span class="nb">cp </span>feature_engineering/feature_repo/feature_store.yaml minio-local/mlpipeline/artifacts/feature_repo/
</code></pre></div></div>

<p>This will create the required bucket and directory structure in MinIO and upload your raw datasets, making them available for the pipeline.</p>

<blockquote>
  <p>Once the upload is complete, you can stop the port-forward process.</p>
</blockquote>

<h3 id="4-install-model-registry-kserve-spark-operator-and-set-policies">4. Install Model Registry, KServe, Spark Operator, and Set Policies</h3>

<p>While the datasets are uploading to MinIO, you can proceed to install the remaining Kubeflow components and set up the required Kubernetes policies. The following steps summarize what’s in <code class="language-plaintext highlighter-rouge">setup.sh</code>:</p>

<h4 id="install-model-registry">Install Model Registry</h4>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl apply <span class="nt">-k</span> <span class="s2">"https://github.com/kubeflow/model-registry/manifests/kustomize/overlays/db?ref=v0.2.16"</span>
</code></pre></div></div>

<h4 id="install-kserve">Install KServe</h4>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl create namespace kserve
kubectl config set-context <span class="nt">--current</span> <span class="nt">--namespace</span><span class="o">=</span>kserve
curl <span class="nt">-s</span> <span class="s2">"https://raw.githubusercontent.com/kserve/kserve/release-0.15/hack/quick_install.sh"</span> | bash
kubectl config set-context <span class="nt">--current</span> <span class="nt">--namespace</span><span class="o">=</span>kubeflow
</code></pre></div></div>

<h4 id="install-kubeflow-spark-operator">Install Kubeflow Spark Operator</h4>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>helm repo add <span class="nt">--force-update</span> spark-operator https://kubeflow.github.io/spark-operator
helm <span class="nb">install </span>spark-operator spark-operator/spark-operator <span class="se">\</span>
    <span class="nt">--namespace</span> spark-operator <span class="se">\</span>
    <span class="nt">--create-namespace</span>

<span class="c"># Make sure the Spark Operator is watching all namespaces:</span>
helm upgrade spark-operator spark-operator/spark-operator <span class="nt">--set</span> spark.jobNamespaces<span class="o">={}</span> <span class="nt">--namespace</span> spark-operator
</code></pre></div></div>

<h4 id="apply-service-accounts-roles-secrets-and-serving-runtime">Apply Service Accounts, Roles, Secrets, and Serving Runtime</h4>

<p>The <code class="language-plaintext highlighter-rouge">manifests/</code> directory contains several YAML files that set up the necessary service accounts, permissions, secrets, and runtime configuration for both KServe and Spark jobs. Here’s what each file does:</p>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">kserve-sa.yaml</code>: Creates a service account for KServe, referencing the MinIO secret.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">kserve-minio-secret.yaml</code>: Creates a secret with MinIO credentials and endpoint info, so KServe can access models and artifacts in MinIO.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">kserve-role.yaml</code>: Defines a ClusterRole allowing management of KServe InferenceService resources.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">kserve-role-binding.yaml</code>: Binds the above ClusterRole to the <code class="language-plaintext highlighter-rouge">pipeline-runner</code> service account in the <code class="language-plaintext highlighter-rouge">kubeflow</code> namespace, so pipeline steps can create/manage inference services.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">serving-runtime.yaml</code>: Registers a custom ServingRuntime for ONNX models, specifying the container image and runtime configuration for model serving.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">spark-sa.yaml</code>: Creates a service account for Spark jobs in the <code class="language-plaintext highlighter-rouge">kubeflow</code> namespace.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">spark-role.yaml</code>: Defines a Role granting Spark jobs permissions to manage pods, configmaps, services, secrets, PVCs, and SparkApplication resources in the namespace.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">spark-role-binding.yaml</code>: Binds the above Role to both the <code class="language-plaintext highlighter-rouge">spark</code> and <code class="language-plaintext highlighter-rouge">pipeline-runner</code> service accounts in the <code class="language-plaintext highlighter-rouge">kubeflow</code> namespace.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">kustomization.yaml</code>: A Kustomize manifest that groups all the above resources for easy application.</p>
  </li>
</ul>

<p>Apply all of these with:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl apply <span class="nt">-k</span> ./manifests <span class="nt">-n</span> kubeflow
</code></pre></div></div>

<blockquote>
  <p>These resources ensure that KServe and Spark jobs have the right permissions and configuration to run in your Kubeflow environment.</p>
</blockquote>

<h2 id="building-and-understanding-the-pipeline-images">Building and Understanding the Pipeline Images</h2>

<p>In Kubeflow Pipelines, each step of a pipeline runs inside a container. This containerized approach provides several key benefits: isolation between steps, reproducible environments, and the ability to use different runtime requirements for different stages of your pipeline.</p>

<p>While Kubeflow Pipelines provides default images for common tasks, most real-world ML projects require custom images tailored to their specific needs. Each pipeline component in this project uses a specialized container image that includes the necessary dependencies, libraries, and code to execute that particular step of the ML workflow.</p>

<p>This section covers how to build these custom images. For detailed information about what each image does and how the code inside each container works, refer to the individual pipeline step sections that follow.</p>

<blockquote>
  <p><strong>Note:</strong> You only need to build and push these images if you want to modify the code for any of the pipeline components. If you’re using the project as-is, you can use the prebuilt images referenced in the pipeline.</p>
</blockquote>

<p>The pipeline uses custom container images for the following components:</p>

<h3 id="image-locations">Image Locations</h3>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">data_preparation/Containerfile</code></p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">feature_engineering/Containerfile</code></p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">pipeline/Containerfile</code></p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">rest_predictor/Containerfile</code></p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">train/Containerfile</code></p>
  </li>
</ul>

<h3 id="how-to-build">How to Build</h3>

<p>You can build each image using Podman or Docker. For example, to build the data preparation image:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>data_preparation
podman build <span class="nt">-t</span> fraud-detection-e2e-demo-data-preparation:latest <span class="nb">.</span>
<span class="c"># or</span>
<span class="c"># docker build -t fraud-detection-e2e-demo-data-preparation:latest .</span>
</code></pre></div></div>

<p>You can also refer to the <code class="language-plaintext highlighter-rouge">build_images.sh</code> script in the project root to see how to build all images in sequence.</p>

<p>Repeat this process for each component, adjusting the tag and directory as needed.</p>

<h3 id="entry-points">Entry points</h3>

<ul>
  <li><strong>data_preparation:</strong> <code class="language-plaintext highlighter-rouge">python main.py</code></li>
  <li><strong>feature_engineering:</strong> <code class="language-plaintext highlighter-rouge">python feast_feature_engineering.py</code></li>
  <li><strong>pipeline:</strong> Used for orchestrating the pipeline steps (see <code class="language-plaintext highlighter-rouge">fraud-detection-e2e.py</code>)</li>
  <li><strong>rest_predictor:</strong> <code class="language-plaintext highlighter-rouge">python predictor.py</code></li>
  <li><strong>train:</strong> <code class="language-plaintext highlighter-rouge">python train.py</code></li>
</ul>

<h3 id="pushing-images">Pushing Images</h3>

<p>After building, push the images to a container registry accessible by your Kubernetes cluster. Update the image references in your pipeline as needed.</p>

<h2 id="the-kubeflow-pipeline">The Kubeflow Pipeline</h2>

<p>The main pipeline definition is in <code class="language-plaintext highlighter-rouge">pipeline/fraud-detection-e2e.py</code>. This file is the entrypoint for the Kubeflow pipeline and orchestrates all the steps described below.</p>

<p>With your environment and permissions set up, you’re ready to run the end-to-end pipeline. Let’s walk through each stage of the workflow and see how Kubeflow orchestrates the entire machine learning lifecycle—from data preparation to real-time inference.</p>

<h3 id="1-data-preparation-with-spark">1. Data Preparation with Spark</h3>

<p><a href="https://spark.apache.org/">Apache Spark</a> is a powerful open source engine for large-scale data processing and analytics. In this project, we use Spark to efficiently process and transform raw transaction data before it enters the ML pipeline.</p>

<p>To run Spark jobs on Kubernetes, we use the <a href="https://www.kubeflow.org/docs/components/spark-operator/">Kubeflow Spark Operator</a>. The Spark Operator makes it easy to submit and manage Spark applications as native Kubernetes resources, enabling scalable, distributed data processing as part of your MLOps workflow.</p>

<h4 id="container-image-for-data-preparation">Container Image for Data Preparation</h4>

<p>This pipeline step uses a custom container image built from <code class="language-plaintext highlighter-rouge">data_preparation/Containerfile</code>. The image includes:</p>

<ul>
  <li><strong>PySpark and dependencies:</strong> Required libraries for distributed data processing</li>
  <li><strong>MinIO client libraries:</strong> For reading from and writing to object storage</li>
  <li><strong>Custom data processing code:</strong> The <code class="language-plaintext highlighter-rouge">main.py</code> script that implements the data transformation logic</li>
</ul>

<p>The container runs with the entry point <code class="language-plaintext highlighter-rouge">python main.py</code>, which orchestrates all the data preparation tasks within the Spark job.</p>

<p>The pipeline begins by launching a Spark job that performs several key data preparation steps, implemented in <code class="language-plaintext highlighter-rouge">data_preparation/main.py</code>:</p>

<h4 id="combining-datasets">Combining Datasets</h4>

<p>The job reads the raw <code class="language-plaintext highlighter-rouge">train.csv</code>, <code class="language-plaintext highlighter-rouge">test.csv</code>, and <code class="language-plaintext highlighter-rouge">validate.csv</code> datasets, adds a <code class="language-plaintext highlighter-rouge">set</code> column to each, and combines them:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_set</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">csv</span><span class="p">(</span><span class="n">INPUT_DIR</span> <span class="o">+</span> <span class="s">"train.csv"</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">inferSchema</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">test_set</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">csv</span><span class="p">(</span><span class="n">INPUT_DIR</span> <span class="o">+</span> <span class="s">"test.csv"</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">inferSchema</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">validate_set</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">csv</span><span class="p">(</span><span class="n">INPUT_DIR</span> <span class="o">+</span> <span class="s">"validate.csv"</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">inferSchema</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">train_set</span> <span class="o">=</span> <span class="n">train_set</span><span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">"set"</span><span class="p">,</span> <span class="n">lit</span><span class="p">(</span><span class="s">"train"</span><span class="p">))</span>
<span class="n">test_set</span> <span class="o">=</span> <span class="n">test_set</span><span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">"set"</span><span class="p">,</span> <span class="n">lit</span><span class="p">(</span><span class="s">"test"</span><span class="p">))</span>
<span class="n">validate_set</span> <span class="o">=</span> <span class="n">validate_set</span><span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">"set"</span><span class="p">,</span> <span class="n">lit</span><span class="p">(</span><span class="s">"valid"</span><span class="p">))</span>

<span class="n">all_sets</span> <span class="o">=</span> <span class="n">train_set</span><span class="p">.</span><span class="n">unionByName</span><span class="p">(</span><span class="n">test_set</span><span class="p">).</span><span class="n">unionByName</span><span class="p">(</span><span class="n">validate_set</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="type-conversion-and-feature-engineering">Type Conversion and Feature Engineering</h4>

<p>It converts certain columns to boolean types and generates unique IDs:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">all_sets</span> <span class="o">=</span> <span class="n">all_sets</span><span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">"fraud"</span><span class="p">,</span> <span class="n">col</span><span class="p">(</span><span class="s">"fraud"</span><span class="p">)</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="n">all_sets</span> <span class="o">=</span> <span class="n">all_sets</span><span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">"repeat_retailer"</span><span class="p">,</span> <span class="n">col</span><span class="p">(</span><span class="s">"repeat_retailer"</span><span class="p">)</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="n">all_sets</span> <span class="o">=</span> <span class="n">all_sets</span><span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">"used_chip"</span><span class="p">,</span> <span class="n">col</span><span class="p">(</span><span class="s">"used_chip"</span><span class="p">)</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="n">all_sets</span> <span class="o">=</span> <span class="n">all_sets</span><span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">"used_pin_number"</span><span class="p">,</span> <span class="n">col</span><span class="p">(</span><span class="s">"used_pin_number"</span><span class="p">)</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="n">all_sets</span> <span class="o">=</span> <span class="n">all_sets</span><span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">"online_order"</span><span class="p">,</span> <span class="n">col</span><span class="p">(</span><span class="s">"online_order"</span><span class="p">)</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">)</span>

<span class="n">w</span> <span class="o">=</span> <span class="n">Window</span><span class="p">.</span><span class="n">orderBy</span><span class="p">(</span><span class="n">lit</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="n">all_sets</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">all_sets</span>
    <span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">"idx"</span><span class="p">,</span> <span class="n">row_number</span><span class="p">().</span><span class="n">over</span><span class="p">(</span><span class="n">w</span><span class="p">))</span>
    <span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">"user_id"</span><span class="p">,</span> <span class="n">concat</span><span class="p">(</span><span class="n">lit</span><span class="p">(</span><span class="s">"user_"</span><span class="p">),</span> <span class="n">col</span><span class="p">(</span><span class="s">"idx"</span><span class="p">)</span> <span class="o">-</span> <span class="n">lit</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span>
    <span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">"transaction_id"</span><span class="p">,</span> <span class="n">concat</span><span class="p">(</span><span class="n">lit</span><span class="p">(</span><span class="s">"txn_"</span><span class="p">),</span> <span class="n">col</span><span class="p">(</span><span class="s">"idx"</span><span class="p">)</span> <span class="o">-</span> <span class="n">lit</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span>
    <span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="s">"idx"</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div></div>

<h4 id="timestamping">Timestamping</h4>

<p>The job adds <code class="language-plaintext highlighter-rouge">created</code> and <code class="language-plaintext highlighter-rouge">updated</code> timestamp columns:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">date_col</span> <span class="ow">in</span> <span class="p">[</span><span class="s">"created"</span><span class="p">,</span> <span class="s">"updated"</span><span class="p">]:</span>
    <span class="n">all_sets</span> <span class="o">=</span> <span class="n">all_sets</span><span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="n">date_col</span><span class="p">,</span> <span class="n">current_timestamp</span><span class="p">())</span>
</code></pre></div></div>

<h4 id="point-in-time-feature-calculation">Point-in-Time Feature Calculation</h4>

<p>Using the raw transaction history, the Spark job calculates features such as the number of previous transactions, average/max/stddev of previous transaction amounts, and days since the last/first transaction:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calculate_point_in_time_features</span><span class="p">(</span><span class="n">label_dataset</span><span class="p">:</span> <span class="n">DataFrame</span><span class="p">,</span> <span class="n">transactions_df</span><span class="p">:</span> <span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataFrame</span><span class="p">:</span>
    <span class="c1"># ... (see full code in data_preparation/main.py)
</span>    <span class="c1"># Aggregates and joins features for each user at each point in time
</span></code></pre></div></div>

<h4 id="output">Output</h4>

<p>The final processed data is saved as both a CSV (for entity definitions) and a Parquet file (for feature storage) in MinIO:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">entity_df</span><span class="p">.</span><span class="n">write</span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">"header"</span><span class="p">,</span> <span class="bp">True</span><span class="p">).</span><span class="n">mode</span><span class="p">(</span><span class="s">"overwrite"</span><span class="p">).</span><span class="n">csv</span><span class="p">(</span><span class="n">entity_file_name</span><span class="p">)</span>
<span class="n">df</span><span class="p">.</span><span class="n">write</span><span class="p">.</span><span class="n">mode</span><span class="p">(</span><span class="s">"overwrite"</span><span class="p">).</span><span class="n">parquet</span><span class="p">(</span><span class="n">parquet_file_name</span><span class="p">)</span>
</code></pre></div></div>

<p>All of this logic is orchestrated by the <code class="language-plaintext highlighter-rouge">prepare_data</code> component in the pipeline, which launches the Spark job on Kubernetes.</p>

<h3 id="2-feature-engineering-with-feast">2. Feature Engineering with Feast</h3>

<p><a href="https://feast.dev/">Feast</a> is an open source feature store that lets you manage and serve features for both training and inference, ensuring consistency and reducing the risk of training/serving skew. In machine learning, a “feature” is an individual measurable property or characteristic of the data being analyzed—in our fraud detection case, features include transaction amounts, distances from previous transactions, merchant types, and user behavior patterns that help the model distinguish between legitimate and fraudulent activity.</p>

<h4 id="container-image-for-feature-engineering">Container Image for Feature Engineering</h4>

<p>This pipeline step uses a custom container image built from <code class="language-plaintext highlighter-rouge">feature_engineering/Containerfile</code>. The image includes:</p>

<ul>
  <li><strong>Feast feature store:</strong> The complete Feast installation for feature management</li>
  <li><strong>Python dependencies:</strong> Required libraries for feature processing and materialization</li>
  <li><strong>Feature repository definition:</strong> The <code class="language-plaintext highlighter-rouge">repo_definition.py</code> file that defines the feature views and entities</li>
  <li><strong>MinIO client libraries:</strong> For uploading the materialized features and online store to object storage</li>
</ul>

<p>The container runs with the entry point <code class="language-plaintext highlighter-rouge">python feast_feature_engineering.py</code>, which handles the Feast operations including applying feature definitions, materializing features, and uploading the results to MinIO.</p>

<p>After data preparation, the pipeline uses Feast to register, materialize, and store features for downstream steps. This process starts with defining the features you want to use. For example, in <code class="language-plaintext highlighter-rouge">feature_repo/repo_definition.py</code>, you’ll find a <code class="language-plaintext highlighter-rouge">FeatureView</code> that lists features like <code class="language-plaintext highlighter-rouge">distance_from_home</code> and <code class="language-plaintext highlighter-rouge">ratio_to_median_purchase_price</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">transactions_fv</span> <span class="o">=</span> <span class="n">FeatureView</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s">"transactions"</span><span class="p">,</span>
    <span class="n">entities</span><span class="o">=</span><span class="p">[</span><span class="n">transaction</span><span class="p">],</span>
    <span class="n">schema</span><span class="o">=</span><span class="p">[</span>
        <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">"user_id"</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">feast</span><span class="p">.</span><span class="n">types</span><span class="p">.</span><span class="n">String</span><span class="p">),</span>
        <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">"distance_from_home"</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">feast</span><span class="p">.</span><span class="n">types</span><span class="p">.</span><span class="n">Float32</span><span class="p">),</span>
        <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">"ratio_to_median_purchase_price"</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">feast</span><span class="p">.</span><span class="n">types</span><span class="p">.</span><span class="n">Float32</span><span class="p">),</span>
        <span class="c1"># ... other features
</span>    <span class="p">],</span>
    <span class="n">online</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">source</span><span class="o">=</span><span class="n">transaction_source</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>

<p>Once the features are defined, the pipeline runs two key Feast commands. First, it applies the feature definitions to the store:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">subprocess</span><span class="p">.</span><span class="n">run</span><span class="p">([</span><span class="s">"feast"</span><span class="p">,</span> <span class="s">"apply"</span><span class="p">],</span> <span class="n">cwd</span><span class="o">=</span><span class="n">feature_repo_path</span><span class="p">,</span> <span class="n">check</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>Then, it materializes the computed features from the Parquet file into Feast’s online store, making them available for real-time inference:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">subprocess</span><span class="p">.</span><span class="n">run</span><span class="p">([</span><span class="s">"feast"</span><span class="p">,</span> <span class="s">"materialize"</span><span class="p">,</span> <span class="n">start_date</span><span class="p">,</span> <span class="n">end_date</span><span class="p">],</span> <span class="n">cwd</span><span class="o">=</span><span class="n">feature_repo_path</span><span class="p">,</span> <span class="n">check</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>Finally, the resulting feature data and the online store database are uploaded to MinIO, so they’re accessible to the rest of the pipeline:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">client</span><span class="p">.</span><span class="n">fput_object</span><span class="p">(</span><span class="n">MINIO_BUCKET</span><span class="p">,</span> <span class="n">object_path</span><span class="p">,</span> <span class="n">local_file_path</span><span class="p">)</span>
</code></pre></div></div>

<p>By using Feast in this way, you ensure that the same features are available for both model training and real-time predictions, making your ML workflow robust and reproducible.</p>

<h3 id="3-model-training">3. Model Training</h3>

<p>With the features materialized in Feast, the next step is to train the fraud detection model. The pipeline’s <code class="language-plaintext highlighter-rouge">train_model</code> component retrieves the processed features and prepares them for training. The features used include behavioral and transaction-based signals such as <code class="language-plaintext highlighter-rouge">distance_from_last_transaction</code>, <code class="language-plaintext highlighter-rouge">ratio_to_median_purchase_price</code>, <code class="language-plaintext highlighter-rouge">used_chip</code>, <code class="language-plaintext highlighter-rouge">used_pin_number</code>, and <code class="language-plaintext highlighter-rouge">online_order</code>.</p>

<h4 id="container-image-for-model-training">Container Image for Model Training</h4>

<p>This pipeline step uses a custom container image built from <code class="language-plaintext highlighter-rouge">train/Containerfile</code>. The image includes:</p>

<ul>
  <li><strong>Machine learning libraries:</strong> TensorFlow/Keras for neural network training, scikit-learn for data preprocessing</li>
  <li><strong>ONNX Runtime:</strong> For converting and exporting the trained model to ONNX format</li>
  <li><strong>PySpark:</strong> For loading and processing the feature data from Parquet files</li>
  <li><strong>MinIO client libraries:</strong> For downloading features and uploading the trained model artifacts</li>
</ul>

<p>The container runs with the entry point <code class="language-plaintext highlighter-rouge">python train.py</code>.</p>

<p>The training script loads the features, splits the data into train, validation, and test sets, and scales the input features for better model performance:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_features</span> <span class="o">=</span> <span class="n">features</span><span class="p">.</span><span class="nb">filter</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="s">"set"</span><span class="p">]</span> <span class="o">==</span> <span class="s">"train"</span><span class="p">)</span>
<span class="n">validate_features</span> <span class="o">=</span> <span class="n">features</span><span class="p">.</span><span class="nb">filter</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="s">"set"</span><span class="p">]</span> <span class="o">==</span> <span class="s">"valid"</span><span class="p">)</span>
<span class="n">test_features</span> <span class="o">=</span> <span class="n">features</span><span class="p">.</span><span class="nb">filter</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="s">"set"</span><span class="p">]</span> <span class="o">==</span> <span class="s">"test"</span><span class="p">)</span>
<span class="c1"># ... select and scale features ...
</span></code></pre></div></div>

<p>It then builds and trains a neural network model using Keras, handling class imbalance and exporting the trained model in ONNX format for portable, high-performance inference:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">(</span><span class="n">feature_indexes</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">),</span> <span class="n">class_weight</span><span class="o">=</span><span class="n">class_weights</span><span class="p">)</span>
<span class="n">save_model</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">model_path</span><span class="p">)</span>  <span class="c1"># Exports to ONNX
</span></code></pre></div></div>

<p>By structuring the training step this way, the pipeline ensures that the model is trained on the same features that will be available at inference time, supporting a robust and reproducible MLOps workflow.</p>

<h3 id="4-model-registration">4. Model Registration</h3>

<p>Once the model is trained, it’s important to track, version, and manage it before deploying to production. This is where the <a href="https://www.kubeflow.org/docs/components/model-registry/">Kubeflow Model Registry</a> comes in. The Model Registry acts as a centralized service for managing machine learning models and their metadata, making it easier to manage deployments, rollbacks, and audits.</p>

<h4 id="container-image-for-model-registration">Container Image for Model Registration</h4>

<p>This pipeline step uses a custom container image built from <code class="language-plaintext highlighter-rouge">pipeline/Containerfile</code>. The image includes:</p>

<ul>
  <li><strong>Kubeflow Pipelines SDK:</strong> For pipeline orchestration and component definitions</li>
  <li><strong>Model Registry client:</strong> Python libraries for interacting with the Kubeflow Model Registry</li>
  <li><strong>Pipeline orchestration code:</strong> The core pipeline definition and component functions</li>
</ul>

<p>The container is used as the base image for the <code class="language-plaintext highlighter-rouge">register_model</code> component, which executes the model registration logic inline within the pipeline definition. This approach allows the registration step to run lightweight operations without requiring a separate, specialized container image.</p>

<p>In the pipeline, the <code class="language-plaintext highlighter-rouge">register_model</code> component takes the trained model artifact and registers it in the Model Registry. This process includes:</p>

<ul>
  <li><strong>Assigning a unique name and version:</strong> The model is registered with a name (e.g., <code class="language-plaintext highlighter-rouge">"fraud-detection"</code>) and a version, which is typically tied to the pipeline run ID for traceability.</li>
  <li><strong>Storing metadata:</strong> Along with the model artifact, metadata such as the model format, storage location, and additional tags or descriptions can be stored for governance and reproducibility.</li>
  <li><strong>Making the model discoverable:</strong> Registered models can be easily found and referenced for deployment, monitoring, or rollback.</li>
</ul>

<p>Here’s how the registration step is implemented in the pipeline:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">dsl</span><span class="p">.</span><span class="n">component</span><span class="p">(</span><span class="n">base_image</span><span class="o">=</span><span class="n">PIPELINE_IMAGE</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">register_model</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">Input</span><span class="p">[</span><span class="n">Model</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">NamedTuple</span><span class="p">(</span><span class="s">'outputs'</span><span class="p">,</span> <span class="n">model_name</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">model_version</span><span class="o">=</span><span class="nb">str</span><span class="p">):</span>
    <span class="kn">from</span> <span class="nn">model_registry</span> <span class="kn">import</span> <span class="n">ModelRegistry</span>

    <span class="n">registry</span> <span class="o">=</span> <span class="n">ModelRegistry</span><span class="p">(</span>
        <span class="n">server_address</span><span class="o">=</span><span class="s">"http://model-registry-service.kubeflow.svc.cluster.local"</span><span class="p">,</span>
        <span class="n">port</span><span class="o">=</span><span class="mi">8080</span><span class="p">,</span>
        <span class="n">author</span><span class="o">=</span><span class="s">"fraud-detection-e2e-pipeline"</span><span class="p">,</span>
        <span class="n">user_token</span><span class="o">=</span><span class="s">"non-used"</span><span class="p">,</span>
        <span class="n">is_secure</span><span class="o">=</span><span class="bp">False</span>
    <span class="p">)</span>

    <span class="n">model_name</span> <span class="o">=</span> <span class="s">"fraud-detection"</span>
    <span class="n">model_version</span> <span class="o">=</span> <span class="s">""</span>

    <span class="n">registry</span><span class="p">.</span><span class="n">register_model</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
        <span class="n">uri</span><span class="o">=</span><span class="n">model</span><span class="p">.</span><span class="n">uri</span><span class="p">,</span>
        <span class="n">version</span><span class="o">=</span><span class="n">model_version</span><span class="p">,</span>
        <span class="n">model_format_name</span><span class="o">=</span><span class="s">"onnx"</span><span class="p">,</span>
        <span class="n">model_source_class</span><span class="o">=</span><span class="s">"pipelinerun"</span><span class="p">,</span>
        <span class="n">model_source_group</span><span class="o">=</span><span class="s">"fraud-detection"</span><span class="p">,</span>
        <span class="n">model_source_id</span><span class="o">=</span><span class="s">""</span><span class="p">,</span>
        <span class="n">model_source_kind</span><span class="o">=</span><span class="s">"kfp"</span><span class="p">,</span>
        <span class="n">model_source_name</span><span class="o">=</span><span class="s">"fraud-detection-e2e-pipeline"</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">model_version</span><span class="p">)</span>
</code></pre></div></div>

<p>By registering the model in this way, you ensure that every model deployed for inference is discoverable, reproducible, and governed—an essential part of any production-grade MLOps workflow.</p>

<h3 id="5-real-time-inference-with-kserve">5. Real-Time Inference with KServe</h3>

<p>The final stage of the pipeline is deploying the registered model as a real-time inference service using KServe. <a href="https://kserve.github.io/website/">KServe</a> is an open source model serving platform for Kubernetes that standardizes how you deploy, scale, and manage machine learning models in production.</p>

<h4 id="container-image-for-real-time-inference">Container Image for Real-Time Inference</h4>

<p>This pipeline step uses a custom container image built from <code class="language-plaintext highlighter-rouge">rest_predictor/Containerfile</code>. The image includes:</p>

<ul>
  <li><strong>KServe Python SDK:</strong> For building custom model serving endpoints</li>
  <li><strong>ONNX Runtime:</strong> For running the trained model in ONNX format</li>
  <li><strong>Feast feature store client:</strong> For retrieving real-time features during inference</li>
  <li><strong>Model Registry client:</strong> For downloading the registered model artifacts</li>
  <li><strong>Custom predictor code:</strong> The <code class="language-plaintext highlighter-rouge">predictor.py</code> script that implements the inference logic</li>
</ul>

<p>The container runs with the entry point <code class="language-plaintext highlighter-rouge">python predictor.py</code>.</p>

<p>The pipeline’s <code class="language-plaintext highlighter-rouge">serve</code> component creates a KServe InferenceService using this custom Python predictor.</p>

<p>This is done by creating a Kubernetes custom resource (CR) of kind <code class="language-plaintext highlighter-rouge">InferenceService</code>, which tells KServe how to deploy and manage the model server. The resource specifies the container image, command, arguments, and service account to use for serving the model.</p>

<p>Here’s how the InferenceService is defined and created in the pipeline:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">inference_service</span> <span class="o">=</span> <span class="n">kserve</span><span class="p">.</span><span class="n">V1beta1InferenceService</span><span class="p">(</span>
    <span class="n">api_version</span><span class="o">=</span><span class="n">kserve</span><span class="p">.</span><span class="n">constants</span><span class="p">.</span><span class="n">KSERVE_GROUP</span> <span class="o">+</span> <span class="s">"/v1beta1"</span><span class="p">,</span>
    <span class="n">kind</span><span class="o">=</span><span class="s">"InferenceService"</span><span class="p">,</span>
    <span class="n">metadata</span><span class="o">=</span><span class="n">client</span><span class="p">.</span><span class="n">V1ObjectMeta</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">model_name</span> <span class="o">+</span> <span class="s">"-"</span> <span class="o">+</span> <span class="n">job_id</span><span class="p">,</span>
        <span class="n">namespace</span><span class="o">=</span><span class="n">kserve</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">get_default_target_namespace</span><span class="p">(),</span>
        <span class="n">labels</span><span class="o">=</span><span class="p">{</span>
            <span class="s">"modelregistry/registered-model-id"</span><span class="p">:</span> <span class="n">model</span><span class="p">.</span><span class="nb">id</span><span class="p">,</span>
            <span class="s">"modelregistry/model-version-id"</span><span class="p">:</span> <span class="n">model_version</span><span class="p">.</span><span class="nb">id</span>
        <span class="p">},</span>
    <span class="p">),</span>
    <span class="n">spec</span><span class="o">=</span><span class="n">kserve</span><span class="p">.</span><span class="n">V1beta1InferenceServiceSpec</span><span class="p">(</span>
        <span class="n">predictor</span><span class="o">=</span><span class="n">kserve</span><span class="p">.</span><span class="n">V1beta1PredictorSpec</span><span class="p">(</span>
            <span class="n">service_account_name</span><span class="o">=</span><span class="s">"kserve-sa"</span><span class="p">,</span>
            <span class="n">containers</span><span class="o">=</span><span class="p">[</span>
                <span class="n">V1Container</span><span class="p">(</span>
                    <span class="n">name</span><span class="o">=</span><span class="s">"inference-container"</span><span class="p">,</span>
                    <span class="n">image</span><span class="o">=</span><span class="n">rest_predictor_image</span><span class="p">,</span>
                    <span class="n">command</span><span class="o">=</span><span class="p">[</span><span class="s">"python"</span><span class="p">,</span> <span class="s">"predictor.py"</span><span class="p">],</span>
                    <span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="s">"--model-name"</span><span class="p">,</span> <span class="n">model_name</span><span class="p">,</span> <span class="s">"--model-version"</span><span class="p">,</span> <span class="n">model_version_name</span><span class="p">]</span>
                <span class="p">)</span>
            <span class="p">]</span>
        <span class="p">)</span>
    <span class="p">),</span>
<span class="p">)</span>
<span class="n">ks_client</span> <span class="o">=</span> <span class="n">kserve</span><span class="p">.</span><span class="n">KServeClient</span><span class="p">()</span>
<span class="n">ks_client</span><span class="p">.</span><span class="n">create</span><span class="p">(</span><span class="n">inference_service</span><span class="p">)</span>
</code></pre></div></div>

<p>The custom predictor does more than just run the model: it also integrates directly with the Feast online feature store. When a prediction request arrives with a <code class="language-plaintext highlighter-rouge">user_id</code>, the predictor first fetches the user’s latest features from Feast and then feeds them to the ONNX model for inference. Here’s a simplified view of the predictor’s logic:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ONNXModel</span><span class="p">(</span><span class="n">kserve</span><span class="p">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># ... download model and initialize Feast feature store ...
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">feature_store</span> <span class="o">=</span> <span class="n">FeatureStore</span><span class="p">(</span><span class="n">repo_path</span><span class="o">=</span><span class="n">feature_repo_path</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">ort</span><span class="p">.</span><span class="n">InferenceSession</span><span class="p">(</span><span class="s">"/app/model"</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ready</span> <span class="o">=</span> <span class="bp">True</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">payload</span><span class="p">:</span> <span class="n">Dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
        <span class="n">user_id</span> <span class="o">=</span> <span class="n">payload</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"user_id"</span><span class="p">)</span>
        <span class="n">feature_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">feature_store</span><span class="p">.</span><span class="n">get_online_features</span><span class="p">(</span>
            <span class="n">entity_rows</span><span class="o">=</span><span class="p">[{</span><span class="s">"user_id"</span><span class="p">:</span> <span class="n">user_id</span><span class="p">}],</span>
            <span class="n">features</span><span class="o">=</span><span class="n">features_to_request</span><span class="p">,</span>
        <span class="p">).</span><span class="n">to_dict</span><span class="p">()</span>
        <span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span>
            <span class="p">[</span>
                <span class="n">feature_dict</span><span class="p">[</span><span class="s">"distance_from_last_transaction"</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">feature_dict</span><span class="p">[</span><span class="s">"ratio_to_median_purchase_price"</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">feature_dict</span><span class="p">[</span><span class="s">"used_chip"</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">feature_dict</span><span class="p">[</span><span class="s">"used_pin_number"</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">feature_dict</span><span class="p">[</span><span class="s">"online_order"</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
            <span class="p">]</span>
        <span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="p">{</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">get_inputs</span><span class="p">()[</span><span class="mi">0</span><span class="p">].</span><span class="n">name</span><span class="p">:</span> <span class="n">input_data</span><span class="p">})</span>
        <span class="k">return</span> <span class="p">{</span><span class="s">"user_id"</span><span class="p">:</span> <span class="n">user_id</span><span class="p">,</span> <span class="s">"prediction"</span><span class="p">:</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">tolist</span><span class="p">()}</span>
</code></pre></div></div>

<blockquote>
  <p><strong>Note:</strong><br />
By default, KServe supports several model serving runtimes, including <a href="https://github.com/triton-inference-server/server">Triton Inference Server</a> (often used via the <code class="language-plaintext highlighter-rouge">kserve-tritonserver</code> runtime). However, the official Triton server does not support macOS/arm64, which is why this project uses a custom Python predictor for local development and demonstration.<br />
If you are running on a supported platform (such as x86_64 Linux), you may want to use the <code class="language-plaintext highlighter-rouge">kserve-tritonserver</code> runtime for production workloads, as it offers high performance and native ONNX support.<br />
If you want to use Feast for online feature retrieval at inference time, a custom Python predictor (like the one in this repo) is the most straightforward approach. If you use the standard <code class="language-plaintext highlighter-rouge">kserve-tritonserver</code> runtime, you would need to implement feature fetching as a <a href="https://github.com/triton-inference-server/python_backend">Triton Python backend</a> or as a pre-processing step outside of Triton, since Triton itself does not natively integrate with Feast.</p>
</blockquote>

<p>By structuring the inference step this way, the pipeline ensures that the deployed model always uses the freshest features for each prediction, supporting robust, real-time fraud detection.</p>

<h2 id="importing-and-running-the-pipeline">Importing and Running the Pipeline</h2>

<p>Once your environment is set up and the data is uploaded, you’re ready to run the pipeline.</p>

<h3 id="import-the-pipeline">Import the Pipeline</h3>

<ol>
  <li>Open the Kubeflow Pipelines UI (usually at <a href="http://localhost:8080">http://localhost:8080</a> if you used the default port-forward).</li>
  <li>Click <strong>Pipelines</strong> in the sidebar, then click <strong>Upload pipeline</strong>.</li>
  <li>Upload the compiled pipeline YAML file (e.g., <code class="language-plaintext highlighter-rouge">pipeline/fraud-detection-e2e.yaml</code>).</li>
</ol>

<h3 id="run-the-pipeline">Run the Pipeline</h3>

<ol>
  <li>After uploading, click on your pipeline in the list.</li>
  <li>Click <strong>Create run</strong>.</li>
  <li>Optionally customize the run name and description (the defaults work fine), then click <strong>Start</strong>.</li>
</ol>

<p>You can monitor the progress and view logs for each step directly in the UI.</p>

<h2 id="testing-the-live-endpoint">Testing the Live Endpoint</h2>

<p>With the inference service running, you can now interact with your deployed model in real time. Let’s see how to send prediction requests and interpret the results.</p>

<p>Before sending requests, port-forward the inference pod so the service is accessible locally. <strong>Run this command in a separate terminal window:</strong></p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl <span class="nt">-n</span> kubeflow get pods <span class="nt">-l</span> <span class="nv">component</span><span class="o">=</span>predictor <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[*].metadata.name}"</span> | <span class="nb">tr</span> <span class="s1">' '</span> <span class="s1">'\n'</span> | <span class="nb">grep</span> <span class="s1">'^fraud-detection'</span> | <span class="nb">head</span> <span class="nt">-n1</span> | xargs <span class="nt">-I</span> <span class="o">{}</span> kubectl port-forward <span class="nt">-n</span> kubeflow pod/<span class="o">{}</span> 8081:8080
</code></pre></div></div>

<p>With the port-forward active, you can now send a request to the model:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-X</span> POST http://localhost:8081/v1/models/onnx-model:predict <span class="se">\</span>
<span class="nt">-H</span> <span class="s2">"Content-Type: application/json"</span> <span class="se">\</span>
<span class="nt">-d</span> <span class="s1">'{"user_id": "user_0"}'</span>
</code></pre></div></div>

<p>The service retrieves features for <code class="language-plaintext highlighter-rouge">user_0</code>, runs a prediction, and returns the fraud probability.</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="nl">"user_id"</span><span class="p">:</span><span class="s2">"user_0"</span><span class="p">,</span><span class="nl">"prediction"</span><span class="p">:[[</span><span class="mf">0.8173668384552002</span><span class="p">]]}</span><span class="w">
</span></code></pre></div></div>

<blockquote>
  <p>Note: The result of the prediction may vary depending on the initial raw data you uploaded.<br />
Try sending requests with a few different <code class="language-plaintext highlighter-rouge">user_id</code> values (e.g., <code class="language-plaintext highlighter-rouge">"user_1"</code>, <code class="language-plaintext highlighter-rouge">"user_2"</code>, etc.) to see how the predictions change.</p>
</blockquote>

<h2 id="conclusion">Conclusion</h2>

<p>This post has walked you through a complete, reproducible AI/ML workflow—from raw data to a live model serving endpoint—using Kubeflow and open source tools. Along the way, you’ve seen how to prepare data with Spark, manage features with Feast, train and register models, and deploy real-time inference services with KServe, all orchestrated in a portable pipeline you can run on your own laptop.</p>

<p>By following this blueprint, you can adapt and extend the process for your own machine learning projects, whether you’re working locally or scaling up to production. Kubeflow’s modular platform and ecosystem make it possible to manage the entire ML lifecycle in a consistent, automated, and open way.</p>

<p>Ready to try it yourself? The complete source code for this project is available on <a href="https://github.com/hbelmiro/fraud_detection_e2e_demo/tree/kubeflow">GitHub</a>.</p>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="kubeflow/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/fraud-detection-e2e/" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>The Machine Learning Toolkit for Kubernetes.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/kubeflow" target="_blank" title="kubeflow"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/kubeflow" target="_blank" title="kubeflow"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
